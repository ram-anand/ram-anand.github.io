{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multiple Layer Neural Network Implementation(Forward and Back Propagation-Gradient Descent).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOTLEPpP+ivmilrbC5uOLrZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ram-anand/ram-anand.github.io/blob/main/Multiple_Layer_Neural_Network_Implementation(Forward_and_Back_Propagation_Gradient_Descent).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdR2clIYsC1t"
      },
      "source": [
        "## Custom Neural Network\r\n",
        "\r\n",
        "    Author: Kumar Ramanand\r\n",
        "    Last revision: 26-12-2020\r\n",
        "    Github: https://github.com/ram-anand\r\n",
        "\r\n",
        "Implemented a neural network \r\n",
        "  -  supports multiple layers\r\n",
        "  -  custom loss functions (MSE, MLE and MAE)\r\n",
        "  -  batch gradient descent\r\n",
        "  -  RELU, SIGMOID and TANH activations\r\n",
        "  -  simple structure for forward and back propagation\r\n",
        "  -  training for multiple epochs\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1T4_7_SBr8bb"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPRihC86r9Kf"
      },
      "source": [
        "class CustomNeuralNetwork:\r\n",
        "    def __init__(self):\r\n",
        "        self.n = 0 # number of samples\r\n",
        "        self.m = 0 # number of variables\r\n",
        "        self.epoch = 1 # number of epochs\r\n",
        "        self.learning_rate = 0.001\r\n",
        "        self.X = None # X data in input examples\r\n",
        "        self.Y = None # Y labels in input examples\r\n",
        "        self.YP = 0.0\r\n",
        "        self.layer_count = 0\r\n",
        "        self.layer_variables = [] \r\n",
        "        self.layer_weights = []\r\n",
        "        self.layer_bias = []\r\n",
        "        self.layer_activation = []\r\n",
        "        self.layer_output = {}\r\n",
        "        self.layer_derivative = {}\r\n",
        "        self.training_loss = 0.0\r\n",
        "        self.loss_method = None\r\n",
        "        \r\n",
        "#     def model_input_labels(self, data, labels):\r\n",
        "#         self.n = np.array(data).shape[0] # number of input examples\r\n",
        "#         self.m = np.array(labels).shape[1] # number of variables\r\n",
        "#         self.X = data\r\n",
        "#         self.Y = labels\r\n",
        "        \r\n",
        "    def model_add_layer(self, input_shape, list_layerdepth_layertype_tuple=[(1,\"relu\")]):\r\n",
        "        self.n = input_shape[0]\r\n",
        "        self.m = input_shape[1]\r\n",
        "        for layerdepth, layertype in list_layerdepth_layertype_tuple:\r\n",
        "            if self.layer_count == 0:\r\n",
        "                self.layer_weights.append(np.random.rand(self.m, layerdepth))\r\n",
        "            else:\r\n",
        "                self.layer_weights.append(np.random.rand(self.layer_variables[self.layer_count-1], layerdepth))\r\n",
        "            self.layer_bias.append(np.random.rand(1, layerdepth))\r\n",
        "            self.layer_variables.append(layerdepth)\r\n",
        "            self.layer_activation.append(layertype)\r\n",
        "            self.layer_count += 1\r\n",
        "        print(\"total layer count: \", self.layer_count)\r\n",
        "        print(\"layers: \", list_layerdepth_layertype_tuple)\r\n",
        "           \r\n",
        "    def relu(self, a):\r\n",
        "        b = np.maximum(a, 0.0)\r\n",
        "        #a[a < 0.0] = 0.0\r\n",
        "        # data, derivative of relu output w.r.t. to input\r\n",
        "        c = np.greater(a, 0.0).astype(int)\r\n",
        "        return b, c  \r\n",
        "\r\n",
        "    def sigmoid(self, a):\r\n",
        "        b = 1/np.exp(-a)\r\n",
        "        # data, derivative of sigmoid output w.r.t. to input\r\n",
        "        c = b * (1-b)\r\n",
        "        return b, c # data, derivative \r\n",
        "\r\n",
        "    def tanh(self, a):\r\n",
        "        b = (np.exp(a) - np.exp(-a))/(np.exp(a) + np.exp(-a))\r\n",
        "        # data, derivative of tanh output w.r.t. to input\r\n",
        "        c = (1-b**2)\r\n",
        "        return b, c # data, derivative         \r\n",
        "\r\n",
        "    def model_feed_forward(self, data):\r\n",
        "        # calculate output for each layer: from 0th (first) layer to last layer\r\n",
        "#        print(\"input data: \\n\", data)\r\n",
        "        for layer in range(self.layer_count):\r\n",
        "#            print(\"-------------------------------------------------\")\r\n",
        "#            print(\"layer: \", layer+1, \" of \", self.layer_count)\r\n",
        "#             print(\"layer variables: \", self.layer_variables[layer])\r\n",
        "#             print(\"layer weight: \\n\", self.layer_weights[layer])\r\n",
        "#             print(\"layer bias: \\n\", self.layer_bias[layer])\r\n",
        "            self.X = data\r\n",
        "            if layer == 0:\r\n",
        "                z = (np.dot(self.X, self.layer_weights[layer]) + self.layer_bias[layer])#.reshape(self.n, self.layer_variables[layer])\r\n",
        "#                print(\"layer sum: \\n\", z)\r\n",
        "            else:\r\n",
        "                z = (np.dot(self.layer_output[layer-1], self.layer_weights[layer]) + self.layer_bias[layer])#.reshape(self.n, self.layer_variables[layer])\r\n",
        "#                print(\"layer sum: \\n\", z)\r\n",
        "            if self.layer_activation[layer] == \"relu\":\r\n",
        "                self.layer_output[layer] = self.relu(z)[0]\r\n",
        "                self.layer_derivative[layer] = self.relu(z)[1]\r\n",
        "#                print(\"layer output: \\n\", self.relu(z)[0])\r\n",
        "#                print(\"layer output: \\n\", self.layer_output[layer])\r\n",
        "            elif self.layer_activation[layer] == \"sigmoid\":\r\n",
        "                self.layer_output[layer] = self.sigmoid(z)[0]\r\n",
        "                self.layer_derivative[layer] = self.sigmoid(z)[1]\r\n",
        "#                print(\"layer output: \\n\", self.sigmoid(z)[0])\r\n",
        "#                print(\"layer output: \\n\", self.layer_output[layer])\r\n",
        "            elif self.layer_activation[layer] == \"tanh\":\r\n",
        "                self.layer_output[layer] = self.tanh(z)[0]\r\n",
        "                self.layer_derivative[layer] = self.tanh(z)[1]\r\n",
        "#                print(\"layer output: \\n\", self.tanh(z)[0])\r\n",
        "#                print(\"layer output: \\n\", self.layer_output[layer])\r\n",
        "            else:\r\n",
        "                print(\"activation type : \", self.layer_activation[layer], \"is undefined, use predefine activations - relu, sigmoid or tanh\")\r\n",
        "                return None\r\n",
        "        return self.layer_output[layer]\r\n",
        "        \r\n",
        "    def model_training(self, data=None, labels=None, losstype = \"mse\", learning_rate = 0.01, epoch=1):\r\n",
        "\r\n",
        "        if (data is None):\r\n",
        "            print(\"please provide sample data in data=X, labels=Y form in model training\")\r\n",
        "        else:\r\n",
        "            self.X = data\r\n",
        "            if (self.n != np.array(data).shape[0] or self.m != np.array(data).shape[1]):\r\n",
        "                print(\"reconfigure layer required as data dimension \",(self.n, self.m) ,\" did not match with layer input dimension \", (np.array(data).shape[0], np.array(data).shape[1]))\r\n",
        "\r\n",
        "#             self.n = np.array(data).shape[0] # number of input examples\r\n",
        "#             self.m = np.array(labels).shape[1] # number of variables\r\n",
        "                      \r\n",
        "        if (labels is None):\r\n",
        "            print(\"please provide sample data in data=X, labels=Y form in model training\")\r\n",
        "        else:\r\n",
        "            self.Y = labels\r\n",
        "\r\n",
        "        self.loss_method = losstype\r\n",
        "        self.learning_rate = learning_rate\r\n",
        "        self.epoch = epoch\r\n",
        "        for i in range(self.epoch): \r\n",
        "            print(\"************************************************\")\r\n",
        "            print(\"training epoch: \", i)\r\n",
        "            print(\"************************************************\")\r\n",
        "            \r\n",
        "            print(\"input dim: \",(self.n, self.m), \", input data: \\n\", self.X)\r\n",
        "            print(\"label dim: \",(self.n, 1), \", labels: \\n\", self.Y)\r\n",
        "\r\n",
        "            # feed foreward output at last layer (output layer)\r\n",
        "            self.YP = self.model_feed_forward(self.X)\r\n",
        "            \r\n",
        "            # calculating error \r\n",
        "            print((\"===============================================\"))\r\n",
        "            if self.loss_method == \"mle\":\r\n",
        "                self.training_loss = (1/self.n) * np.sum(-self.Y * np.log(self.YP) - (1-self.Y) * np.log(1+self.YP))\r\n",
        "                self.training_derivative = ((-self.Y/self.YP) + (1-self.Y)/(1-self.YP)).reshape(self.n,1)\r\n",
        "            elif self.loss_method == \"mae\":\r\n",
        "                self.training_loss = (1/self.n) * np.sum(np.abs(self.Y - self.YP))\r\n",
        "                self.training_derivative = 1\r\n",
        "            elif self.loss_method == \"mse\":\r\n",
        "                #self.training_loss = (0.5/self.n) * np.sum(np.dot((self.Y - self.YP).T, (self.Y - self.YP)))\r\n",
        "                #self.training_loss = (0.5/self.n) * ((self.Y - self.YP)**2).mean(axis=0)\r\n",
        "                self.training_loss = (0.5/self.n) * (np.square(self.Y - self.YP)).mean(axis=0)\r\n",
        "                self.training_derivative = -(1/self.n) * np.sum(np.abs(self.Y - self.YP))\r\n",
        "            else:\r\n",
        "                print(\"loss type : \", self.loss_method, \"is undefined, use predefine loss methods - mae, mle or mse\")\r\n",
        "                break\r\n",
        "            \r\n",
        "            print(self.loss_method, \" - training loss: \", self.training_loss)\r\n",
        "            print(\"===============================================\")\r\n",
        "            # x - matrix multiplication, * - element wise matrix multiplication\r\n",
        "            # update derivative for each layer: \r\n",
        "            # update sequence: from last or output layer to 0th (first or input) layer\r\n",
        "            for layer in range(self.layer_count-1,-1,-1):\r\n",
        "                if layer == self.layer_count-1:\r\n",
        "                    # loss function derivative w.r.t. current(last or output) layer input  = loss function derivative w.r.t. current(last or output) layer output x current(last or output) layer output derivative w.r.t. its input\r\n",
        "                    self.layer_derivative[layer] = self.training_derivative * self.layer_derivative[layer]\r\n",
        "                    #self.layer_derivative[layer] = self.training_derivative * (self.layer_output[layer] * (1 - self.layer_output[layer])) # example for sigmoid output from current layer\r\n",
        "                else:\r\n",
        "                    # loss function derivative w.r.t. current layer(hidden or inner) input  = higher layer derivative (calculated previosuly) x higher layer weights in transposed form * current(hidden or inner) layer's output derivative w.r.t. its input\r\n",
        "                    self.layer_derivative[layer] = np.dot(self.layer_derivative[layer+1], self.layer_weights[layer+1].T) * self.layer_derivative[layer]\r\n",
        "                    #self.layer_derivative[layer] = np.dot(self.layer_derivative[layer+1], self.layer_weights[layer+1].T) * (self.layer_output[layer] * (1 - self.layer_output[layer])) # example for sigmoid output from current layer\r\n",
        "            \r\n",
        "            # x - matrix multiplication, * - element wise matrix multiplication\r\n",
        "            # weight and bias update at each layer using loss function's derivative w.r.t. layer input \r\n",
        "            # update sequence: from last layer to 0th(first) layer\r\n",
        "            for layer in range(self.layer_count-1,-1,-1):  \r\n",
        "                if layer == 0:\r\n",
        "                    print(\"old weight: \\n\", self.layer_weights[layer])\r\n",
        "                    print(\"old bias: \\n\", self.layer_bias[layer])\r\n",
        "                    # current layer weights = current layer weights - learning rate * (input for current layer(which is sample data) in transposed form x loss function derivative w.r.t. current layer input)\r\n",
        "                    self.layer_weights[layer] = self.layer_weights[layer] - (self.learning_rate/self.n) * np.dot(self.X.T, self.layer_derivative[layer])\r\n",
        "                    self.layer_bias[layer] = self.layer_bias[layer] - (self.learning_rate/self.n) * np.sum(self.layer_derivative[layer], keepdims=True)                    \r\n",
        "                    print(\"new weight: \\n\", self.layer_weights[layer])\r\n",
        "                    print(\"new bias: \\n\", self.layer_bias[layer])\r\n",
        "                    print(\"layer: \", layer, \" weight and bias updated\")\r\n",
        "                    print(\"===============================================\")\r\n",
        "                else:\r\n",
        "                    print(\"old weight: \\n\", self.layer_weights[layer])\r\n",
        "                    print(\"old bias: \\n\", self.layer_bias[layer])\r\n",
        "                    # current layer weights = current layer weights - learning rate * (input for current layer(means output of layer prior to this layer) in transposed form  x loss function derivative w.r.t. current layer input)\r\n",
        "                    self.layer_weights[layer] = self.layer_weights[layer] - (self.learning_rate/self.n) * np.dot(self.layer_output[layer-1].T, self.layer_derivative[layer])\r\n",
        "                    self.layer_bias[layer] = self.layer_bias[layer] - (self.learning_rate/self.n) * np.sum(self.layer_derivative[layer], keepdims=True)\r\n",
        "                    print(\"new weight: \\n\", self.layer_weights[layer])\r\n",
        "                    print(\"new bias: \\n\", self.layer_bias[layer])\r\n",
        "                    print(\"layer: \", layer, \" weight and bias updated\")\r\n",
        "                    print(\"===============================================\")"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F215OwtssX5I"
      },
      "source": [
        "Train model using training data and labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_g6OMXwsXHx",
        "outputId": "94c7bbce-a7aa-4bce-bca4-184aa5f6d8c4"
      },
      "source": [
        "number_of_samples = 5\r\n",
        "number_of_variables = 4\r\n",
        "inputs = np.random.rand(number_of_samples, number_of_variables)\r\n",
        "labels = np.random.randint(0,2, (number_of_samples, 1))\r\n",
        "print(\"data columns: variable_1,  variable_2, variable_3, variable_4,   actual_label\")\r\n",
        "for row, label in zip(inputs, labels):\r\n",
        "  print(\"input row: \", row, \", label: \", label)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data columns: variable_1,  variable_2, variable_3, variable_4,   actual_label\n",
            "input row:  [0.90261069 0.76407068 0.5166661  0.49162398] , label:  [0]\n",
            "input row:  [0.62762702 0.24233076 0.21679735 0.97608848] , label:  [0]\n",
            "input row:  [0.93952445 0.7612189  0.26742162 0.66416611] , label:  [1]\n",
            "input row:  [0.63295327 0.65261062 0.24243109 0.28554563] , label:  [1]\n",
            "input row:  [0.42292638 0.01225002 0.05385473 0.81184832] , label:  [1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQoUZ_8_sldo",
        "outputId": "3eebbe34-442d-460d-a4a7-1901755eae07"
      },
      "source": [
        "model = CustomNeuralNetwork()\r\n",
        "model.model_add_layer((5,4), [(2, \"tanh\"),(2, \"relu\"),(2, \"relu\"),(1, \"sigmoid\")])\r\n",
        "model.model_training(inputs, labels, learning_rate=0.1, losstype = \"mle\", epoch=2)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total layer count:  4\n",
            "layers:  [(2, 'tanh'), (2, 'relu'), (2, 'relu'), (1, 'sigmoid')]\n",
            "************************************************\n",
            "training epoch:  0\n",
            "************************************************\n",
            "input dim:  (5, 4) , input data: \n",
            " [[0.90261069 0.76407068 0.5166661  0.49162398]\n",
            " [0.62762702 0.24233076 0.21679735 0.97608848]\n",
            " [0.93952445 0.7612189  0.26742162 0.66416611]\n",
            " [0.63295327 0.65261062 0.24243109 0.28554563]\n",
            " [0.42292638 0.01225002 0.05385473 0.81184832]]\n",
            "label dim:  (5, 1) , labels: \n",
            " [[0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]]\n",
            "===============================================\n",
            "mle  - training loss:  -2.11912367238131\n",
            "===============================================\n",
            "old weight: \n",
            " [[0.13506909]\n",
            " [0.63157728]]\n",
            "old bias: \n",
            " [[0.50601938]]\n",
            "new weight: \n",
            " [[-2.26667638]\n",
            " [-0.67891107]]\n",
            "new bias: \n",
            " [[-0.22867921]]\n",
            "layer:  3  weight and bias updated\n",
            "===============================================\n",
            "old weight: \n",
            " [[0.54881424 0.38531344]\n",
            " [0.91900604 0.2650414 ]]\n",
            "old bias: \n",
            " [[0.81740459 0.82742297]]\n",
            "new weight: \n",
            " [[ 0.43987401 -0.12408644]\n",
            " [ 0.71933552 -0.66860943]]\n",
            "new bias: \n",
            " [[0.25415057 0.26416896]]\n",
            "layer:  2  weight and bias updated\n",
            "===============================================\n",
            "old weight: \n",
            " [[0.21079576 0.57228093]\n",
            " [0.38846341 0.99621212]]\n",
            "old bias: \n",
            " [[0.53938084 0.54899692]]\n",
            "new weight: \n",
            " [[-0.01760913  0.36255201]\n",
            " [ 0.17710023  0.80213146]]\n",
            "new bias: \n",
            " [[0.09194463 0.1015607 ]]\n",
            "layer:  1  weight and bias updated\n",
            "===============================================\n",
            "old weight: \n",
            " [[0.86179769 0.62251587]\n",
            " [0.5093543  0.21308655]\n",
            " [0.7872766  0.69470527]\n",
            " [0.87745588 0.97002225]]\n",
            "old bias: \n",
            " [[0.75140744 0.20054082]]\n",
            "new weight: \n",
            " [[0.85761596 0.58769911]\n",
            " [0.50691084 0.18908382]\n",
            " [0.78595419 0.68244297]\n",
            " [0.87285129 0.93770885]]\n",
            "new bias: \n",
            " [[0.69071611 0.13984949]]\n",
            "layer:  0  weight and bias updated\n",
            "===============================================\n",
            "************************************************\n",
            "training epoch:  1\n",
            "************************************************\n",
            "input dim:  (5, 4) , input data: \n",
            " [[0.90261069 0.76407068 0.5166661  0.49162398]\n",
            " [0.62762702 0.24233076 0.21679735 0.97608848]\n",
            " [0.93952445 0.7612189  0.26742162 0.66416611]\n",
            " [0.63295327 0.65261062 0.24243109 0.28554563]\n",
            " [0.42292638 0.01225002 0.05385473 0.81184832]]\n",
            "label dim:  (5, 1) , labels: \n",
            " [[0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]]\n",
            "===============================================\n",
            "mle  - training loss:  1.7068869717659323\n",
            "===============================================\n",
            "old weight: \n",
            " [[-2.26667638]\n",
            " [-0.67891107]]\n",
            "old bias: \n",
            " [[-0.22867921]]\n",
            "new weight: \n",
            " [[-2.20295358]\n",
            " [-0.67891107]]\n",
            "new bias: \n",
            " [[-0.17406066]]\n",
            "layer:  3  weight and bias updated\n",
            "===============================================\n",
            "old weight: \n",
            " [[ 0.43987401 -0.12408644]\n",
            " [ 0.71933552 -0.66860943]]\n",
            "old bias: \n",
            " [[0.25415057 0.26416896]]\n",
            "new weight: \n",
            " [[ 0.41203999 -0.12408644]\n",
            " [ 0.57930211 -0.66860943]]\n",
            "new bias: \n",
            " [[0.13034799 0.14036638]]\n",
            "layer:  2  weight and bias updated\n",
            "===============================================\n",
            "old weight: \n",
            " [[-0.01760913  0.36255201]\n",
            " [ 0.17710023  0.80213146]]\n",
            "old bias: \n",
            " [[0.09194463 0.1015607 ]]\n",
            "new weight: \n",
            " [[-0.0702668   0.27643977]\n",
            " [ 0.13100409  0.72674944]]\n",
            "new bias: \n",
            " [[-0.05156851 -0.04195243]]\n",
            "layer:  1  weight and bias updated\n",
            "===============================================\n",
            "old weight: \n",
            " [[0.85761596 0.58769911]\n",
            " [0.50691084 0.18908382]\n",
            " [0.78595419 0.68244297]\n",
            " [0.87285129 0.93770885]]\n",
            "old bias: \n",
            " [[0.69071611 0.13984949]]\n",
            "new weight: \n",
            " [[0.85650264 0.5739925 ]\n",
            " [0.50626071 0.17932742]\n",
            " [0.7856667  0.67852711]\n",
            " [0.87163303 0.92535176]]\n",
            "new bias: \n",
            " [[0.66597038 0.11510377]]\n",
            "layer:  0  weight and bias updated\n",
            "===============================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRPwHKiEbK6S"
      },
      "source": [
        "Prediction using sample data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGtzAIl1tP5e",
        "outputId": "8b6678f3-20cd-43a3-e305-291d2eec261f"
      },
      "source": [
        "example = np.random.rand(4)\r\n",
        "print(\"input data: \", example)\r\n",
        "output = (model.model_feed_forward(example) > 0).astype(int)\r\n",
        "print(\"label: \", output)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input data:  [0.37517401 0.35816061 0.2046096  0.76298819]\n",
            "label:  [[1]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}