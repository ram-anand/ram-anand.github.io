{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multiple Layer Neural Network Implementation(Forward and Back Propagation-Gradient Descent).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPjh07J7w2I39r4OsKbJKf6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ram-anand/ram-anand.github.io/blob/main/Multiple_Layer_Neural_Network_Implementation(Forward_and_Back_Propagation_Gradient_Descent).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdR2clIYsC1t"
      },
      "source": [
        "## Custom Neural Network\r\n",
        "\r\n",
        "    Author: Kumar Ramanand\r\n",
        "    Last revision: 26-12-2020\r\n",
        "    Github: https://github.com/ram-anand\r\n",
        "\r\n",
        "### Implemented a neural network \r\n",
        "  -  supports multiple layers\r\n",
        "  -  custom loss functions (MSE, MLE and MAE)\r\n",
        "  -  batch gradient descent\r\n",
        "  -  RELU, SIGMOID and TANH activations\r\n",
        "  -  simple structure for forward and back propagation\r\n",
        "  -  training for multiple epochs\r\n",
        "  -  faster processing , uses vectorized matrix calculations\r\n",
        "\r\n",
        "\r\n",
        "## **How to Use**\r\n",
        "\r\n",
        "      # Create model\r\n",
        "          model = CustomNeuralNetwork()\r\n",
        "      # Adding layers\r\n",
        "          model.model_add_layer((5,4), [(2, \"tanh\"),(2, \"relu\"),(2, \"relu\"),(1, \"sigmoid\")])\r\n",
        "      # Training model\r\n",
        "          model.model_training(inputs, labels, learning_rate=0.1, losstype = \"mle\", epoch=2)\r\n",
        "\r\n",
        "      # Prediction using a sample data\r\n",
        "          model.model_feed_forward(example)\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1T4_7_SBr8bb"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPRihC86r9Kf"
      },
      "source": [
        "class CustomNeuralNetwork:\r\n",
        "    def __init__(self):\r\n",
        "        self.n = 0 # number of samples\r\n",
        "        self.m = 0 # number of variables\r\n",
        "        self.epoch = 1 # number of epochs\r\n",
        "        self.learning_rate = 0.001 # learning rate for gradient descent\r\n",
        "        self.X = None # X data in input examples\r\n",
        "        self.Y = None # Y labels in input examples\r\n",
        "        self.YP = 0.0 # predicted labels for input examples\r\n",
        "        self.layer_count = 0 # total number of layers starting from 0 to last/output layer\r\n",
        "        self.layer_variables = [] # variable/neurons count list, stores number of neurons in each layer\r\n",
        "        self.layer_activation = [] # activation list, stores activation used in each layer\r\n",
        "        self.layer_weights = [] # weight matrix, dimesion = (number of neurons/output in prior layer, number of neurons in current layer)\r\n",
        "        self.layer_bias = [] # bias matrix, dimesion = (number of neurons in the layer, 1)\r\n",
        "        self.layer_output = {} # dictionary of output matrix at each layer, {0:output matrix 0, 1:output matrix 1,...}, dimension of output matrix = (number of neurons in the layer, 1) \r\n",
        "        self.layer_derivative = {} # dictionary of derivative(of loss function w.r.t. layer input) matrix at each layer, {0:derivative matrix 0, 1:derivative matrix 1,...}, dimension of derivative matrix = (number of neurons in the layer, 1) \r\n",
        "        self.training_loss = 0.0 # total training loss in each epoch\r\n",
        "        self.loss_method = None # method to calculate training loss, mean squared error(mse), mean absolute error(mae), maximum likelyhood estimate (mle)\r\n",
        "        self.convergence_limit = 0.00001 # convergence error limit when training should stop\r\n",
        "\r\n",
        "#     def model_input_labels(self, data, labels):\r\n",
        "#         self.n = np.array(data).shape[0] # number of input examples\r\n",
        "#         self.m = np.array(labels).shape[1] # number of variables\r\n",
        "#         self.X = data\r\n",
        "#         self.Y = labels\r\n",
        "        \r\n",
        "    def model_add_layer(self, input_shape, list_layerdepth_layertype_tuple=[(1,\"relu\")]):\r\n",
        "        self.n = input_shape[0]\r\n",
        "        self.m = input_shape[1]\r\n",
        "        for layerdepth, layertype in list_layerdepth_layertype_tuple:\r\n",
        "            if self.layer_count == 0:\r\n",
        "                self.layer_weights.append(np.random.rand(self.m, layerdepth))\r\n",
        "            else:\r\n",
        "                self.layer_weights.append(np.random.rand(self.layer_variables[self.layer_count-1], layerdepth))\r\n",
        "            self.layer_bias.append(np.random.rand(1, layerdepth))\r\n",
        "            self.layer_variables.append(layerdepth)\r\n",
        "            self.layer_activation.append(layertype)\r\n",
        "            self.layer_count += 1\r\n",
        "        print(\"total layer count: \", self.layer_count)\r\n",
        "        print(\"layers: \", list_layerdepth_layertype_tuple)\r\n",
        "           \r\n",
        "    def relu(self, a):\r\n",
        "        b = np.maximum(a, 0.0)\r\n",
        "        #a[a < 0.0] = 0.0\r\n",
        "        # data, derivative of relu output w.r.t. to input\r\n",
        "        c = np.greater(a, 0.0).astype(int)\r\n",
        "        return b, c  \r\n",
        "\r\n",
        "    def sigmoid(self, a):\r\n",
        "        b = 1/np.exp(-a)\r\n",
        "        # data, derivative of sigmoid output w.r.t. to input\r\n",
        "        c = b * (1-b)\r\n",
        "        return b, c # data, derivative \r\n",
        "\r\n",
        "    def tanh(self, a):\r\n",
        "        b = (np.exp(a) - np.exp(-a))/(np.exp(a) + np.exp(-a))\r\n",
        "        # data, derivative of tanh output w.r.t. to input\r\n",
        "        c = (1-b**2)\r\n",
        "        return b, c # data, derivative         \r\n",
        "\r\n",
        "    def model_feed_forward(self, data):\r\n",
        "        # calculate output for each layer: from 0th (first) layer to last layer\r\n",
        "#        print(\"input data: \\n\", data)\r\n",
        "        for layer in range(self.layer_count):\r\n",
        "#            print(\"-------------------------------------------------\")\r\n",
        "#            print(\"layer: \", layer+1, \" of \", self.layer_count)\r\n",
        "#             print(\"layer variables: \", self.layer_variables[layer])\r\n",
        "#             print(\"layer weight: \\n\", self.layer_weights[layer])\r\n",
        "#             print(\"layer bias: \\n\", self.layer_bias[layer])\r\n",
        "            self.X = data\r\n",
        "            if layer == 0:\r\n",
        "                z = (np.dot(self.X, self.layer_weights[layer]) + self.layer_bias[layer])#.reshape(self.n, self.layer_variables[layer])\r\n",
        "#                print(\"layer sum: \\n\", z)\r\n",
        "            else:\r\n",
        "                z = (np.dot(self.layer_output[layer-1], self.layer_weights[layer]) + self.layer_bias[layer])#.reshape(self.n, self.layer_variables[layer])\r\n",
        "#                print(\"layer sum: \\n\", z)\r\n",
        "            if self.layer_activation[layer] == \"relu\":\r\n",
        "                self.layer_output[layer] = self.relu(z)[0]\r\n",
        "                self.layer_derivative[layer] = self.relu(z)[1]\r\n",
        "#                print(\"layer output: \\n\", self.relu(z)[0])\r\n",
        "#                print(\"layer output: \\n\", self.layer_output[layer])\r\n",
        "            elif self.layer_activation[layer] == \"sigmoid\":\r\n",
        "                self.layer_output[layer] = self.sigmoid(z)[0]\r\n",
        "                self.layer_derivative[layer] = self.sigmoid(z)[1]\r\n",
        "#                print(\"layer output: \\n\", self.sigmoid(z)[0])\r\n",
        "#                print(\"layer output: \\n\", self.layer_output[layer])\r\n",
        "            elif self.layer_activation[layer] == \"tanh\":\r\n",
        "                self.layer_output[layer] = self.tanh(z)[0]\r\n",
        "                self.layer_derivative[layer] = self.tanh(z)[1]\r\n",
        "#                print(\"layer output: \\n\", self.tanh(z)[0])\r\n",
        "#                print(\"layer output: \\n\", self.layer_output[layer])\r\n",
        "            else:\r\n",
        "                print(\"activation type : \", self.layer_activation[layer], \"is undefined, use predefine activations - relu, sigmoid or tanh\")\r\n",
        "                return None\r\n",
        "        return self.layer_output[layer]\r\n",
        "        \r\n",
        "    def model_training(self, data=None, labels=None, losstype = \"mse\", learning_rate = 0.01, convergence_limit = 0.001, epoch=1):\r\n",
        "\r\n",
        "        if (data is None):\r\n",
        "            print(\"please provide sample data in data=X, labels=Y form in model training\")\r\n",
        "        else:\r\n",
        "            self.X = data\r\n",
        "            if (self.n != np.array(data).shape[0] or self.m != np.array(data).shape[1]):\r\n",
        "                print(\"reconfigure layer required as data dimension \",(self.n, self.m) ,\" did not match with layer input dimension \", (np.array(data).shape[0], np.array(data).shape[1]))\r\n",
        "\r\n",
        "#             self.n = np.array(data).shape[0] # number of input examples\r\n",
        "#             self.m = np.array(labels).shape[1] # number of variables\r\n",
        "                      \r\n",
        "        if (labels is None):\r\n",
        "            print(\"please provide sample data in data=X, labels=Y form in model training\")\r\n",
        "        else:\r\n",
        "            self.Y = labels\r\n",
        "\r\n",
        "        self.loss_method = losstype\r\n",
        "        self.learning_rate = learning_rate\r\n",
        "        self.convergence_limit = convergence_limit\r\n",
        "        self.epoch = epoch\r\n",
        "        for i in range(self.epoch): \r\n",
        "            print(\"************************************************\")\r\n",
        "            print(\"training epoch: \", i)\r\n",
        "            print(\"************************************************\")\r\n",
        "            \r\n",
        "            print(\"input dim: \",(self.n, self.m), \", input data: \\n\", self.X)\r\n",
        "            print(\"label dim: \",(self.n, 1), \", labels: \\n\", self.Y)\r\n",
        "\r\n",
        "            # feed foreward output at last layer (output layer)\r\n",
        "            self.YP = self.model_feed_forward(self.X)\r\n",
        "            \r\n",
        "            # calculating error \r\n",
        "            print((\"===============================================\"))\r\n",
        "            if self.loss_method == \"mle\":\r\n",
        "                self.training_loss = (1/self.n) * np.sum(-self.Y * np.log(self.YP) - (1-self.Y) * np.log(1+self.YP))\r\n",
        "                self.training_derivative = ((-self.Y/self.YP) + (1-self.Y)/(1-self.YP)).reshape(self.n,1)\r\n",
        "            elif self.loss_method == \"mae\":\r\n",
        "                self.training_loss = (1/self.n) * np.sum(np.abs(self.Y - self.YP))\r\n",
        "                self.training_derivative = 1\r\n",
        "            elif self.loss_method == \"mse\":\r\n",
        "                #self.training_loss = (0.5/self.n) * np.sum(np.dot((self.Y - self.YP).T, (self.Y - self.YP)))\r\n",
        "                #self.training_loss = (0.5/self.n) * ((self.Y - self.YP)**2).mean(axis=0)\r\n",
        "                self.training_loss = (0.5/self.n) * (np.square(self.Y - self.YP)).mean(axis=0)\r\n",
        "                self.training_derivative = -(1/self.n) * np.sum(np.abs(self.Y - self.YP))\r\n",
        "            else:\r\n",
        "                print(\"loss type : \", self.loss_method, \"is undefined, use predefine loss methods - mae, mle or mse\")\r\n",
        "                break\r\n",
        "            \r\n",
        "            print(self.loss_method, \" - training loss: \", self.training_loss)\r\n",
        "            if self.training_loss < self.convergence_limit:\r\n",
        "              print(\"model training error converged, convergence limit: \",self.convergence_limit,\", current training error: \",self.training_loss)\r\n",
        "              break\r\n",
        "            print(\"===============================================\")\r\n",
        "            # x - matrix multiplication, * - element wise matrix multiplication\r\n",
        "            # update derivative for each layer: \r\n",
        "            # update sequence: from last or output layer to 0th (first or input) layer\r\n",
        "            for layer in range(self.layer_count-1,-1,-1):\r\n",
        "                if layer == self.layer_count-1:\r\n",
        "                    # loss function derivative w.r.t. current(last or output) layer input  = loss function derivative w.r.t. current(last or output) layer output x current(last or output) layer output derivative w.r.t. its input\r\n",
        "                    self.layer_derivative[layer] = self.training_derivative * self.layer_derivative[layer]\r\n",
        "                    #self.layer_derivative[layer] = self.training_derivative * (self.layer_output[layer] * (1 - self.layer_output[layer])) # example for sigmoid output from current layer\r\n",
        "                else:\r\n",
        "                    # loss function derivative w.r.t. current layer(hidden or inner) input  = higher layer derivative (calculated previosuly) x higher layer weights in transposed form * current(hidden or inner) layer's output derivative w.r.t. its input\r\n",
        "                    self.layer_derivative[layer] = np.dot(self.layer_derivative[layer+1], self.layer_weights[layer+1].T) * self.layer_derivative[layer]\r\n",
        "                    #self.layer_derivative[layer] = np.dot(self.layer_derivative[layer+1], self.layer_weights[layer+1].T) * (self.layer_output[layer] * (1 - self.layer_output[layer])) # example for sigmoid output from current layer\r\n",
        "            \r\n",
        "            # x - matrix multiplication, * - element wise matrix multiplication\r\n",
        "            # weight and bias update at each layer using loss function's derivative w.r.t. layer input \r\n",
        "            # update sequence: from last layer to 0th(first) layer\r\n",
        "            for layer in range(self.layer_count-1,-1,-1):  \r\n",
        "                if layer == 0:\r\n",
        "                    print(\"old weight: \\n\", self.layer_weights[layer])\r\n",
        "                    print(\"old bias: \\n\", self.layer_bias[layer])\r\n",
        "                    # current layer weights = current layer weights - learning rate * (input for current layer(which is sample data) in transposed form x loss function derivative w.r.t. current layer input)\r\n",
        "                    self.layer_weights[layer] = self.layer_weights[layer] - (self.learning_rate/self.n) * np.dot(self.X.T, self.layer_derivative[layer])\r\n",
        "                    self.layer_bias[layer] = self.layer_bias[layer] - (self.learning_rate/self.n) * np.sum(self.layer_derivative[layer], keepdims=True)                    \r\n",
        "                    print(\"new weight: \\n\", self.layer_weights[layer])\r\n",
        "                    print(\"new bias: \\n\", self.layer_bias[layer])\r\n",
        "                    print(\"layer: \", layer, \" weight and bias updated\")\r\n",
        "                    print(\"===============================================\")\r\n",
        "                else:\r\n",
        "                    print(\"old weight: \\n\", self.layer_weights[layer])\r\n",
        "                    print(\"old bias: \\n\", self.layer_bias[layer])\r\n",
        "                    # current layer weights = current layer weights - learning rate * (input for current layer(means output of layer prior to this layer) in transposed form  x loss function derivative w.r.t. current layer input)\r\n",
        "                    self.layer_weights[layer] = self.layer_weights[layer] - (self.learning_rate/self.n) * np.dot(self.layer_output[layer-1].T, self.layer_derivative[layer])\r\n",
        "                    self.layer_bias[layer] = self.layer_bias[layer] - (self.learning_rate/self.n) * np.sum(self.layer_derivative[layer], keepdims=True)\r\n",
        "                    print(\"new weight: \\n\", self.layer_weights[layer])\r\n",
        "                    print(\"new bias: \\n\", self.layer_bias[layer])\r\n",
        "                    print(\"layer: \", layer, \" weight and bias updated\")\r\n",
        "                    print(\"===============================================\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F215OwtssX5I"
      },
      "source": [
        "Train model using training data and labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_g6OMXwsXHx",
        "outputId": "11e006f6-8248-4791-936b-598ee3caa1bd"
      },
      "source": [
        "number_of_samples = 5\r\n",
        "number_of_variables = 4\r\n",
        "inputs = np.random.rand(number_of_samples, number_of_variables)\r\n",
        "labels = np.random.randint(0,2, (number_of_samples, 1))\r\n",
        "print(\"data columns: variable_1,  variable_2, variable_3, variable_4,   actual_label\")\r\n",
        "for row, label in zip(inputs, labels):\r\n",
        "  print(\"input row: \", row, \", label: \", label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data columns: variable_1,  variable_2, variable_3, variable_4,   actual_label\n",
            "input row:  [0.20711923 0.80985653 0.59274475 0.32508462] , label:  [1]\n",
            "input row:  [0.80128437 0.10399709 0.9993516  0.85643245] , label:  [1]\n",
            "input row:  [0.05125211 0.55969057 0.67633784 0.07084215] , label:  [0]\n",
            "input row:  [0.03169936 0.5811244  0.28407813 0.65874811] , label:  [1]\n",
            "input row:  [0.64226163 0.23612786 0.44969191 0.22866992] , label:  [1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQoUZ_8_sldo",
        "outputId": "e2528e83-1e7b-41c4-a9d4-7eeff041f56f"
      },
      "source": [
        "model = CustomNeuralNetwork()\r\n",
        "model.model_add_layer((5,4), [(2, \"tanh\"),(2, \"relu\"),(2, \"relu\"),(1, \"sigmoid\")])\r\n",
        "model.model_training(inputs, labels, learning_rate=0.1, losstype = \"mle\", epoch=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total layer count:  4\n",
            "layers:  [(2, 'tanh'), (2, 'relu'), (2, 'relu'), (1, 'sigmoid')]\n",
            "************************************************\n",
            "training epoch:  0\n",
            "************************************************\n",
            "input dim:  (5, 4) , input data: \n",
            " [[0.20711923 0.80985653 0.59274475 0.32508462]\n",
            " [0.80128437 0.10399709 0.9993516  0.85643245]\n",
            " [0.05125211 0.55969057 0.67633784 0.07084215]\n",
            " [0.03169936 0.5811244  0.28407813 0.65874811]\n",
            " [0.64226163 0.23612786 0.44969191 0.22866992]]\n",
            "label dim:  (5, 1) , labels: \n",
            " [[1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]]\n",
            "===============================================\n",
            "mle  - training loss:  -3.195029247850842\n",
            "model training error converged, convergence limit:  0.001 , current training error:  -3.195029247850842\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRPwHKiEbK6S"
      },
      "source": [
        "Prediction using sample data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGtzAIl1tP5e",
        "outputId": "75b3fa93-ba87-46b1-9e32-b386407a8043"
      },
      "source": [
        "example = np.random.rand(4) * 0.01\r\n",
        "print(\"input data: \", example)\r\n",
        "output = (model.model_feed_forward(example) > 0).astype(int)\r\n",
        "print(\"label: \", output)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input data:  [0.00894269 0.00235017 0.00356326 0.00893417]\n",
            "label:  [[1]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}