{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multiple Layer Neural Network Implementation(Forward and Back Propagation-Gradient Descent).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO6YLfqQali6jOpZW04hS5U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ram-anand/ram-anand.github.io/blob/main/Multiple_Layer_Neural_Network_Implementation(Forward_and_Back_Propagation_Gradient_Descent).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdR2clIYsC1t"
      },
      "source": [
        "## Custom Neural Network\r\n",
        "\r\n",
        "    Author: Kumar Ramanand\r\n",
        "    Last revision: 26-12-2020\r\n",
        "    Github: https://github.com/ram-anand\r\n",
        "\r\n",
        "Implemented a neural network \r\n",
        "  -  supports multiple layers\r\n",
        "  -  custom loss functions (MSE, MLE and MAE)\r\n",
        "  -  batch gradient descent\r\n",
        "  -  RELU, SIGMOID and TANH activations\r\n",
        "  -  Simple structure for forward and back propagation\r\n",
        "  -  training for multiple epochs\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1T4_7_SBr8bb"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPRihC86r9Kf"
      },
      "source": [
        "class CustomNeuralNetwork:\r\n",
        "    def __init__(self):\r\n",
        "        self.n = 0 # number of samples\r\n",
        "        self.m = 0 # number of variables\r\n",
        "        self.epoch = 1 # number of epochs\r\n",
        "        self.learning_rate = 0.001\r\n",
        "        self.X = None # X data in input examples\r\n",
        "        self.Y = None # Y labels in input examples\r\n",
        "        self.YP = 0.0\r\n",
        "        self.layer_count = 0\r\n",
        "        self.layer_variables = [] \r\n",
        "        self.layer_weights = []\r\n",
        "        self.layer_bias = []\r\n",
        "        self.layer_activation = []\r\n",
        "        self.layer_output = {}\r\n",
        "        self.layer_derivative = {}\r\n",
        "        self.training_loss = 0.0\r\n",
        "        self.loss_method = None\r\n",
        "        \r\n",
        "#     def model_input_labels(self, data, labels):\r\n",
        "#         self.n = np.array(data).shape[0] # number of input examples\r\n",
        "#         self.m = np.array(labels).shape[1] # number of variables\r\n",
        "#         self.X = data\r\n",
        "#         self.Y = labels\r\n",
        "        \r\n",
        "    def model_add_layer(self, input_shape, list_layerdepth_layertype_tuple=[(1,\"relu\")]):\r\n",
        "        self.n = input_shape[0]\r\n",
        "        self.m = input_shape[1]\r\n",
        "        for layerdepth, layertype in list_layerdepth_layertype_tuple:\r\n",
        "            if self.layer_count == 0:\r\n",
        "                self.layer_weights.append(np.random.rand(self.m, layerdepth))\r\n",
        "            else:\r\n",
        "                self.layer_weights.append(np.random.rand(self.layer_variables[self.layer_count-1], layerdepth))\r\n",
        "            self.layer_bias.append(np.random.rand(1, layerdepth))\r\n",
        "            self.layer_variables.append(layerdepth)\r\n",
        "            self.layer_activation.append(layertype)\r\n",
        "            self.layer_count += 1\r\n",
        "        print(\"total layer count: \", self.layer_count)\r\n",
        "        print(\"layers: \", list_layerdepth_layertype_tuple)\r\n",
        "           \r\n",
        "    def relu(self, a):\r\n",
        "        b = np.maximum(a, 0.0)\r\n",
        "        #a[a < 0.0] = 0.0\r\n",
        "        # data, derivative of relu output w.r.t. to input\r\n",
        "        c = np.greater(a, 0.0).astype(int)\r\n",
        "        return b, c  \r\n",
        "\r\n",
        "    def sigmoid(self, a):\r\n",
        "        b = 1/np.exp(-a)\r\n",
        "        # data, derivative of sigmoid output w.r.t. to input\r\n",
        "        c = b * (1-b)\r\n",
        "        return b, c # data, derivative \r\n",
        "\r\n",
        "    def tanh(self, a):\r\n",
        "        b = (np.exp(a) - np.exp(-a))/(np.exp(a) + np.exp(-a))\r\n",
        "        # data, derivative of tanh output w.r.t. to input\r\n",
        "        c = (1-b**2)\r\n",
        "        return b, c # data, derivative         \r\n",
        "\r\n",
        "    def model_feed_forward(self, data):\r\n",
        "        # calculate output for each layer: from 0th (first) layer to last layer\r\n",
        "#        print(\"input data: \\n\", data)\r\n",
        "        for layer in range(self.layer_count):\r\n",
        "#            print(\"-------------------------------------------------\")\r\n",
        "#            print(\"layer: \", layer+1, \" of \", self.layer_count)\r\n",
        "#             print(\"layer variables: \", self.layer_variables[layer])\r\n",
        "#             print(\"layer weight: \\n\", self.layer_weights[layer])\r\n",
        "#             print(\"layer bias: \\n\", self.layer_bias[layer])\r\n",
        "            self.X = data\r\n",
        "            if layer == 0:\r\n",
        "                z = (np.dot(self.X, self.layer_weights[layer]) + self.layer_bias[layer])#.reshape(self.n, self.layer_variables[layer])\r\n",
        "#                print(\"layer sum: \\n\", z)\r\n",
        "            else:\r\n",
        "                z = (np.dot(self.layer_output[layer-1], self.layer_weights[layer]) + self.layer_bias[layer])#.reshape(self.n, self.layer_variables[layer])\r\n",
        "#                print(\"layer sum: \\n\", z)\r\n",
        "            if self.layer_activation[layer] == \"relu\":\r\n",
        "                self.layer_output[layer] = self.relu(z)[0]\r\n",
        "                self.layer_derivative[layer] = self.relu(z)[1]\r\n",
        "#                print(\"layer output: \\n\", self.relu(z)[0])\r\n",
        "#                print(\"layer output: \\n\", self.layer_output[layer])\r\n",
        "            elif self.layer_activation[layer] == \"sigmoid\":\r\n",
        "                self.layer_output[layer] = self.sigmoid(z)[0]\r\n",
        "                self.layer_derivative[layer] = self.sigmoid(z)[1]\r\n",
        "#                print(\"layer output: \\n\", self.sigmoid(z)[0])\r\n",
        "#                print(\"layer output: \\n\", self.layer_output[layer])\r\n",
        "            elif self.layer_activation[layer] == \"tanh\":\r\n",
        "                self.layer_output[layer] = self.tanh(z)[0]\r\n",
        "                self.layer_derivative[layer] = self.tanh(z)[1]\r\n",
        "#                print(\"layer output: \\n\", self.tanh(z)[0])\r\n",
        "#                print(\"layer output: \\n\", self.layer_output[layer])\r\n",
        "            else:\r\n",
        "                print(\"activation type : \", self.layer_activation[layer], \"is undefined, use predefine activations - relu, sigmoid or tanh\")\r\n",
        "                return None\r\n",
        "        return self.layer_output[layer]\r\n",
        "        \r\n",
        "    def model_training(self, data=None, labels=None, losstype = \"mse\", learning_rate = 0.01, epoch=1):\r\n",
        "\r\n",
        "        if (data is None):\r\n",
        "            print(\"please provide sample data in data=X, labels=Y form in model training\")\r\n",
        "        else:\r\n",
        "            self.X = data\r\n",
        "            if (self.n != np.array(data).shape[0] or self.m != np.array(data).shape[1]):\r\n",
        "                print(\"reconfigure layer required as data dimension \",(self.n, self.m) ,\" did not match with layer input dimension \", (np.array(data).shape[0], np.array(data).shape[1]))\r\n",
        "\r\n",
        "#             self.n = np.array(data).shape[0] # number of input examples\r\n",
        "#             self.m = np.array(labels).shape[1] # number of variables\r\n",
        "                      \r\n",
        "        if (labels is None):\r\n",
        "            print(\"please provide sample data in data=X, labels=Y form in model training\")\r\n",
        "        else:\r\n",
        "            self.Y = labels\r\n",
        "\r\n",
        "        self.loss_method = losstype\r\n",
        "        self.learning_rate = learning_rate\r\n",
        "        self.epoch = epoch\r\n",
        "        for i in range(self.epoch): \r\n",
        "            print(\"************************************************\")\r\n",
        "            print(\"training epoch: \", i)\r\n",
        "            print(\"************************************************\")\r\n",
        "            \r\n",
        "            print(\"input dim: \",(self.n, self.m), \", input data: \\n\", self.X)\r\n",
        "            print(\"label dim: \",(self.n, 1), \", labels: \\n\", self.Y)\r\n",
        "\r\n",
        "            # feed foreward output at last layer (output layer)\r\n",
        "            self.YP = self.model_feed_forward(self.X)\r\n",
        "            \r\n",
        "            # calculating error \r\n",
        "            print((\"===============================================\"))\r\n",
        "            if self.loss_method == \"mle\":\r\n",
        "                self.training_loss = (1/self.n) * np.sum(-self.Y * np.log(self.YP) - (1-self.Y) * np.log(1+self.YP))\r\n",
        "                self.training_derivative = ((-self.Y/self.YP) + (1-self.Y)/(1-self.YP)).reshape(self.n,1)\r\n",
        "            elif self.loss_method == \"mae\":\r\n",
        "                self.training_loss = (1/self.n) * np.sum(np.abs(self.Y - self.YP))\r\n",
        "                self.training_derivative = 1\r\n",
        "            elif self.loss_method == \"mse\":\r\n",
        "                #self.training_loss = (0.5/self.n) * np.sum(np.dot((self.Y - self.YP).T, (self.Y - self.YP)))\r\n",
        "                #self.training_loss = (0.5/self.n) * ((self.Y - self.YP)**2).mean(axis=0)\r\n",
        "                self.training_loss = (0.5/self.n) * (np.square(self.Y - self.YP)).mean(axis=0)\r\n",
        "                self.training_derivative = -(1/self.n) * np.sum(np.abs(self.Y - self.YP))\r\n",
        "            else:\r\n",
        "                print(\"loss type : \", self.loss_method, \"is undefined, use predefine loss methods - mae, mle or mse\")\r\n",
        "                break\r\n",
        "            \r\n",
        "            print(self.loss_method, \" - training loss: \", self.training_loss)\r\n",
        "            print(\"===============================================\")\r\n",
        "            # x - matrix multiplication, * - element wise matrix multiplication\r\n",
        "            # update derivative for each layer: \r\n",
        "            # update sequence: from last or output layer to 0th (first or input) layer\r\n",
        "            for layer in range(self.layer_count-1,-1,-1):\r\n",
        "                if layer == self.layer_count-1:\r\n",
        "                    # loss function derivative w.r.t. current(last or output) layer input  = loss function derivative w.r.t. current(last or output) layer output x current(last or output) layer output derivative w.r.t. its input\r\n",
        "                    self.layer_derivative[layer] = self.training_derivative * self.layer_derivative[layer]\r\n",
        "                    #self.layer_derivative[layer] = self.training_derivative * (self.layer_output[layer] * (1 - self.layer_output[layer])) # example for sigmoid output from current layer\r\n",
        "                else:\r\n",
        "                    # loss function derivative w.r.t. current layer input  = higher layer derivative (calculated previosuly) x higher layer weights in transposed form * current(hidden) layer's output derivative w.r.t. its input\r\n",
        "                    self.layer_derivative[layer] = np.dot(self.layer_derivative[layer+1], self.layer_weights[layer+1].T) * self.layer_derivative[layer]\r\n",
        "                    #self.layer_derivative[layer] = np.dot(self.layer_derivative[layer+1], self.layer_weights[layer+1].T) * (self.layer_output[layer] * (1 - self.layer_output[layer])) # example for sigmoid output from current layer\r\n",
        "            \r\n",
        "            # x - matrix multiplication, * - element wise matrix multiplication\r\n",
        "            # weight and bias update at each layer using loss function's derivative w.r.t. layer input \r\n",
        "            # update sequence: from last layer to 0th(first) layer\r\n",
        "            for layer in range(self.layer_count-1,-1,-1):  \r\n",
        "                if layer == 0:\r\n",
        "                    print(\"old weight: \\n\", self.layer_weights[layer])\r\n",
        "                    print(\"old bias: \\n\", self.layer_bias[layer])\r\n",
        "                    # current layer weights = current layer weights - learning rate * (input for current layer in transposed form x loss function derivative w.r.t. current layer input)\r\n",
        "                    self.layer_weights[layer] = self.layer_weights[layer] - (self.learning_rate/self.n) * np.dot(self.X.T, self.layer_derivative[layer])\r\n",
        "                    self.layer_bias[layer] = self.layer_bias[layer] - (self.learning_rate/self.n) * np.sum(self.layer_derivative[layer], keepdims=True)                    \r\n",
        "                    print(\"new weight: \\n\", self.layer_weights[layer])\r\n",
        "                    print(\"new bias: \\n\", self.layer_bias[layer])\r\n",
        "                    print(\"layer: \", layer, \" weight and bias updated\")\r\n",
        "                    print(\"===============================================\")\r\n",
        "                else:\r\n",
        "                    print(\"old weight: \\n\", self.layer_weights[layer])\r\n",
        "                    print(\"old bias: \\n\", self.layer_bias[layer])\r\n",
        "                    # current layer weights = current layer weights - learning rate * (input for current layer in transposed form  x loss function derivative w.r.t. current layer input)\r\n",
        "                    self.layer_weights[layer] = self.layer_weights[layer] - (self.learning_rate/self.n) * np.dot(self.layer_output[layer-1].T, self.layer_derivative[layer])\r\n",
        "                    self.layer_bias[layer] = self.layer_bias[layer] - (self.learning_rate/self.n) * np.sum(self.layer_derivative[layer], keepdims=True)\r\n",
        "                    print(\"new weight: \\n\", self.layer_weights[layer])\r\n",
        "                    print(\"new bias: \\n\", self.layer_bias[layer])\r\n",
        "                    print(\"layer: \", layer, \" weight and bias updated\")\r\n",
        "                    print(\"===============================================\")"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F215OwtssX5I"
      },
      "source": [
        "Train model using training data and labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_g6OMXwsXHx",
        "outputId": "080f8f7a-56d2-44b2-d697-ca668865f238"
      },
      "source": [
        "number_of_samples = 5\r\n",
        "number_of_variables = 4\r\n",
        "inputs = np.random.rand(number_of_samples, number_of_variables)\r\n",
        "labels = np.random.randint(0,2, (number_of_samples, 1))\r\n",
        "print(\"data columns: variable_1,  variable_2, variable_3, variable_4,   actual_label\")\r\n",
        "for row, label in zip(inputs, labels):\r\n",
        "  print(\"input row: \", row, \", label: \", label)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data columns: variable_1,  variable_2, variable_3, variable_4,   actual_label\n",
            "input row:  [0.59022015 0.59137068 0.12230778 0.94653576] , label:  [0]\n",
            "input row:  [0.55923812 0.63682229 0.03809029 0.3864149 ] , label:  [1]\n",
            "input row:  [0.8168744  0.63058601 0.44893483 0.48866296] , label:  [0]\n",
            "input row:  [0.66536801 0.01928024 0.64692611 0.81965709] , label:  [0]\n",
            "input row:  [0.07601859 0.59639211 0.0415213  0.5349243 ] , label:  [1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQoUZ_8_sldo",
        "outputId": "8850c9ba-689b-4781-94c2-0d52dcb89ed7"
      },
      "source": [
        "model = CustomNeuralNetwork()\r\n",
        "model.model_add_layer((5,4), [(2, \"tanh\"),(2, \"relu\"),(2, \"relu\"),(1, \"sigmoid\")])\r\n",
        "model.model_training(inputs, labels, learning_rate=0.1, losstype = \"mle\", epoch=2)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total layer count:  4\n",
            "layers:  [(2, 'tanh'), (2, 'relu'), (2, 'relu'), (1, 'sigmoid')]\n",
            "************************************************\n",
            "training epoch:  0\n",
            "************************************************\n",
            "input dim:  (5, 4) , input data: \n",
            " [[0.59022015 0.59137068 0.12230778 0.94653576]\n",
            " [0.55923812 0.63682229 0.03809029 0.3864149 ]\n",
            " [0.8168744  0.63058601 0.44893483 0.48866296]\n",
            " [0.66536801 0.01928024 0.64692611 0.81965709]\n",
            " [0.07601859 0.59639211 0.0415213  0.5349243 ]]\n",
            "label dim:  (5, 1) , labels: \n",
            " [[0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]]\n",
            "===============================================\n",
            "mle  - training loss:  -3.307151181884831\n",
            "===============================================\n",
            "old weight: \n",
            " [[0.94552635]\n",
            " [0.41040911]]\n",
            "old bias: \n",
            " [[0.447646]]\n",
            "new weight: \n",
            " [[-4.75740977]\n",
            " [-4.70998178]]\n",
            "new bias: \n",
            " [[-2.18929592]]\n",
            "layer:  3  weight and bias updated\n",
            "===============================================\n",
            "old weight: \n",
            " [[0.37999043 0.13794704]\n",
            " [0.62177496 0.93503133]]\n",
            "old bias: \n",
            " [[0.24540354 0.05638997]]\n",
            "new weight: \n",
            " [[-5.35943776 -2.35327238]\n",
            " [-3.55897299 -0.87963733]]\n",
            "new bias: \n",
            " [[-3.3301195  -3.51913307]]\n",
            "layer:  2  weight and bias updated\n",
            "===============================================\n",
            "old weight: \n",
            " [[0.68198887 0.38045465]\n",
            " [0.77743366 0.6930394 ]]\n",
            "old bias: \n",
            " [[0.92357973 0.66776617]]\n",
            "new weight: \n",
            " [[-0.37715471 -2.09394491]\n",
            " [-0.23789727 -1.67900388]]\n",
            "new bias: \n",
            " [[-2.73532398 -2.99113754]]\n",
            "layer:  1  weight and bias updated\n",
            "===============================================\n",
            "old weight: \n",
            " [[0.90096276 0.77392451]\n",
            " [0.84139829 0.30198866]\n",
            " [0.31224416 0.17017588]\n",
            " [0.29333849 0.58615457]]\n",
            "old bias: \n",
            " [[0.90794359 0.68439714]]\n",
            "new weight: \n",
            " [[0.85053475 0.60895954]\n",
            " [0.78699645 0.1083207 ]\n",
            " [0.28566234 0.09617979]\n",
            " [0.22099792 0.36649102]]\n",
            "new bias: \n",
            " [[0.42107206 0.19752561]]\n",
            "layer:  0  weight and bias updated\n",
            "===============================================\n",
            "************************************************\n",
            "training epoch:  1\n",
            "************************************************\n",
            "input dim:  (5, 4) , input data: \n",
            " [[0.59022015 0.59137068 0.12230778 0.94653576]\n",
            " [0.55923812 0.63682229 0.03809029 0.3864149 ]\n",
            " [0.8168744  0.63058601 0.44893483 0.48866296]\n",
            " [0.66536801 0.01928024 0.64692611 0.81965709]\n",
            " [0.07601859 0.59639211 0.0415213  0.5349243 ]]\n",
            "label dim:  (5, 1) , labels: \n",
            " [[0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]]\n",
            "===============================================\n",
            "mle  - training loss:  0.8120246376542231\n",
            "===============================================\n",
            "old weight: \n",
            " [[-4.75740977]\n",
            " [-4.70998178]]\n",
            "old bias: \n",
            " [[-2.18929592]]\n",
            "new weight: \n",
            " [[-4.75740977]\n",
            " [-4.70998178]]\n",
            "new bias: \n",
            " [[-2.16049548]]\n",
            "layer:  3  weight and bias updated\n",
            "===============================================\n",
            "old weight: \n",
            " [[-5.35943776 -2.35327238]\n",
            " [-3.55897299 -0.87963733]]\n",
            "old bias: \n",
            " [[-3.3301195  -3.51913307]]\n",
            "new weight: \n",
            " [[-5.35943776 -2.35327238]\n",
            " [-3.55897299 -0.87963733]]\n",
            "new bias: \n",
            " [[-3.3301195  -3.51913307]]\n",
            "layer:  2  weight and bias updated\n",
            "===============================================\n",
            "old weight: \n",
            " [[-0.37715471 -2.09394491]\n",
            " [-0.23789727 -1.67900388]]\n",
            "old bias: \n",
            " [[-2.73532398 -2.99113754]]\n",
            "new weight: \n",
            " [[-0.37715471 -2.09394491]\n",
            " [-0.23789727 -1.67900388]]\n",
            "new bias: \n",
            " [[-2.73532398 -2.99113754]]\n",
            "layer:  1  weight and bias updated\n",
            "===============================================\n",
            "old weight: \n",
            " [[0.85053475 0.60895954]\n",
            " [0.78699645 0.1083207 ]\n",
            " [0.28566234 0.09617979]\n",
            " [0.22099792 0.36649102]]\n",
            "old bias: \n",
            " [[0.42107206 0.19752561]]\n",
            "new weight: \n",
            " [[0.85053475 0.60895954]\n",
            " [0.78699645 0.1083207 ]\n",
            " [0.28566234 0.09617979]\n",
            " [0.22099792 0.36649102]]\n",
            "new bias: \n",
            " [[0.42107206 0.19752561]]\n",
            "layer:  0  weight and bias updated\n",
            "===============================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRPwHKiEbK6S"
      },
      "source": [
        "Prediction using sample data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGtzAIl1tP5e",
        "outputId": "33514c9b-cce6-49a0-cfe3-8d24a67155eb"
      },
      "source": [
        "example = np.random.rand(4)\r\n",
        "print(\"input data: \", example)\r\n",
        "output = (model.model_feed_forward(example) > 0).astype(int)\r\n",
        "print(\"label: \", output)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input data:  [0.02182876 0.82938546 0.31999545 0.81677998]\n",
            "label:  [[1]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}