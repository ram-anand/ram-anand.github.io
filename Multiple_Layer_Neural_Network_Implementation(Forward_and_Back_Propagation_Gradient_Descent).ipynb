{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multiple Layer Neural Network Implementation(Forward and Back Propagation-Gradient Descent).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO/pP7EDJFU+kRY0rgmnQ0I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ram-anand/ram-anand.github.io/blob/main/Multiple_Layer_Neural_Network_Implementation(Forward_and_Back_Propagation_Gradient_Descent).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdR2clIYsC1t"
      },
      "source": [
        "## Custom Neural Network\r\n",
        "\r\n",
        "Implemented a neural network \r\n",
        "  -  supports multiple layers\r\n",
        "  -   custom loss functions (MSE, MLE and MAE)\r\n",
        "  -  batch gradient descent\r\n",
        "  - RELU, SIGMOID and TANH activations\r\n",
        "  - Simple structure for forward and back propagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKn2ZDysY5Nn"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1T4_7_SBr8bb"
      },
      "source": [
        "import numpy as np\r\n",
        "import math"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPRihC86r9Kf"
      },
      "source": [
        "class CustomNeuralNetwork:\r\n",
        "    def __init__(self):\r\n",
        "        self.n = 0 # number of samples\r\n",
        "        self.m = 0 # number of variables\r\n",
        "        self.epoch = 1 # number of epochs\r\n",
        "        self.learning_rate = 0.001\r\n",
        "        self.X = None # X data in input examples\r\n",
        "        self.Y = None # Y labels in input examples\r\n",
        "        self.YP = 0.0\r\n",
        "        self.layer_count = 0\r\n",
        "        self.layer_variables = [] \r\n",
        "        self.layer_weights = []\r\n",
        "        self.layer_bias = []\r\n",
        "        self.layer_activation = []\r\n",
        "        self.layer_output = {}\r\n",
        "        self.layer_derivative = {}\r\n",
        "        self.training_loss = 0.0\r\n",
        "        self.loss_method = None\r\n",
        "        \r\n",
        "#     def model_input_labels(self, data, labels):\r\n",
        "#         self.n = np.array(data).shape[0] # number of input examples\r\n",
        "#         self.m = np.array(labels).shape[1] # number of variables\r\n",
        "#         self.X = data\r\n",
        "#         self.Y = labels\r\n",
        "        \r\n",
        "    def model_add_layer(self, input_shape, list_layerdepth_layertype_tuple=[(1,\"relu\")]):\r\n",
        "        self.n = input_shape[0]\r\n",
        "        self.m = input_shape[1]\r\n",
        "        for layerdepth, layertype in list_layerdepth_layertype_tuple:\r\n",
        "            if self.layer_count == 0:\r\n",
        "                self.layer_weights.append(np.random.rand(self.m, layerdepth))\r\n",
        "            else:\r\n",
        "                self.layer_weights.append(np.random.rand(self.layer_variables[self.layer_count-1], layerdepth))\r\n",
        "            self.layer_bias.append(np.random.rand(1, layerdepth))\r\n",
        "            self.layer_variables.append(layerdepth)\r\n",
        "            self.layer_activation.append(layertype)\r\n",
        "            self.layer_count += 1\r\n",
        "        print(\"total layer count: \", self.layer_count)\r\n",
        "        print(\"layers: \", list_layerdepth_layertype_tuple)\r\n",
        "           \r\n",
        "    def relu(self, a):\r\n",
        "        b = np.maximum(a, 0.0)\r\n",
        "        #a[a < 0.0] = 0.0\r\n",
        "        c = np.greater(a, 0.0).astype(int)\r\n",
        "        return b, c  # data, derivative\r\n",
        "\r\n",
        "    def sigmoid(self, a):\r\n",
        "        b = 1/np.exp(-a)\r\n",
        "        return b, b * (1-b) # data, derivative \r\n",
        "\r\n",
        "    def tanh(self, a):\r\n",
        "        b = (np.exp(a) - np.exp(-a))/(np.exp(a) + np.exp(-a))\r\n",
        "        return b, (1-b**2) # data, derivative         \r\n",
        "\r\n",
        "    def model_feed_forward(self, data):\r\n",
        "        # calculate output for each layer: from 0th (first) layer to last layer\r\n",
        "        print(\"input data: \\n\", data)\r\n",
        "        for layer in range(self.layer_count):\r\n",
        "            print(\"-------------------------------------------------\")\r\n",
        "            print(\"layer: \", layer+1, \" of \", self.layer_count)\r\n",
        "#             print(\"layer variables: \", self.layer_variables[layer])\r\n",
        "#             print(\"layer weight: \\n\", self.layer_weights[layer])\r\n",
        "#             print(\"layer bias: \\n\", self.layer_bias[layer])\r\n",
        "            self.X = data\r\n",
        "            if layer == 0:\r\n",
        "                z = (np.dot(self.X, self.layer_weights[layer]) + self.layer_bias[layer])#.reshape(self.n, self.layer_variables[layer])\r\n",
        "                print(\"layer sum: \\n\", z)\r\n",
        "            else:\r\n",
        "                z = (np.dot(self.layer_output[layer-1], self.layer_weights[layer]) + self.layer_bias[layer])#.reshape(self.n, self.layer_variables[layer])\r\n",
        "                print(\"layer sum: \\n\", z)\r\n",
        "            if self.layer_activation[layer] == \"relu\":\r\n",
        "                self.layer_output[layer] = self.relu(z)[0]\r\n",
        "                self.layer_derivative[layer] = self.relu(z)[1]\r\n",
        "                print(\"layer output: \\n\", self.relu(z)[0])\r\n",
        "                #print(\"layer output: \\n\", self.layer_output[layer])\r\n",
        "            elif self.layer_activation[layer] == \"sigmoid\":\r\n",
        "                self.layer_output[layer] = self.sigmoid(z)[0]\r\n",
        "                self.layer_derivative[layer] = self.sigmoid(z)[1]\r\n",
        "                print(\"layer output: \\n\", self.sigmoid(z)[0])\r\n",
        "#             print(\"layer output: \\n\", self.layer_output[layer])\r\n",
        "            elif self.layer_activation[layer] == \"tanh\":\r\n",
        "                self.layer_output[layer] = self.tanh(z)[0]\r\n",
        "                self.layer_derivative[layer] = self.tanh(z)[1]\r\n",
        "                print(\"layer output: \\n\", self.tanh(z)[0])\r\n",
        "#             print(\"layer output: \\n\", self.layer_output[layer])\r\n",
        "            else:\r\n",
        "                print(\"activation type : \", self.layer_activation[layer], \"is undefined, use predefine activations - relu, sigmoid or tanh\")\r\n",
        "                return None\r\n",
        "        return self.layer_output[layer]\r\n",
        "        \r\n",
        "    def model_training(self, data=None, labels=None, losstype = \"mse\", learning_rate = 0.01, epoch=1):\r\n",
        "\r\n",
        "        if (data is None):\r\n",
        "            print(\"please provide sample data in data=X, labels=Y form in model training\")\r\n",
        "        else:\r\n",
        "            self.X = data\r\n",
        "            if (self.n != np.array(data).shape[0] or self.m != np.array(data).shape[1]):\r\n",
        "                print(\"reconfigure layer required as data dimension \",(self.n, self.m) ,\" did not match with layer input dimension \", (np.array(data).shape[0], np.array(data).shape[1]))\r\n",
        "\r\n",
        "#             self.n = np.array(data).shape[0] # number of input examples\r\n",
        "#             self.m = np.array(labels).shape[1] # number of variables\r\n",
        "                      \r\n",
        "        if (labels is None):\r\n",
        "            print(\"please provide sample data in data=X, labels=Y form in model training\")\r\n",
        "        else:\r\n",
        "            self.Y = labels\r\n",
        "\r\n",
        "        self.loss_method = losstype\r\n",
        "        self.learning_rate = learning_rate\r\n",
        "        self.epoch = epoch\r\n",
        "        for i in range(self.epoch): \r\n",
        "            print(\"************************************************\")\r\n",
        "            print(\"training epoch: \", i)\r\n",
        "            print(\"************************************************\")\r\n",
        "            \r\n",
        "            print(\"input dim: \",(self.n, self.m), \", input data: \\n\", self.X)\r\n",
        "            print(\"label dim: \",(self.n, 1), \", labels: \\n\", self.Y)\r\n",
        "\r\n",
        "            # feed foreward output at last layer (output layer)\r\n",
        "            self.YP = self.model_feed_forward(self.X)\r\n",
        "            \r\n",
        "            # calculating error \r\n",
        "            print((\"===============================================\"))\r\n",
        "            if self.loss_method == \"mle\":\r\n",
        "                self.training_loss = (1/self.n) * np.sum(-self.Y * np.log(self.YP) - (1-self.Y) * np.log(1+self.YP))\r\n",
        "                self.training_derivative = ((-self.Y/self.YP) + (1-self.Y)/(1-self.YP)).reshape(self.n,1)\r\n",
        "            elif self.loss_method == \"mae\":\r\n",
        "                self.training_loss = (1/self.n) * np.sum(np.abs(self.Y - self.YP))\r\n",
        "                self.training_derivative = 1\r\n",
        "            elif self.loss_method == \"mse\":\r\n",
        "                #self.training_loss = (0.5/self.n) * np.sum(np.dot((self.Y - self.YP).T, (self.Y - self.YP)))\r\n",
        "                #self.training_loss = (0.5/self.n) * ((self.Y - self.YP)**2).mean(axis=0)\r\n",
        "                self.training_loss = (0.5/self.n) * (np.square(self.Y - self.YP)).mean(axis=0)\r\n",
        "                self.training_derivative = -(1/self.n) * np.sum(np.abs(self.Y - self.YP))\r\n",
        "            else:\r\n",
        "                print(\"loss type : \", self.loss_method, \"is undefined, use predefine loss methods - mae, mle or mse\")\r\n",
        "                break\r\n",
        "            \r\n",
        "            print(self.loss_method, \" - training loss: \", self.training_loss)\r\n",
        "            print(\"===============================================\")\r\n",
        "            \r\n",
        "            # derivative for each layer : from last layer to 0th (first) layer\r\n",
        "            for layer in range(self.layer_count-1,-1,-1):\r\n",
        "                if layer == self.layer_count-1:\r\n",
        "                    self.layer_derivative[layer] = self.training_derivative * self.layer_derivative[layer]\r\n",
        "                    #self.layer_derivative[layer] = self.training_derivative * (self.layer_output[layer] * (1 - self.layer_output[layer]))\r\n",
        "                else:\r\n",
        "                    # current layer derivative = next layer derivative x next layer weight.transpose * (current layer output *(1 - current layer output))\r\n",
        "                    self.layer_derivative[layer] = np.dot(self.layer_derivative[layer+1], self.layer_weights[layer+1].T) * self.layer_derivative[layer]\r\n",
        "                    #self.layer_derivative[layer] = np.dot(self.layer_derivative[layer+1], self.layer_weights[layer+1].T) * (self.layer_output[layer] * (1 - self.layer_output[layer]))\r\n",
        "            \r\n",
        "            # weight and bias update at each layer : from last layer to 0th(first) layer\r\n",
        "            for layer in range(self.layer_count-1,-1,-1):  \r\n",
        "                if layer == 0:\r\n",
        "                    print(\"old weight: \\n\", self.layer_weights[layer])\r\n",
        "                    print(\"old bias: \\n\", self.layer_bias[layer])\r\n",
        "                    # current layer weights = current layer weights - (current layer input.transpose x current layer derivative)\r\n",
        "                    self.layer_weights[layer] = self.layer_weights[layer] - (self.learning_rate/self.n) * np.dot(self.X.T, self.layer_derivative[layer])\r\n",
        "                    self.layer_bias[layer] = self.layer_bias[layer] - (self.learning_rate/self.n) * np.sum(self.layer_derivative[layer], keepdims=True)                    \r\n",
        "                    print(\"new weight: \\n\", self.layer_weights[layer])\r\n",
        "                    print(\"new bias: \\n\", self.layer_bias[layer])\r\n",
        "                    print(\"layer: \", layer, \" weight and bias updated\")\r\n",
        "                    print(\"===============================================\")\r\n",
        "                else:\r\n",
        "                    print(\"old weight: \\n\", self.layer_weights[layer])\r\n",
        "                    print(\"old bias: \\n\", self.layer_bias[layer])\r\n",
        "                    # current layer weights = current layer weights - (current layer input.transpose x current layer derivative)\r\n",
        "                    self.layer_weights[layer] = self.layer_weights[layer] - (self.learning_rate/self.n) * np.dot(self.layer_output[layer-1].T, self.layer_derivative[layer])\r\n",
        "                    self.layer_bias[layer] = self.layer_bias[layer] - (self.learning_rate/self.n) * np.sum(self.layer_derivative[layer], keepdims=True)\r\n",
        "                    print(\"new weight: \\n\", self.layer_weights[layer])\r\n",
        "                    print(\"new bias: \\n\", self.layer_bias[layer])\r\n",
        "                    print(\"layer: \", layer, \" weight and bias updated\")\r\n",
        "                    print(\"===============================================\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F215OwtssX5I"
      },
      "source": [
        "Train model using training data and labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_g6OMXwsXHx",
        "outputId": "ce356ac5-7784-4732-8076-1922bc288e60"
      },
      "source": [
        "number_of_samples = 5\r\n",
        "number_of_variables = 4\r\n",
        "inputs = np.random.rand(number_of_samples, number_of_variables)\r\n",
        "labels = np.random.randint(0,2, (number_of_samples, 1))\r\n",
        "\r\n",
        "for row, label in zip(inputs, labels):\r\n",
        "  print(\"input data: \", row, \"label: \", label)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input data:  [0.54809226 0.54372535 0.80092463 0.81469276] label:  [1]\n",
            "input data:  [0.22277819 0.51712384 0.4016781  0.08952102] label:  [0]\n",
            "input data:  [0.98859575 0.34532742 0.16519472 0.38896186] label:  [0]\n",
            "input data:  [0.86346987 0.7173575  0.83334805 0.40107228] label:  [1]\n",
            "input data:  [0.55170577 0.00600045 0.09947874 0.93172151] label:  [1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQoUZ_8_sldo",
        "outputId": "3850a1d1-3510-49b8-87e7-d72788731424"
      },
      "source": [
        "model = CustomNeuralNetwork()\r\n",
        "model.model_add_layer((5,4), [(2, \"tanh\"),(2, \"relu\"),(2, \"relu\"),(1, \"tanh\")])\r\n",
        "model.model_training(inputs, labels, learning_rate=0.1, losstype = \"mle\", epoch=2)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total layer count:  4\n",
            "layers:  [(2, 'tanh'), (2, 'relu'), (2, 'relu'), (1, 'tanh')]\n",
            "************************************************\n",
            "training epoch:  0\n",
            "************************************************\n",
            "input dim:  (5, 4) , input data: \n",
            " [[0.54809226 0.54372535 0.80092463 0.81469276]\n",
            " [0.22277819 0.51712384 0.4016781  0.08952102]\n",
            " [0.98859575 0.34532742 0.16519472 0.38896186]\n",
            " [0.86346987 0.7173575  0.83334805 0.40107228]\n",
            " [0.55170577 0.00600045 0.09947874 0.93172151]]\n",
            "label dim:  (5, 1) , labels: \n",
            " [[1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]]\n",
            "input data: \n",
            " [[0.54809226 0.54372535 0.80092463 0.81469276]\n",
            " [0.22277819 0.51712384 0.4016781  0.08952102]\n",
            " [0.98859575 0.34532742 0.16519472 0.38896186]\n",
            " [0.86346987 0.7173575  0.83334805 0.40107228]\n",
            " [0.55170577 0.00600045 0.09947874 0.93172151]]\n",
            "-------------------------------------------------\n",
            "layer:  1  of  4\n",
            "layer sum: \n",
            " [[1.63246719 1.82893736]\n",
            " [0.75224142 0.89268844]\n",
            " [1.30077738 1.61531701]\n",
            " [1.65051489 1.90571426]\n",
            " [1.23941911 1.47067329]]\n",
            "layer output: \n",
            " [[0.92641213 0.94972201]\n",
            " [0.63648425 0.71271906]\n",
            " [0.86192315 0.92394191]\n",
            " [0.92892824 0.95672399]\n",
            " [0.84528984 0.89970581]]\n",
            "-------------------------------------------------\n",
            "layer:  2  of  4\n",
            "layer sum: \n",
            " [[1.26058774 2.22904174]\n",
            " [1.1306978  1.76697499]\n",
            " [1.23280633 2.15092771]\n",
            " [1.26191879 2.23757968]\n",
            " [1.224916   2.11467725]]\n",
            "layer output: \n",
            " [[1.26058774 2.22904174]\n",
            " [1.1306978  1.76697499]\n",
            " [1.23280633 2.15092771]\n",
            " [1.26191879 2.23757968]\n",
            " [1.224916   2.11467725]]\n",
            "-------------------------------------------------\n",
            "layer:  3  of  4\n",
            "layer sum: \n",
            " [[2.00202464 2.91230524]\n",
            " [1.67440565 2.59936549]\n",
            " [1.94564396 2.85400334]\n",
            " [2.00789553 2.9170966 ]\n",
            " [1.92033451 2.83158448]]\n",
            "layer output: \n",
            " [[2.00202464 2.91230524]\n",
            " [1.67440565 2.59936549]\n",
            " [1.94564396 2.85400334]\n",
            " [2.00789553 2.9170966 ]\n",
            " [1.92033451 2.83158448]]\n",
            "-------------------------------------------------\n",
            "layer:  4  of  4\n",
            "layer sum: \n",
            " [[3.3545603 ]\n",
            " [3.04160322]\n",
            " [3.29713119]\n",
            " [3.35951275]\n",
            " [3.27436509]]\n",
            "layer output: \n",
            " [[0.9975635 ]\n",
            " [0.99544868]\n",
            " [0.99726735]\n",
            " [0.99758748]\n",
            " [0.99714023]]\n",
            "===============================================\n",
            "mle  - training loss:  -0.2749860149172652\n",
            "===============================================\n",
            "old weight: \n",
            " [[0.18814932]\n",
            " [0.80308044]]\n",
            "old bias: \n",
            " [[0.63906536]]\n",
            "new weight: \n",
            " [[0.04421542]\n",
            " [0.58622867]]\n",
            "new bias: \n",
            " [[0.55951979]]\n",
            "layer:  3  weight and bias updated\n",
            "===============================================\n",
            "old weight: \n",
            " [[0.17096381 0.92706035]\n",
            " [0.66097055 0.41665826]]\n",
            "old bias: \n",
            " [[0.31317882 0.81491568]]\n",
            "new weight: \n",
            " [[0.15328067 0.85158315]\n",
            " [0.63166408 0.29156903]]\n",
            "new bias: \n",
            " [[0.23433089 0.73606774]]\n",
            "layer:  2  weight and bias updated\n",
            "===============================================\n",
            "old weight: \n",
            " [[0.41431776 0.84524616]\n",
            " [0.04121332 0.91562711]]\n",
            "old bias: \n",
            " [[0.83761756 0.57640422]]\n",
            "new weight: \n",
            " [[ 0.36806369  0.81791246]\n",
            " [-0.00931913  0.88576512]]\n",
            "new bias: \n",
            " [[0.73932771 0.47811437]]\n",
            "layer:  1  weight and bias updated\n",
            "===============================================\n",
            "old weight: \n",
            " [[0.69794801 0.90297646]\n",
            " [0.06767455 0.09477491]\n",
            " [0.60623203 0.54035349]\n",
            " [0.56446929 0.58502358]]\n",
            "old bias: \n",
            " [[0.26771571 0.37309454]]\n",
            "new weight: \n",
            " [[0.68701503 0.89839564]\n",
            " [0.05645628 0.08927466]\n",
            " [0.59827772 0.53635713]\n",
            " [0.56015928 0.58321453]]\n",
            "new bias: \n",
            " [[0.23211318 0.33749202]]\n",
            "layer:  0  weight and bias updated\n",
            "===============================================\n",
            "************************************************\n",
            "training epoch:  1\n",
            "************************************************\n",
            "input dim:  (5, 4) , input data: \n",
            " [[0.54809226 0.54372535 0.80092463 0.81469276]\n",
            " [0.22277819 0.51712384 0.4016781  0.08952102]\n",
            " [0.98859575 0.34532742 0.16519472 0.38896186]\n",
            " [0.86346987 0.7173575  0.83334805 0.40107228]\n",
            " [0.55170577 0.00600045 0.09947874 0.93172151]]\n",
            "label dim:  (5, 1) , labels: \n",
            " [[1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]]\n",
            "input data: \n",
            " [[0.54809226 0.54372535 0.80092463 0.81469276]\n",
            " [0.22277819 0.51712384 0.4016781  0.08952102]\n",
            " [0.98859575 0.34532742 0.16519472 0.38896186]\n",
            " [0.86346987 0.7173575  0.83334805 0.40107228]\n",
            " [0.55170577 0.00600045 0.09947874 0.93172151]]\n",
            "-------------------------------------------------\n",
            "layer:  1  of  4\n",
            "layer sum: \n",
            " [[1.57489058 1.7831589 ]\n",
            " [0.70482112 0.8514539 ]\n",
            " [1.24750214 1.57192269]\n",
            " [1.58906723 1.85815478]\n",
            " [1.19291046 1.43042742]]\n",
            "layer output: \n",
            " [[0.9178002  0.94503389]\n",
            " [0.60741902 0.69182826]\n",
            " [0.84758172 0.91733105]\n",
            " [0.92000619 0.95250804]\n",
            " [0.83147934 0.89175416]]\n",
            "-------------------------------------------------\n",
            "layer:  2  of  4\n",
            "layer sum: \n",
            " [[1.06832975 2.06587264]\n",
            " [0.95644936 1.5877273 ]\n",
            " [1.04274304 1.98390187]\n",
            " [1.06907204 2.07429729]\n",
            " [1.03705469 1.94807641]]\n",
            "layer output: \n",
            " [[1.06832975 2.06587264]\n",
            " [0.95644936 1.5877273 ]\n",
            " [1.04274304 1.98390187]\n",
            " [1.06907204 2.07429729]\n",
            " [1.03705469 1.94807641]]\n",
            "-------------------------------------------------\n",
            "layer:  3  of  4\n",
            "layer sum: \n",
            " [[1.70302272 2.24818383]\n",
            " [1.38384638 2.013496  ]\n",
            " [1.64732278 2.20249448]\n",
            " [1.70845805 2.25127232]\n",
            " [1.62382121 2.18720479]]\n",
            "layer output: \n",
            " [[1.70302272 2.24818383]\n",
            " [1.38384638 2.013496  ]\n",
            " [1.64732278 2.20249448]\n",
            " [1.70845805 2.25127232]\n",
            " [1.62382121 2.18720479]]\n",
            "-------------------------------------------------\n",
            "layer:  4  of  4\n",
            "layer sum: \n",
            " [[1.95276947]\n",
            " [1.80107622]\n",
            " [1.92352227]\n",
            " [1.95482036]\n",
            " [1.91351988]]\n",
            "layer output: \n",
            " [[0.96053424]\n",
            " [0.94691735]\n",
            " [0.95820654]\n",
            " [0.96069262]\n",
            " [0.95738005]]\n",
            "===============================================\n",
            "mle  - training loss:  -0.24287100830950914\n",
            "===============================================\n",
            "old weight: \n",
            " [[0.04421542]\n",
            " [0.58622867]]\n",
            "old bias: \n",
            " [[0.55951979]]\n",
            "new weight: \n",
            " [[-0.0658705 ]\n",
            " [ 0.43261346]]\n",
            "new bias: \n",
            " [[0.48637558]]\n",
            "layer:  3  weight and bias updated\n",
            "===============================================\n",
            "old weight: \n",
            " [[0.15328067 0.85158315]\n",
            " [0.63166408 0.29156903]]\n",
            "old bias: \n",
            " [[0.23433089 0.73606774]]\n",
            "new weight: \n",
            " [[0.15006015 0.80888405]\n",
            " [0.62593951 0.21567007]]\n",
            "new bias: \n",
            " [[0.18821755 0.68995441]]\n",
            "layer:  2  weight and bias updated\n",
            "===============================================\n",
            "old weight: \n",
            " [[ 0.36806369  0.81791246]\n",
            " [-0.00931913  0.88576512]]\n",
            "old bias: \n",
            " [[0.73932771 0.47811437]]\n",
            "new weight: \n",
            " [[ 0.3415276   0.80748391]\n",
            " [-0.03879881  0.87417975]]\n",
            "new bias: \n",
            " [[0.68777163 0.42655829]]\n",
            "layer:  1  weight and bias updated\n",
            "===============================================\n",
            "old weight: \n",
            " [[0.68701503 0.89839564]\n",
            " [0.05645628 0.08927466]\n",
            " [0.59827772 0.53635713]\n",
            " [0.56015928 0.58321453]]\n",
            "old bias: \n",
            " [[0.23211318 0.33749202]]\n",
            "new weight: \n",
            " [[0.68152762 0.89664029]\n",
            " [0.05080533 0.08714223]\n",
            " [0.59435988 0.53483443]\n",
            " [0.5581767  0.58258036]]\n",
            "new bias: \n",
            " [[0.21562679 0.32100562]]\n",
            "layer:  0  weight and bias updated\n",
            "===============================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGtzAIl1tP5e",
        "outputId": "2deea362-9de2-49fe-8ebb-c7c9715165c9"
      },
      "source": [
        "example = np.random.rand(4)\r\n",
        "print(\"input data: \", example)\r\n",
        "output = (model.model_feed_forward(example) > 0).astype(int)\r\n",
        "print(\"label: \", output)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input data:  [0.22473111 0.38303981 0.27818426 0.49010033]\n",
            "input data: \n",
            " [0.22473111 0.38303981 0.27818426 0.49010033]\n",
            "-------------------------------------------------\n",
            "layer:  1  of  4\n",
            "layer sum: \n",
            " [[0.82715186 0.99019288]]\n",
            "layer output: \n",
            " [[0.67894373 0.75744456]]\n",
            "-------------------------------------------------\n",
            "layer:  2  of  4\n",
            "layer sum: \n",
            " [[0.8902617  1.63693712]]\n",
            "layer output: \n",
            " [[0.8902617  1.63693712]]\n",
            "-------------------------------------------------\n",
            "layer:  3  of  4\n",
            "layer sum: \n",
            " [[1.34643398 1.76311124]]\n",
            "layer output: \n",
            " [[1.34643398 1.76311124]]\n",
            "-------------------------------------------------\n",
            "layer:  4  of  4\n",
            "layer sum: \n",
            " [[1.16043095]]\n",
            "layer output: \n",
            " [[0.82118027]]\n",
            "label:  [[1]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}