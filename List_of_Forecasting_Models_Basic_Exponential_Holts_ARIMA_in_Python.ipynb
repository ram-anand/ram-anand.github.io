{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "List of Forecasting Models Basic-Exponential-Holts-ARIMA in Python.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ram-anand/ram-anand.github.io/blob/main/List_of_Forecasting_Models_Basic_Exponential_Holts_ARIMA_in_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPafGSDSjIBG"
      },
      "source": [
        "### List of Forecasting methods with examples (Python) for Timeseries Data\r\n",
        "\r\n",
        "1. Naive Method\r\n",
        "2. Seasonal Naive Method\r\n",
        "3. Drift Method\r\n",
        "4. Simple Average\r\n",
        "5. Moving Average (MA)\r\n",
        "6. Weighted Moving Average (WMA)\r\n",
        "7. Single Exponential Smoothing (Only Level no trend or Seasonality)\r\n",
        "8. Holt Linear or Double Exponential Smoothing (Leval & Trend. No Seasonality)\r\n",
        "9. Holt Winters or Triple Exponential Smoothing (Leval, Trend, Seasonality)\r\n",
        "10. Auto Regressive (AR)\r\n",
        "11. Auto Regressive Moving Average (ARMA)\r\n",
        "12. Auto Regressive Integrated Moving Average (ARIMA)\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMhmiUeLjIBO"
      },
      "source": [
        "# A Series\r\n",
        "series = [3,10,12,13,12,10,12]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WXngtoxjIBP"
      },
      "source": [
        "### Naive Method\n",
        "\n",
        " - forecast value = last observation\n",
        " - $\\hat{y}_{t+h} = y_{t}$ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mo2d25DijIBQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vok-RSzYjIBQ"
      },
      "source": [
        "### Seasonal Naive Method\n",
        "\n",
        " - forecast value = last observed value from the same season\n",
        " - $\\hat{y}_{t+h} = y_{t+h-L}$ , Season Length = L\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uanvVHfujIBQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70EL8U2mjIBQ"
      },
      "source": [
        "### Drift Method\n",
        " - forecast value = last observation + drift * h\n",
        " - $\\hat{y}_{t+h} = y_{t} + h * c$ where $c = (y_{t} - y_{1})/(t - 1)$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YQN0FBPjIBR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LnH-402jIBR"
      },
      "source": [
        "### Simple Average\n",
        "- mean of all values in the series, each term gets **equal weight or probability**\n",
        "- $\\bar{y} = 1/t\\sum_{i=1}^{t}y_{i}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyHp5VEWjIBR"
      },
      "source": [
        "def average(series):\n",
        "    return float(sum(series))/len(series)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8DnEeDbjIBS",
        "outputId": "6ce439b4-f78e-422a-8ab0-6d9af92e3e16"
      },
      "source": [
        "# Given the above series, the average is:\n",
        "average(series)\n",
        "# 10.285714285714286"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10.285714285714286"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EZJL6OLjIBT"
      },
      "source": [
        "### Moving Average \n",
        "- mean of last n terms (sliding window = q), each term gets **equal weight or probability**\n",
        "- MA(q) = $1/q\\sum_{i=t-q}^{t}y_{i}$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MV8REbl4jIBT"
      },
      "source": [
        "# moving average using n last points\n",
        "def moving_average(series, n):\n",
        "    return average(series[-n:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUa04pHVjIBT",
        "outputId": "27785caa-29b0-42eb-8fff-6016e8a823a0"
      },
      "source": [
        "moving_average(series, 3)\n",
        "# 11.333333333333334"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11.333333333333334"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9SShDcqjIBU",
        "outputId": "e5b15441-eaa5-46c1-f190-98174609d268"
      },
      "source": [
        "moving_average(series, 4)\n",
        "# 11.75"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11.75"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sg6NLrFwjIBU"
      },
      "source": [
        "### Weighted Moving Average (WMA)\n",
        "- mean of all terms, each term gets **different weight or probability**\n",
        "- A weighted moving average is a moving average where within the sliding window values are given different weights, typically so that more recent points matter more.\n",
        "- mean of last n terms (sliding window = n), each term gets **equal weight or probability**\n",
        "- MA(q) = $1/q\\sum_{i=1}^{q}w_{i}*y_{t-q+i}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTYBw7JFjIBU"
      },
      "source": [
        "# weighted average, weights is a list of weights\n",
        "def weighted_average(series, weights):\n",
        "    result = 0.0\n",
        "    weights.reverse()\n",
        "    for n in range(len(weights)):\n",
        "        result += series[-n-1] * weights[n]\n",
        "    return result/len(weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbSMFauMjIBV",
        "outputId": "c522623a-4fff-438d-c188-8739dd23760b"
      },
      "source": [
        "weights = [0.1, 0.2, 0.3, 0.4]\n",
        "weighted_average(series, weights)\n",
        "# 11.5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11.500000000000002"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVhU178ojIBV"
      },
      "source": [
        "### Regression Based Forecast\n",
        "\n",
        " - Forecast = $\\hat{y}_{t} = β_{0}+β_{1}∗y_{1}+β_{2}∗y_{2}+...+β_{p}∗y_{p}$\n",
        " - Residuals/error sum of squares, SSE = sum of (actual-predicted)^2 = $\\sum_{i=1}^{t}(y_{i}-\\hat{y}_{i})^2$\n",
        " - Total sum of squares, SST = sum of (actual-mean)^2 = $\\sum_{i=1}^{t}(y_{i}-\\bar{y})^2$\n",
        " - Regression sum of squares, SSR = sum of (predicted-mean)^2 = $\\sum_{i=1}^{t}(\\hat{y}_{i}-\\bar{y})^2$\n",
        " - Coeffiecient of determination $R^{2} = SSR/SST = (1-SSE)/SST$, model with the highest value is better\n",
        " - Adjusted $\\bar{R}^{2} = MSR/MST = 1 - (MSE/MST)$, model with the highest value is better\n",
        " - $MSR = SSR/t, MSE = SSE/(t-p-1), MST = SST/(t-1)$, k = 1,2,..,t = number of observation considered for regression modelling \n",
        " - Residual analysis\n",
        "  - Standard deviation of the residuals of regression or residual standard error, $σ_{e} = \\sqrt{MSE}$\n",
        "  - average of the residuals should be zero, and that the correlation between the residuals should be zero\n",
        "  - ACF plot of residuals: should not show strong correlation among observations\n",
        "  - Breusch-Godfrey test or LM (Lagrange Multiplier) test for serial correlation specifically designed for use with regression models\n",
        "    - Null Hypothesis: No serial correlation in residuals\n",
        "  - Residual plots(scatterplot) against predictor: residuals should be randomly scattered without showing any systematic patterns\n",
        "  - Residuals plot against the fitted values should also show no pattern, If exist, there may be heteroscedasticity (variance not constant) \n",
        " - Akaike’s Information Criterion, $AIC = -t*log (SSE/t) + 2(p+2)$\n",
        " - Bayesian Information Criterion, $BIC = -t*log (SSE/t) + log(t)(p+2)$\n",
        " - Model with the minimum value of the AIC/BIC is often the best model for forecasting\n",
        " - For large values of t, minimising the AIC is equivalent to minimising the CV (MSE for leave-one-out cross-validation)\n",
        " - For large values of t, minimising BIC is similar to leave v-out cross-validation  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wOGvOrXjIBW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apGn5f4Aqeml"
      },
      "source": [
        "### Exponetial Smoothing\r\n",
        "\r\n",
        "1. Single Exponential Smoothing \r\n",
        "  - level only, no trend or seasonal component\r\n",
        "  - $\\hat{y}_{t} = forecast_{t} = level_{t} = α * value_{t-1} + (1-α)* level_{t-1}$\r\n",
        "2. Double Exponential Smoothing or Holts Linear method\r\n",
        "  - Trend is always additive, no seasonal component \r\n",
        "  - $\\hat{y}_{t} = forecast_{t} = level_{t} + trend_{t}$\r\n",
        "3. Triple Exponential Smoothing or Holt-Winters\r\n",
        "  - level, trend and seasonal component\r\n",
        "  - Additive seasonality $\\hat{y}_{t} = forecast_{t} = (level_{t} + trend_{t}) + seasonal_{t}$\r\n",
        "  - Multiplicative seasonality $\\hat{y}_{t} = forecast_{t} = (level_{t} + trend_{t}) * seasonal_{t}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTk67CGAjIBW"
      },
      "source": [
        "### Holt - Winters or Triple Exponential Smoothing \n",
        "\n",
        "---\n",
        "#### Additive Model - seasonal component adds (seasonal length = L)\n",
        "---\n",
        "${forecast}_{t+h} = level_{t} + h*trend_{t} + seasonal_{t−L+h}$\n",
        "> Forecast $\\hat{y}_{t+h} = ℓ_{t} + h*b_{t} + s_{t−L+h}$\n",
        "---\n",
        "${level}_{t} = α*(value_{t}−seasonal_{t−L})+(1−α)*(level_{t-1} + trend_{t−1})$\n",
        "> Level $ℓ_{t} = α*(y_{t}−s_{t−L})+(1−α)*(ℓ_{t-1} + b_{t−1})$ \n",
        "---\n",
        "$trend_{t} = β*(level_{t} − level_{t−1})+(1−β)*trend_{t−1}$\n",
        "> Trend $b_{t} = β*(ℓ_{t} − ℓ_{t−1})+(1−β)*b_{t−1}$\n",
        "---\n",
        "$seasonal_{t} = γ*(value_{t} − level_{t-1} - trend_{t-1})+(1−γ)*seasonal_{t−L}$ \n",
        "> Seasonal $s_{t} = γ*(y_{t} − ℓ_{t-1} - b_{t-1})+(1−γ)*s_{t−L}$\n",
        "---\n",
        "#### Multiplicative Model - seasonal component multiply (seasonal length = L)\n",
        "---\n",
        "${forecast}_{t+h} = (level_{t} + h*trend_{t}) * seasonal_{t−L+h}$\n",
        "> Forecast $\\hat{y}_{t+h} = (ℓ_{t} + h*b_{t}) * s_{t−L+h}$\n",
        "---\n",
        "${level}_{t} = α*value_{t}/seasonal_{t−L}+(1−α)*(level_{t-1} + trend_{t−1})$\n",
        "> Level $ℓ_{t} = α*y_{t}/s_{t−L}+(1−α)*(ℓ_{t-1} + b_{t−1})$ \n",
        "---\n",
        "$trend_{t} = β*(level_{t} − level_{t−1})+(1−β)*trend_{t−1}$\n",
        "> Trend $b_{t} = β*(ℓ_{t} − ℓ_{t−1})+(1−β)*b_{t−1}$ \n",
        "---\n",
        "$seasonal_{t} = γ*value_{t}/(level_{t-1} + trend_{t-1})+(1−γ)*seasonal_{t−L}$\n",
        "> Seasonal $s_{t} = γ*y_{t}/(ℓ_{t-1} + b_{t-1})+(1−γ)*s_{t−L}$\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5yOLqYljIBW"
      },
      "source": [
        "### Error Trend Seasonal Model (ETS state space model)\n",
        "\n",
        "---\n",
        "ETS Additive Model: Triple exponential smoothing with additive errors\n",
        "---\n",
        "${forecast}_{t} = level_{t-1} + trend_{t−1} + seasonal_{t−L} + error_{t}$\n",
        "> Forecast $\\hat{y}_{t} = ℓ_{t-1} + b_{t−1} + s_{t−L} + e_{t}$ \n",
        "---\n",
        "${level}_{t} = level_{t-1} + trend_{t−1} + α * error_{t}$\n",
        "> Level $ℓ_{t} = ℓ_{t-1} + b_{t−1} + α * e_{t}$ \n",
        "---\n",
        "${trend}_{t} = trend_{t−1} + β * error_{t}$\n",
        "> Trend $b_{t} = b_{t−1} + β * e_{t}$ \n",
        "---\n",
        "${seasonal}_{t} = seasonal_{t-L} + γ * error_{t}$  \n",
        "> Seasonal $s_{t} = s_{t−L} + γ * e_{t}$ \n",
        "---\n",
        "ETS Multiplicative Model : Triple exponential smoothing with multiplicative errors\n",
        "---\n",
        "${forecast}_{t} = (level_{t-1} + trend_{t−1} + seasonal_{t−L}) * (1 + error_{t})$\n",
        "> $\\hat{y}_{t} = (ℓ_{t-1} + b_{t−1} + s_{t−L}) * (1 + e_{t})$\n",
        "---\n",
        "${level}_{t} = level_{t-1} + trend_{t−1} + α * (level_{t-1} + trend_{t−1} + seasonal_{t-L}) * error_{t}$\n",
        "> $ℓ_{t} = ℓ_{t-1} + b_{t−1} + α * (ℓ_{t-1} + b_{t−1} + s_{t-1}) * e_{t}$ \n",
        "---\n",
        "${trend}_{t} = trend_{t−1} + β * (level_{t-1} + trend_{t−1} + seasonal_{t-L}) * error_{t}$\n",
        "> $b_{t} = b_{t−1} + β * (ℓ_{t-1} + b_{t−1} + s_{t-1}) * e_{t}$ \n",
        "---\n",
        "${seasonal}_{t} = seasonal_{t-L} + γ * (level_{t-1} + trend_{t−1} + seasonal_{t-L}) * error_{t}$  \n",
        "> $s_{t} = s_{t−L} + γ * (ℓ_{t-1} + b_{t−1} + s_{t-1}) * e_{t}$ \n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HT0WQX30jIBX"
      },
      "source": [
        "# given a series and alpha, return series of smoothed points\n",
        "def exponential_smoothing(series, alpha):\n",
        "    result = [series[0]] # first value is same as series\n",
        "    for n in range(1, len(series)):\n",
        "        result.append(alpha * series[n] + (1 - alpha) * result[n-1])\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SRfDVS-jIBX",
        "outputId": "be29ef24-6c7d-4afb-8e3b-a0e0111da63d"
      },
      "source": [
        "exponential_smoothing(series, 0.1)\n",
        "# [3, 3.7, 4.53, 5.377, 6.0393, 6.43537, 6.991833]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3, 3.7, 4.53, 5.377, 6.0393, 6.43537, 6.991833]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5sFaTU-jIBY",
        "outputId": "02b96e34-40a9-4502-df97-ba8c167e0e6f"
      },
      "source": [
        "exponential_smoothing(series, 0.9)\n",
        "# [3, 9.3, 11.73, 12.873000000000001, 12.0873, 10.20873, 11.820873]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3, 9.3, 11.73, 12.873000000000001, 12.0873, 10.20873, 11.820873]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acOcsvuojIBY"
      },
      "source": [
        "# given a series and alpha, return series of smoothed points\n",
        "def double_exponential_smoothing(series, alpha, beta):\n",
        "    result = [series[0]]\n",
        "    for n in range(1, len(series)+1):\n",
        "        if n == 1:\n",
        "            level, trend = series[0], series[1] - series[0]\n",
        "        if n >= len(series): # we are forecasting\n",
        "          value = result[-1]\n",
        "        else:\n",
        "          value = series[n]\n",
        "        last_level, level = level, alpha*value + (1-alpha)*(level+trend)\n",
        "        trend = beta*(level-last_level) + (1-beta)*trend\n",
        "        result.append(level+trend)\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzdHSdkBjIBY",
        "outputId": "a7729687-713b-4dc5-c342-a7a287ea9afb"
      },
      "source": [
        "double_exponential_smoothing(series, alpha=0.9, beta=0.9)\n",
        "# [3, 17.0, 15.45, 14.210500000000001, 11.396044999999999, 8.183803049999998, 12.753698384500002, 13.889016464000003]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3,\n",
              " 17.0,\n",
              " 15.45,\n",
              " 14.210500000000001,\n",
              " 11.396044999999999,\n",
              " 8.183803049999998,\n",
              " 12.753698384500002,\n",
              " 13.889016464000003]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqZBo8g8jIBZ"
      },
      "source": [
        "def triple_exponential_smoothing(series, slen, alpha, beta, gamma, n_preds):\n",
        "    result = []\n",
        "    seasonals = initial_seasonal_components(series, slen)\n",
        "    for i in range(len(series)+n_preds):\n",
        "        if i == 0: # initial values\n",
        "            smooth = series[0]\n",
        "            trend = initial_trend(series, slen)\n",
        "            result.append(series[0])\n",
        "            continue\n",
        "        if i >= len(series): # we are forecasting\n",
        "            m = i - len(series) + 1\n",
        "            result.append((smooth + m*trend) + seasonals[i%slen])\n",
        "        else:\n",
        "            val = series[i]\n",
        "            last_smooth, smooth = smooth, alpha*(val-seasonals[i%slen]) + (1-alpha)*(smooth+trend)\n",
        "            trend = beta * (smooth-last_smooth) + (1-beta)*trend\n",
        "            seasonals[i%slen] = gamma*(val-smooth) + (1-gamma)*seasonals[i%slen]\n",
        "            result.append(smooth+trend+seasonals[i%slen])\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uAGVQIojIBZ"
      },
      "source": [
        "def initial_trend(series, slen):\n",
        "    sum = 0.0\n",
        "    for i in range(slen):\n",
        "        sum += float(series[i+slen] - series[i]) / slen\n",
        "    return sum / slen\n",
        "def initial_seasonal_components(series, slen):\n",
        "    seasonals = {}\n",
        "    season_averages = []\n",
        "    n_seasons = int(len(series)/slen)\n",
        "    # compute season averages\n",
        "    for j in range(n_seasons):\n",
        "        season_averages.append(sum(series[slen*j:slen*j+slen])/float(slen))\n",
        "    # compute initial values\n",
        "    for i in range(slen):\n",
        "        sum_of_vals_over_avg = 0.0\n",
        "        for j in range(n_seasons):\n",
        "            sum_of_vals_over_avg += series[slen*j+i]-season_averages[j]\n",
        "        seasonals[i] = sum_of_vals_over_avg/n_seasons\n",
        "    return seasonals"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgGZcjdxjIBa",
        "outputId": "3cf7ef7f-0522-48a5-95d7-0738ab6a6df9"
      },
      "source": [
        "series = [30,21,29,31,40,48,53,47,37,39,31,29,17,9,20,24,27,35,41,38,\n",
        "          27,31,27,26,21,13,21,18,33,35,40,36,22,24,21,20,17,14,17,19,\n",
        "          26,29,40,31,20,24,18,26,17,9,17,21,28,32,46,33,23,28,22,27,\n",
        "          18,8,17,21,31,34,44,38,31,30,26,32]\n",
        "initial_trend(series, 12)\n",
        "# -0.7847222222222222"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.7847222222222222"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASZNB4yBjIBa",
        "outputId": "0be40770-53b0-42dc-94ab-b0de8102ce95"
      },
      "source": [
        "initial_seasonal_components(series, 12)\n",
        "# {0: -7.4305555555555545, 1: -15.097222222222221, 2: -7.263888888888888, 3: -5.097222222222222, 4: 3.402777777777778, 5: 8.069444444444445, 6: 16.569444444444446, 7: 9.736111111111112, 8: -0.7638888888888887, 9: 1.902777777777778, 10: -3.263888888888889, 11: -0.7638888888888887}\n",
        "# # forecast 24 points (i.e. two seasons)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: -7.4305555555555545,\n",
              " 1: -15.097222222222221,\n",
              " 2: -7.263888888888888,\n",
              " 3: -5.097222222222222,\n",
              " 4: 3.402777777777778,\n",
              " 5: 8.069444444444445,\n",
              " 6: 16.569444444444446,\n",
              " 7: 9.736111111111112,\n",
              " 8: -0.7638888888888887,\n",
              " 9: 1.902777777777778,\n",
              " 10: -3.263888888888889,\n",
              " 11: -0.7638888888888887}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xnf_zioojIBb",
        "outputId": "8b0fbb70-79b7-48fb-db60-bf6e47b8e3e6"
      },
      "source": [
        "triple_exponential_smoothing(series, 12, 0.716, 0.029, 0.993, 24)[:10]\n",
        "# [30, 20.34449316666667, 28.410051892109554, 30.438122252647577, 39.466817731253066, ..."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[30,\n",
              " 20.34449316666667,\n",
              " 28.410051892109554,\n",
              " 30.438122252647577,\n",
              " 39.466817731253066,\n",
              " 47.54961891047195,\n",
              " 52.52339682497974,\n",
              " 46.53453460769274,\n",
              " 36.558407328055765,\n",
              " 38.56283307754578]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGunP248uyMJ"
      },
      "source": [
        "### Stationary Series\r\n",
        " - No predictable patterns in the long-term, its properties do not depend on the time at which the series is observed\r\n",
        " - Time plots will show the series to be roughly horizontal (although some cyclic behaviour is possible), with constant mean and variance\r\n",
        "     - white noise series ( it is normally distributed mean 0 and constant variance) - Stationary\r\n",
        "     - a time series with cyclic behaviour (but with no trend or seasonality) - Stationary\r\n",
        "     - time series with trends, or with seasonality or both - Non Stationary\r\n",
        "\r\n",
        "#### LOG Transformation (base 10)\r\n",
        " - help to stabilise the variance of a time series\r\n",
        "\r\n",
        "#### Differencing \r\n",
        " - compute the differences between consecutive observations or with lagged observations\r\n",
        " - help stabilise the mean of a time series by removing changes in the level of a time series, and therefore eliminating (or reducing) trend and seasonality and making series stationary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMm7_t_ZuyGG"
      },
      "source": [
        "### PACF\r\n",
        " - if PACF (partial autocorrelation function) is zero for lag h>p then the series is in AR(p) process\r\n",
        "\r\n",
        "### ACF\r\n",
        "- ACVF : auto covariance function at lag h = covariance $(y_{t+h}, y_{t})$\r\n",
        "- ACF : auto correlation function at lag h = covariance $(y_{t+h}, y_{t})/covariance(y_{t+0}, y_{t})$\r\n",
        "- if ACVF/ACF is zero for lag h>q then the series is in MA(q) process\r\n",
        "\r\n",
        "### The ARMA, ARIMA model with lowest AIC or BIC is better\r\n",
        "\r\n",
        " - Akaike’s Information Criterion (AIC)= -2log L + 2(p+q)\r\n",
        " - Bayesian Information Criterion (BIC) = -2log L + log(n)(p+q)  \r\n",
        "  - L is the likelihood of the model \r\n",
        "  - p is the total number of parameters \r\n",
        "  - q is total number of initial states that have been estimated (including the residual variance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iUnCMbYux-J"
      },
      "source": [
        "### Random Walk Model\r\n",
        " - When the first differenced series is white noise, $y_{t} = y_{t-1}+ e_{t}$\r\n",
        " - Random walk models are widely used for non-stationary data, particularly financial and economic data \r\n",
        " - Random walks typically have:\r\n",
        "  - long periods of apparent trends up or down\r\n",
        "  - sudden and unpredictable changes in direction\r\n",
        " - The forecasts from a random walk model are equal to the last observation (naive forecast) $y_{t} = y_{t-1}$, as future movements are unpredictable\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uevFHYaquxvv"
      },
      "source": [
        "### AR: Auto Regressive \r\n",
        "Let us consider a series with values X0,X1,X2,X3,X4,X5…….\r\n",
        "\r\n",
        "Auto means its past values of the Time Series, regression means it follows regresson equation Y=a+b*X\r\n",
        " - AR(1), AR series of order = 1: $y_{3} = a + b*y_{2} + e$ \r\n",
        " - AR(2), AR series of order = 2: $y_{3} = a + b*y_{2} + c*y_{1} + e$ where a,b,c are constants\r\n",
        " - e = white noise drawn from normal distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdxlssXxvHi4"
      },
      "source": [
        "### MA: Moving Average\r\n",
        "\r\n",
        " - MA(1), MA series of order = 1: $y_{3} = (mean + e_{3}) + p*e_{2}$ \r\n",
        " - AR(2), MA series of order = 2: $y_{3} = (mean + e_{3}) + p*e_{2} + q*e_{1}$ where p,q are constants\r\n",
        " - e = white noise drawn from normal distribution\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ny4WqM3LvHcV"
      },
      "source": [
        "### ARMA : Auto Regressive Moving Average \r\n",
        " - if Order is AR=1, MA=2 than, $y_{3} = a + b*y_{2} + e + (mean + e_{3}) + p*e_{2}$\r\n",
        " - If order is AR=2, MA=1 than, $y_{3} = a + b*y_{2} + c*y_{1} + e + (mean + e_{3}) + p*e_{2}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nazkpoFYjIBb"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### ARIMA : AutoRegression Integrated MovingAverage\n",
        " - Here Integrated term refers to differencing/lag that is, for example, calculating 5th term with order 3, it would be 4th + (4th-3rd)+(3rd-2nd) terms.\n",
        " \n",
        " - If Order is AR=1,I=1,MA=1 than equation is, X3=a+b*X2+X2+c*E2\n",
        "    - AR terms is a+b*X2\n",
        "    - MA term is c*E2\n",
        "    - Difference term is X2\n",
        "    \n",
        " - If order is AR=1,I=2,MA=1 than equation is X3=a+b*X2+X2+(X2-X1)+c*E2\n",
        "    - AR terms is a+b*X2\n",
        "    - MA term is c*E2\n",
        "    - Difference term is X2+(X2-X1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHtK41DYvHVT"
      },
      "source": [
        ""
      ]
    }
  ]
}