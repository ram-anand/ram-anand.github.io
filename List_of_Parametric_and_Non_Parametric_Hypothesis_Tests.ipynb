{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "List of Parametric and Non Parametric Hypothesis Tests.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNOLAx6xxrOGT4ksY7kZwfX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ram-anand/ram-anand.github.io/blob/main/List_of_Parametric_and_Non_Parametric_Hypothesis_Tests.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSiTHOYBnqEV"
      },
      "source": [
        "## Introduction to Hypothesis Tests\r\n",
        "\r\n",
        "Gathering and analyzing samples instead of the entire population is often the most practical and cost effective way to make inferences about an entire population. From this sample data, we may be able get an estimation the actual population parameter with some degree of error. Or we can say the actual population parameter may lies within the confidence interval below and above the estimate. \r\n",
        "\r\n",
        "A statistical hypothesis is an assumption about a population parameter and hypothesis test is a formal statistical test we use to reject or fail to reject a statistical hypothesis.\r\n",
        "\r\n",
        "To test whether a statistical hypothesis about a population parameter is true, we obtain a random sample from the population and perform a hypothesis test on the sample data.\r\n",
        "\r\n",
        "<figure>\r\n",
        "<img src=\"https://www.dxbydt.com/wp-content/uploads/2015/09/inferential-statistics-sample-population.png\" width=\"350\"/>\r\n",
        "<img src=\"https://www.netquest.com/hs-fs/hubfs/sampling_image.jpg?width=804&name=sampling_image.jpg\" width=\"250\"/>\r\n",
        "<img src=\"https://www.six-sigma-material.com/images/xPopSamples.GIF.pagespeed.ic.v4-6UWWh-1.webp\" width=\"300\"/>\r\n",
        "</figure>\r\n",
        "\r\n",
        "There are two types of statistical hypotheses:\r\n",
        "- The null hypothesis: denoted as H0, assumes that the sample data occurs purely from chance. This hypothesis states that there is no effect of experiment on the variable of interest.\r\n",
        "- The alternative hypothesis: denoted as H1 or Ha, assumes the sample data is influenced by some non-random cause. This hypothesis states that there is an effect of experiment and there will be significant difference between the control group and treatment (experimental group).\r\n",
        "\r\n",
        "A statistical hypothesis can be one-tailed or two-tailed.\r\n",
        "- A one-tailed hypothesis tests if a statistic is less than or greater than some value.\r\n",
        "- A two-tailed hypothesis is used to test if the statistic is equal to or not equal to some value in its statement. The equal statement is always part of the null hypothesis.\r\n",
        "\r\n",
        "<img src=\"https://qphs.fs.quoracdn.net/main-qimg-8d3108b7ec80883512b6a6f925ddf7ad\" width=\"800\"/>\r\n",
        "\r\n",
        "**Dependent or Paired sample:**\r\n",
        "If the values in one sample affect the values in the other sample, then the samples are dependent.\r\n",
        "\r\n",
        "**Independent or unpaired sample:**\r\n",
        "If the values in one sample reveal no information about those of the other sample, then the samples are independent.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvdH5mD3L_k_"
      },
      "source": [
        "### Confidence Interval\r\n",
        "\r\n",
        "A confidence interval (CI) is how much uncertainty there is with any particular statistic. Confidence intervals are often used with a margin of error. \r\n",
        "\r\n",
        ">Confidence Interval, $CI = \\text{point estimate} \\pm \\text{error margin}$\\\r\n",
        "$\\text{error margin} = \\text{critical value}*\\text{standard error}$\\\r\n",
        "e.g. $CI = (\\mu-z_{\\alpha/2}*s.e., \\mu+z_{\\alpha/2}*s.e.)$\r\n",
        "\r\n",
        ">Point estimate: The point estimate of your confidence interval will be whatever statistical estimate you are making (e.g. population mean, the difference between population means, proportions, variation among groups).\r\n",
        "\r\n",
        "- The confidence interval (CI) is a range of values that’s likely to include a population value with a certain degree of confidence (confidence level). It is a range of values we are fairly sure our true value lies in.\r\n",
        "\r\n",
        "- A confidence interval is the mean of your estimate plus and minus the variation in that estimate. This is the range of values you expect your estimate to fall between if you redo your test, within a certain level of confidence.\r\n",
        "\r\n",
        "#### Confidence Level\r\n",
        "\r\n",
        "Confidence, in statistics, is another way to describe probability.\r\n",
        "The confidence level is the percentage of times you expect to reproduce an estimate between the upper and lower bounds (interval) of the confidence interval, and is set by the alpha value called significance level.\r\n",
        "\r\n",
        "> Confidence level, $CL = 1 − \\alpha$\r\n",
        "\r\n",
        "For a significance level of 0.05, or confidence level of 95% says that 95% of experiments like we just did will include the true mean, but 5% won't.\r\n",
        "\r\n",
        "So there is a 1-in-20 chance (5%) that our Confidence Interval does NOT include the true mean.\r\n",
        "\r\n",
        "For example, if you construct a confidence interval with a 95% confidence level, you are confident that 95 out of 100 times the estimate will fall between the upper and lower values specified by the confidence interval.\r\n",
        "\r\n",
        "#### Error Margin vs Sample size\r\n",
        "\r\n",
        "Error margin is the range above or below the mean estimate \r\n",
        "\r\n",
        "<figure>\r\n",
        "<img src=\"https://sites.google.com/site/hellobenchen/_/rsrc/1570154043247/home/wiki/math/marginoferr/margin_of_error.png\" width=\"450\"/>\r\n",
        "<img src=\"http://www.geoib.com/uploads/7/6/3/9/7639044/4776976.jpg\" width=\"450\"/>\r\n",
        "<center><figcaption> Source: (1) hellobenchen (2) geoib.com</figcaption></center>\r\n",
        "</figure>\r\n",
        "\r\n",
        "\r\n",
        ">$\\text{error margin} = \\text{critical value}*s.e.$,\\\r\n",
        "$s.e.=s.d./\\sqrt{n}$\r\n",
        "\r\n",
        "**sample size**, $n = (\\text{critical value})^2*(\\frac{\\text{standard deviation}}{\\text{error margin}})^2$\r\n",
        "\r\n",
        "<figure>\r\n",
        "<img src=\"https://statsandr.com/blog/chi-square-test-of-independence-by-hand_files/Screenshot%202020-01-28%20at%2000.56.28.png\"/>\r\n",
        "<center><figcaption> Source: statsandr.com</figcaption></center>\r\n",
        "</figure>\r\n",
        "\r\n",
        "To find the critical value, follow these steps.\r\n",
        "- Compute alpha: $\\alpha$ = 1 - (confidence level / 100)\r\n",
        "- Find the critical probability, $p* = 1 - \\alpha/2$.\r\n",
        "- Find the critical value (point on x-axis from pdf) having a cumulative probability equal to the critical probability $(p*)$\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0LlaTqPNmB6"
      },
      "source": [
        "### Types of Decision Errors\r\n",
        "\r\n",
        "There are two types of decision errors that one can make when doing a hypothesis test:\r\n",
        "\r\n",
        "<figure><center>\r\n",
        "<img src=\"https://i1.wp.com/statisticsbyjim.com/wp-content/uploads/2018/07/TypesErrorHypothesisTests.png?resize=600%2C400\" width=\"400\"/>\r\n",
        "<img src=\"https://www.researchgate.net/profile/Abdulkerim-Gok/publication/316927316/figure/fig3/AS:667699772391428@1536203439714/Left-Definitions-of-terminologies-in-a-statistical-test-Right-An-illustration-of-power.ppm\" width=\"450\"/><figcaption>source: (1) statisticsbyjim.com (2) Picture by Abdulkerim Gok</\r\n",
        "figcaption></center>\r\n",
        "</figure>\r\n",
        "\r\n",
        "||Do Not Reject H0|Reject H0|\r\n",
        "|---|---|---|\r\n",
        "|Reality|---|---|\r\n",
        "|H0 is True|Correct Decision|Type I Error ($\\alpha$)|\r\n",
        "|H0 is False|Type II Error ($\\beta$)|Correct Decision|\r\n",
        "\r\n",
        "\r\n",
        "- Type I error: You reject the null hypothesis when it is actually true. The probability of committing a Type I error is equal to the significance level, often called alpha ($\\alpha$)  and commonly set at 0.05.\r\n",
        "\r\n",
        "- Type II error: You fail to reject the null hypothesis when it is actually false. The probability of committing a Type II error is called the Beta, denoted as ($\\beta$). \r\n",
        "\r\n",
        "- Statistical significance: \r\n",
        "  - It is a term used by researchers to state that it is unlikely their observations could have occurred under the null hypothesis of a statistical test. \r\n",
        "  - Significance is usually denoted by a p-value, or probability value. Statistical significance is arbitrary – it depends on the threshold (significance level), or alpha value, chosen by the researcher. \r\n",
        "  - The most common threshold is p < 0.05, which means that the data is likely to occur less than 5% of the time under the null hypothesis. \r\n",
        "  - When the p-value falls below the chosen alpha value, then we say the result of the test is statistically significant.\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "  \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4i92e0LhJCG"
      },
      "source": [
        "### Statistical Power (sensitivity)\r\n",
        "\r\n",
        "Power of the test **$(1-\\beta)$** is the probability of a test to correctly reject null hypothesis H0. Power is the probability of avoiding a Type II error. The higher the statistical power of a test, the lower the risk of making a Type II error.\r\n",
        "\r\n",
        "\r\n",
        "A true effect is a real, non-zero relationship between variables in a population. An effect is usually indicated by a real difference between groups or a correlation between variables.\r\n",
        "\r\n",
        "Statistical power, or sensitivity, is the likelihood/chance of a significance test detecting an effect when there actually is one. High power in a study indicates a large chance of a test detecting a true effect. \r\n",
        "Low power means that your test only has a small chance of detecting a true effect or that the results are likely to be distorted by random and systematic error.\r\n",
        "\r\n",
        "Power is usually set at 80% or higher. This means that if there are true effects to be found in 100 different studies with 80% power, only 80 out of 100 statistical tests will actually detect them. \r\n",
        "\r\n",
        "If you don’t ensure sufficient power, your study may not be able to detect a true effect at all. This means that resources like time and money are wasted, and it may even be unethical to collect data from participants (especially in clinical trials).\r\n",
        "\r\n",
        "On the flip side, too much power means your tests are highly sensitive to true effects, including very small ones. This may lead to finding statistically significant results with very little usefulness in the real world.\r\n",
        "\r\n",
        "To balance these pros and cons of low versus high statistical power, you should use a power analysis to set an appropriate level.\r\n",
        "\r\n",
        "What is a power analysis?\r\n",
        "A power analysis is a calculation that helps you determine a minimum sample size for your study.\r\n",
        "\r\n",
        "A power analysis is made up of four main components. If you know or have estimates for any three of these, you can calculate the fourth component.\r\n",
        "\r\n",
        " A power analysis can be used to determine the necessary sample size for a study.\r\n",
        "\r\n",
        "### Factors influencing power\r\n",
        "\r\n",
        ">1. **Sample size**\r\n",
        "- It is the minimum number of observations needed to observe an effect of a certain size with a given power level.\r\n",
        "- Sample size is positively related to power. A small sample (less than 30 units) may only have low power while a large sample has high power.\r\n",
        "Increasing sample size improves power. But there is a point at which increasing your sample size may not yield high enough benefits.\r\n",
        "- Nonparametric tests can be subject to low power mainly due to small sample size. Therefore, it is important to consider the possibility of a Type II error when a nonparametric test fails to reject H0. There may be a true effect or difference, yet the nonparametric test is underpowered to detect it.\r\n",
        "- Your research design is also related to power and sample size:\r\n",
        "  - In a within-subjects design, each participant is tested in all treatments of a study, so individual differences will not unevenly affect the outcomes of different treatments.\r\n",
        "  - In a between-subjects design, each participant only takes part in a single treatment, so with different participants in each treatment, there is a chance that individual differences can affect the results.\r\n",
        "  A within-subjects design is more powerful, so fewer participants are needed. More participants are needed in a between-subjects design to establish relationships between variables.\r\n",
        "\r\n",
        ">2. **Significance level (alpha)**\r\n",
        "- It is the maximum risk of rejecting a true null hypothesis that you are willing to take, usually set at 5%.\r\n",
        "- Significance level is positively correlated with power, increasing the significance level (e.g., from 5% to 10%) increases power. When you decrease the significance level, your significance test becomes more conservative and less sensitive to detecting true effects.\r\n",
        "Increase the significance level. While this makes a test more sensitive to detecting true effects, it also increases the risk of making a Type I error.\r\n",
        "\r\n",
        ">3. **Expected effect size**\r\n",
        "- It is a standardized way of expressing the magnitude of the expected result of your study, usually based on similar studies or a pilot study.\r\n",
        "- Positviely correlated with effect size, To increase the expected effect in an experiment, you could manipulate your independent variable more widely (e.g., spending 1 hour instead of 10 minutes in nature) to increase the effect on the dependent variable (stress level). This may not always be possible because there are limits to how much the outcomes in an experiment may vary.\r\n",
        "\r\n",
        ">4. **Use a one-tailed test**\r\n",
        "- Instead of a two-tailed test, go for one tailed test. When using a t test or z tests, a one-tailed test has higher power. \r\n",
        "- However, a one-tailed test should only be used when there’s a strong reason to expect an effect in a specific direction (e.g., one mean score will be higher than the other), because it won’t be able to detect an effect in the other direction. In contrast, a two-tailed test is able to detect an effect in either direction.\r\n",
        "\r\n",
        "Other factors affecting Power\r\n",
        "\r\n",
        "\r\n",
        "5. **Variability**\r\n",
        "- The variability of the population characteristics affects the power of your test. High population variance reduces power. \r\n",
        "- In other words, using a population that takes on a large range of values for a variable will lower the sensitivity of your test, while using a population where the variable is relatively narrowly distributed will heighten the sensitivity of the test.\r\n",
        "- Using a fairly specific population with defined demographic characteristics can lower the spread of the variable of interest and improve power.\r\n",
        "\r\n",
        "\r\n",
        "6. **Measurement error**\r\n",
        "- The higher the measurement error in a study, the lower the statistical power of a test. Measurement error can be random or systematic. \r\n",
        "- Reduce measurement error can improve power. Increasing the precision and accuracy of your measurement devices and procedures reduces variability, improving reliability and power.  Using multiple measures or methods, known as triangulation, can also help reduce systematic bias.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OAKbnshTRED"
      },
      "source": [
        "### Effect size\r\n",
        "\r\n",
        "It is the quantitative measure of magnitude of experiment effect, it evaluating the strength of a statistical claim. \r\n",
        "The larger is effect size, the stronger is the relationship.\r\n",
        "Examples of effect sizes include \r\n",
        "\r\n",
        "- **Absoulte or unstandardized measure**: \r\n",
        "How large is the difference between the groups (the mean difference), \r\n",
        "These statistics describe the size of the effect, but remain in the original units of the variables gives better intrepretability.\r\n",
        "e.g., the difference between group means = $\\mu_{1}-\\mu_{2}$\r\n",
        "\r\n",
        "- **Standardized measure**:  \r\n",
        "Effect size is the difference in means between the two groups divided by the standard deviation of the control group. Here, the scores are standardized using standard deviation to remove the units of the variables in the effect.\r\n",
        "e.g., the standardized difference between group means = $\\frac{\\mu_{1}-\\mu_{2}}{\\sigma}$\r\n",
        "\r\n",
        "While statistical significance shows that an effect exists in a study, practical significance shows that the effect is large enough to be meaningful in the real world. Statistical significance is denoted by p-values, whereas practical significance is represented by effect sizes.\r\n",
        "\r\n",
        "Statistical significance alone can be misleading because it’s influenced by the sample size. Increasing the sample size always makes it more likely to find a statistically significant effect, no matter how small the effect truly is in the real world.\r\n",
        "\r\n",
        "In contrast, effect sizes are independent of the sample size. Only the data is used to calculate effect sizes.\r\n",
        "\r\n",
        "Standardized effect sizes help you evaluate how big or small an effect is when the units of measurement aren’t intuitive or can help you compare results across studies. It can be used in sample size calculations.\r\n",
        "\r\n",
        "Effect sizes complement statistical hypothesis testing, and play an important role in power analyses, sample size planning, and in meta-analyses.\r\n",
        "\r\n",
        "\r\n",
        "|Test|Effect size|Statistic for Effect size|Notes|Small|Medium|Large|\r\n",
        "|---|---|---|---|---|---|---|\r\n",
        "|One sample z-test|Cohen's D|$\\frac{\\mu_{1}-\\mu_{0}}{\\sigma}$|$\\mu_{1}$ = population mean, <br>$\\mu_{0}$ =  hypothesized population mean,<br>$\\sigma$ = population sd|0.2|0.5|0.8|\r\n",
        "|Two sample z-test|Cohen's D|$\\frac{\\mu_{1}-\\mu_{2}}{\\sigma}$|$\\mu_{1},\\mu_{2}$ = population mean, <br>$\\sigma$ = control group sd||||\r\n",
        "|One proportion z-test|Cohen's D|$\\frac{p_{1}-p_{0}}{\\sqrt{p_{0}(1-p_{0})}}$|$p_{1}$ = sample proportion, <br>$p_{0}$ = hypothesized population proportion||||\r\n",
        "|Two proportion z-test|Cohen's D|$\\frac{p_{1}-p_{2}}{\\sqrt{p(1-p)}}$|$p_{1}, p_{2}$ = sample proportions, <br>$p = \\frac{p_{1}+p_{2}}{2}$ = mean proportion||||\r\n",
        "|One sample t-test|Cohen's D|$\\frac{\\bar{x}-\\mu_{0}}{s}$|$\\bar{x}$ = sample mean,<br>$\\mu_{0}$ = hypothesized population mean,<br> $s = \\frac{\\sum(x-\\bar{x})^2}{n-1}$ = sample sd|||\r\n",
        "|Paired sample t-test|Cohen's D|$\\frac{\\bar{x}-\\bar{y}}{s_{d}} = \\frac{\\bar{d}}{s_{d}}$|$\\bar{d}$ = mean of difference,<br>$s_{d} = \\frac{\\sum(d_{i}-\\bar{d})^2}{n-1}$ = sd of difference|||\r\n",
        "|Paired sample t-test|Cohen's D|$\\frac{t_{d}}{\\sqrt{n}}$|$t_{d} = \\frac{\\bar{d}}{s_{d}/\\sqrt{n}}$ = t-statistic of difference,<br>$s_{d}$ = sd of difference|||\r\n",
        "|Student t-test|Cohen's D|$\\frac{\\bar{x}_{1}-\\bar{x}_{2}}{s_{pooled}} = t(\\frac{1}{n_{1}}+\\frac{1}{n_{2}})$|$\\bar{x}_{1}, \\bar{x}_{2}$ = sample means, <br> t = student t-statistic,<br>$s_{pooled}$ = pooled sample variance||||\r\n",
        "|Student t-test|r squared or eta squared|$\\frac{t^2}{t^2+df_{t}}$|t = student t-statistic,<br> degree of freedom, $df=n_{1}+n_{2}-2$||||\r\n",
        "|Correlation in two variables|r squared|$\\frac{\\sum(x-\\bar{x})(y-\\bar{y})}{\\sum(x-\\bar{x})^2}$|Measures degree of linear relationship <br>between two quantitative variables|0.2|0.5|0.8|\r\n",
        "|Non parametric test|r squared or eta squared|$\\frac{z^2}{n}$|$r^2=\\eta^2$,<br>z = standardized value of test <br>statistic, n = number of observations||||\r\n",
        "|Chi square goodness of fit|Cohen's W|$\\sqrt\\frac{\\chi^2}{N}$|||||\r\n",
        "|Chi square independance test|Phi|$\\sqrt\\frac{\\chi^2}{N}$|N=RC=total observations|0.1|0.3|0.5|\r\n",
        "|Chi square independance test|Cramer's V|$\\sqrt\\frac{\\chi^2}{N*df}$|$df=(R-1)(C-1)$|||||\r\n",
        "|Linear Regression|R squared|$\\frac{SSR}{SST}$|Proportion of variance in one <br>variable explained by the other|0.02|0.13|0.26|\r\n",
        "|Linear Regression|F squared|$\\frac{MSR}{MSE}$||0.02|0.15|0.35|\r\n",
        "|ANOVA|(Partial) Eta Squared, $\\eta^2$|$(\\frac{k-1}{N-k})*F$|$F=\\frac{MSB}{MSE}$|0.01|0.06|0.14|\r\n",
        "|ANOVA|(Partial) Eta Squared, $\\eta^2$|$\\frac{SSB}{SSB+SSE}$||0.01|0.06|0.14|\r\n",
        "|ANOVA|Omega Squared, $\\omega^2$|$\\frac{SSB-df_{SSB}*MSE}{SST+MSE}$||0.01|0.06|0.14|\r\n",
        "|ANOVA|Epsilon Squared, $\\epsilon^2$|$\\frac{SSB-df_{SSB}*MSE}{SST+MSE}$||||\r\n",
        "|ANOVA|Cohen's F|$\\sqrt{\\frac{R^2}{1-R^2}} = \\sqrt{\\frac{\\eta^2}{1-\\eta^2}} = \\sqrt{\\frac{\\omega^2}{1-\\omega^2}}$||0.10|0.25|0.40|\r\n",
        "|Between Groups|Odds ratio (OR) |$\\frac{odds_{group-1}}{odds_{group-2}}$||1.5|2|3|\r\n",
        "|Between groups|Relative risk or risk ratio (RR)|$\\frac{CR%-TR%}{CR%}$|CR% = control group risk %<br>TR% = treatment group risk %|2|3|4|\r\n",
        "|Mann Whitney U test |Rank biserial correlation, r|$(1-\\frac{2U}{n_{1}+n_{2}})$|U = Mann whitney test statistic<br>$n_{1}, n_{2}$ =sample sizes|0.2|0.5|0.8|\r\n",
        "|Friedman test|Kendall's W|$\\frac{\\chi^2}{n(k-1)}$|$\\chi^2$ = Friedman test statistic <br>n = sample size,<br> k = number of groups|0.1|0.3|0.5|\r\n",
        "|Kruskal-Wallis H-test|Eta Squared, $\\eta^2$|$\\frac{H-k+1}{n-k}$|H = Kruskal Wallis test statistic,<br>k = number of groups,<br>n = total observations|0.1|0.3|0.5|\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYOdBQwFiddW"
      },
      "source": [
        "### Sample size\r\n",
        "\r\n",
        "> sample size, $n_{i} = \\frac{z_{1-\\alpha/2}+z_{1-\\beta}}{ES}$\r\n",
        "where ES= effect size\r\n",
        "\r\n",
        "As sample size increases, the power of your test also increases. Therfore,larger sample means that you have collected more information, which makes it easier to correctly reject the null hypothesis when you should.\r\n",
        "\r\n",
        "To ensure that your sample size is big enough, you will need to conduct a power analysis calculation. \r\n",
        "\r\n",
        "For any power calculation, you will need to know:\r\n",
        "\r\n",
        "- What type of test you plan to use (e.g., independent t-test, paired t-test, ANOVA, regression, etc. See Step 6 if you are not familiar with these tests.),\r\n",
        "- The alpha value or significance level you are using (usually 0.01 or 0.05. See the next section of this page for more information.),\r\n",
        "- The expected effect size (See the last section of this page for more information.),\r\n",
        "- The sample size you are planning to use\r\n",
        "When these values are entered, a power value between 0 and 1 will be generated. If the power is less than 0.8, you will need to increase your sample size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5S5jOraFJUzX"
      },
      "source": [
        "### Steps in Hypothesis Tests\r\n",
        "A hypothesis test consists of five steps:\r\n",
        "\r\n",
        "1. State the hypotheses. \r\n",
        "\r\n",
        "State the null and alternative hypotheses. These two hypotheses need to be mutually exclusive, so if one is true then the other must be false.\r\n",
        "\r\n",
        "2. Determine a significance level to use for the hypothesis.\r\n",
        "\r\n",
        "Decide on a significance level. Common choices are .01, .05, and .1. \r\n",
        "\r\n",
        "3. Find the test statistic.\r\n",
        "\r\n",
        "Find the test statistic and the corresponding p-value. Often we are analyzing a population mean or proportion and the general formula to find the test statistic is: (sample statistic – population parameter) / (standard deviation of statistic)\r\n",
        "\r\n",
        "4. Reject or fail to reject the null hypothesis.\r\n",
        "\r\n",
        "Using the test statistic or the p-value, determine if you can reject or fail to reject the null hypothesis based on the significance level/cutoff level provided by the business.\r\n",
        "\r\n",
        "The p-value tells us the strength of evidence in support of a null hypothesis. If the p-value is less than the significance level, we reject the null hypothesis. Otherwise we fail to reject the null hypothesis.\r\n",
        "\r\n",
        "5. Interpret the results. \r\n",
        "\r\n",
        "Interpret the results of the hypothesis test in the context of the question being asked. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpGPaya8NmPa"
      },
      "source": [
        "### Choosing a statistical test\r\n",
        "\r\n",
        "There are broadely two types of statistical test:\r\n",
        "\r\n",
        "1.  Parametric test : \r\n",
        "\r\n",
        "- Parametric tests are more robust, and in general has higher power thus requires less data to make a stronger conclusion than nonparametric\r\n",
        "tests.\r\n",
        "  - the data need to be continuous\r\n",
        "  - the data need to be normally distributed (data points must follow a bellshaped curve).\r\n",
        "  - the data also need to have equal variance\r\n",
        "- Parametric tests involve estimation of the key parameters of the population/distribution (e.g., the mean or difference in means) from the sample data. \r\n",
        "\r\n",
        "2.  Non-parametric test : \r\n",
        "- If the data do not meet the criteria for a parametric test (normally distributed, equal variance, and continuous), it must be analyzed with a nonparametric test.\r\n",
        "- In nonparametric tests, the hypotheses are not about population parameters (e.g., $\\mu=50$ or $\\mu_{1}=\\mu_{2}$). Instead, the null hypothesis is more general.\r\n",
        "- The cost of fewer assumptions is that nonparametric tests are generally less powerful than their parametric counterparts (i.e., when the alternative is true, they may be less likely to reject H0).\r\n",
        "\r\n",
        "For example, when comparing two independent groups in terms of a continuous outcome, the null hypothesis in a parametric test is H0: μ1 =μ2. In a nonparametric test the null hypothesis is that the two populations are equal, often this is interpreted as the two populations are equal in terms of their central tendency.\r\n",
        "\r\n",
        "<figure><center>\r\n",
        "<img src=\"https://dzchilds.github.io/stats-for-bio/images/stats_key.svg\"/><figcaption>source: dzchilds.github.io</figcaption>\r\n",
        "<center></figure>\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77dwekKlOjiX"
      },
      "source": [
        "## Parametric Tests\r\n",
        "\r\n",
        "Assumptions: \r\n",
        "- Continuous: The data follow a continuous or ordinal scale.\r\n",
        "- Normality: Data have a normal distribution (or at least symmetric/bell shaped)\r\n",
        "- Homogeneity of variances: Data from multiple groups have the same variance\r\n",
        "- Independence: Data are independent (randomly selected)\r\n",
        "\r\n",
        "<img src=\"https://miro.medium.com/max/1050/1*XLhQMDdKW-3sd8a8HSFvgA.png\" width=\"450\"/>\r\n",
        "\r\n",
        "When to use z-test or t-test:\r\n",
        "1. Population standard deviation is known or Sample size > 30  \r\n",
        "- One sample z-test\r\n",
        "\r\n",
        "2. Population standard deviation is unknown and sample size < 30\r\n",
        "- One sample t-test\r\n",
        "- Two sample t-test\r\n",
        " - Unpaired t-test (independent samples)\r\n",
        "    - Equal population variance : Student t-test\r\n",
        "    - UnEqual population variance : Welch t-test\r\n",
        " - Paired t-test (dependent samples)\r\n",
        "\r\n",
        "Confidence Interval for test statistic:\r\n",
        ">$\\text{Point Estimate} \\pm \\text{(error margin)}$ where \\\r\n",
        "$\\text{error margin} = \\text{(critical value)} * \\text{(standard error)}$\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F89mAIBEpV8x"
      },
      "source": [
        "### Table of test statistic \r\n",
        "\r\n",
        "|Type of test|When to use|Sample size|Test statistic|Standard error|Degree of freedom|Confidence Interval|Notes or Comments|\r\n",
        "|---|---|---|---|---|---|---|---|\r\n",
        "|One sample z-test|Tests if average of a single sample is equal to <br/> target hypothesized mean|n|$z = \\frac{x-\\mu_{0}}{s.e.}$|$\\sigma/\\sqrt{n}$|-|$\\mu_{0} \\pm z*s.e.$|hypothesized population mean $\\mu_{0}$,<br/> Degree of freedom not defined|\r\n",
        "|One proportion z-test|Tests if proportion of a single sample is equal to <br> target hypothesized proportion|n|$z = \\frac{(p-p_{0})}{s.e.}$|$\\sqrt{\\frac{np_{0}(1-p_{0})}{n}}$|-|$\\mu \\pm z*s.e.$|min( $np_{0}, np_{0}(1-p_{1}))>5$, <br> sample proportion = $p$, <br> hypothesized mean = $np_{0}$, <br>hypothesized variance = $np_{0}(1-p_{0})$|\r\n",
        "|Two proportion z-test|Tests if difference between two proportion is equal to <br> target hypothesized proprtion|n|$z = \\frac{(p_{1}-p_{2})}{s.e.}$|$\\sqrt{p_{pooled}(1-p_{pooled})(\\frac{1}{n_{1}} + \\frac{1}{n_{2}})}$|-|$(p_{1}-p_{2}) \\pm z*s.e$|min( $np_{1}, np_{1}(1-p_{1}), np_{2}, np_{2}(1-p_{2}))>5$, <br> sample proportions = $p_{1}, p_{2}$, <br> $p_{pooled} = \\frac{p_{1}n_{1}+p_{2}n_{2}}{n_{1}+n_{2}}$|\r\n",
        "|One sample t-test|Tests if average of a single sample is equal to <br/> target hypothesized mean|n|$t = \\frac{\\bar{x}-\\mu}{s.e.}$|$s/\\sqrt{n}$|$n-1$|$\\mu \\pm t*s.e.$|sample mean = $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n}x_{i}$, <br/> sample variance = $s^2 = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_{i}-\\bar{x})^2$ |\r\n",
        "|Paired t-test|Tests if average of the differences between paired or<br/>  dependent samples is equal to zero (no difference)|$n_{1},n_{2}$|$t = \\frac{d-\\bar{d}}{s.e.}$|$s_{d}/\\sqrt{n}$|$n-1$|$\\bar{d} \\pm t*s.e.$|difference values, $d_{i}=x_{i}-y_{i}, \\bar{d} = \\sum_{i=1}^{n}d_{i}$, <br/> $s_{d}=\\frac{1}{n-1}\\sum_{i=1}^{n}(d_{i}-\\bar{d})^2$|\r\n",
        "|Student's t-test <br/> (equal population variance)|Tests if difference between the average of <br/> two independent samples is equal to target value|$n_{1},n_{2}$|$t = \\frac{(\\bar{x}_{1}-\\bar{x}_{2})-(\\mu_{1}-\\mu_{2})}{s.e.}$|$\\sqrt{s_{pooled}^2 * (\\frac{1}{n_{1}}+\\frac{1}{n_{2}})}$|$(n_{1}+n_{2}-2)$|$(\\bar{x}_{1}-\\bar{x}_{2}) \\pm t*s.e.$|pooled variance = $s_{pooled}^2 = \\frac{(n_{1}-1)s_{1}^2+(n_{2}-1)s_{2}^2}{(n_{1}+n_{2}-2)}$, <br> approx equal population variance, $\\mu_{1} = \\mu_{2}$|\r\n",
        "|Welch t-test <br/> (unequal population variance)|Tests if difference between the average of <br/> two independent samples is equal to target value|$n_{1},n_{2}$|$t = \\frac{(\\bar{x}_{1}-\\bar{x}_{2})-(\\mu_{1}-\\mu_{2})}{s.e.}$|$\\sqrt{(\\frac{s_{1}^2}{n_{1}}+\\frac{s_{2}^2}{n_{2}})}$|$\\frac{(\\frac{s_{1}^2}{n_{1}}+\\frac{s_{2}^2}{n_{2}})}{\\frac{(s_{1}^2/n_{1})^2}{n_{1}-1}+\\frac{(s_{2}^2/n_{2})^2}{n_{2}-1}}$|$(\\bar{x}_{1}-\\bar{x}_{2}) \\pm t*s.e.$|unequal population variance, $\\mu_{1} \\neq \\mu_{2}$|\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnVf53IAuwrd"
      },
      "source": [
        "### One sample z-test\r\n",
        "> Use to compare true mean of a population to a target value or a reference. \r\n",
        "1. Determine whether the population mean differs from the hypothesized mean that you specify.\r\n",
        "2. Calculate a range of values that is likely to include the population mean.\r\n",
        "- Two tailed z-test \\\r\n",
        "H0: The population mean (μ) equals the hypothesized mean (µ0) \\\r\n",
        "H1: μ ≠ µ0\tThe population mean (μ) differs from the hypothesized mean (µ0) \\\r\n",
        "- One tailed z-test \\\r\n",
        "H0: The population mean (μ) equals the hypothesized mean (µ0) \\\r\n",
        "H1: μ > µ0\tThe population mean (μ) is greater than the hypothesized mean (µ0).\r\n",
        "\r\n",
        "> Input\r\n",
        "1. sample size=$n$, \r\n",
        "2. hypothesized population mean under H0=$\\mu_{0}$, \r\n",
        "3. hypothesized population mean under H1=$\\mu_{1}$, \r\n",
        "4. sample mean = $\\bar{x}$, \r\n",
        "5. sample variance = $s^2$ \r\n",
        "6. population variance = $\\sigma^2$ \r\n",
        "7. For given significane level $\\alpha$, the critical value for two tailed = $z_{\\alpha/2}$\r\n",
        "\r\n",
        "As per central limit theorem, \r\n",
        " - the sample mean has a normal distribution, $\\bar{x} \\sim N(\\mu, \\sigma/\\sqrt{n})$ \r\n",
        " - or z score follows standard normal distribution,  $z \\sim N(0, 1)$ where $z = \\frac{x-mean}{s.e.} = \\frac{x-\\mu_{0}}{\\sigma/\\sqrt{n}}$\r\n",
        "\r\n",
        "- test statistic, $z = \\frac{\\bar{x}-\\mu}{\\sigma/\\sqrt{n}} \\sim N(0, 1)$ i.e. z-score follows standard normal distribution $N(0, 1)$\r\n",
        "\r\n",
        "- standard error, $ s.e. = \\sigma/\\sqrt{n}$\r\n",
        "\r\n",
        "- confidence interval = $\\mu_{0} \\pm z_{\\alpha/2}*s.e.$\r\n",
        "\r\n",
        "- effect size, $ES = \\frac{|\\mu_{1}-\\mu_{0}|}{\\sigma}$\r\n",
        "\r\n",
        "- Error, $e = z_{\\alpha/2}*s.e.$\r\n",
        "\r\n",
        "- Sample size, $n = (\\frac{z_{1-\\alpha/2} + z_{1-\\beta}}{ES})^2 = z_{\\alpha/2}^2*(\\frac{\\sigma }{e})^2$\r\n",
        "\r\n",
        "- cohen's $d = \\frac{\\bar{x}-\\mu}{\\sigma}$\r\n",
        "\r\n",
        "- p-value for $\\bar{x}$ at given significance level probability(SL), $p(\\bar{x}<x_{SL}|N(x;\\mu,\\sigma^2) = p(z<z_{SL}|N(z;0,1) = \\frac{1}{2\\pi}\\int_{-\\infty}^{z_{SL}}e^{-\\frac{z^2}{2}}dz$, where $z=\\frac{\\bar{x}-\\mu}{\\sigma/\\sqrt{n}}$\r\n",
        "\r\n",
        "Decision rule:\r\n",
        ">-  Reject H0 if ($z \\lt z_{\\alpha/2}$ or $z \\gt z_{\\alpha/2}$) or (p-value < SL)\r\n",
        "-  Do not reject H0 if ($z_{\\alpha/2} \\le z \\le z_{\\alpha/2}$) or (p-value >= SL)\r\n",
        "\r\n",
        "<figure>\r\n",
        "<img src=\"https://saylordotorg.github.io/text_introductory-statistics/section_12/ecf5f771ca148089665859c88d8679df.jpg\" width=\"500\"/>\r\n",
        "<figcaption>Source: saylordotorg.github.io</figcaption>\r\n",
        "</figure>\r\n",
        "\r\n",
        "Effect Size\r\n",
        ">|Cohen's d|Interpretation|\r\n",
        "|---|---|\r\n",
        "|0 - 0.2|Little or no effect|\r\n",
        "|0.2 - 0.5|Small effect size|\r\n",
        "|0.5 - 0.8|Medium effect size|\r\n",
        "|0.8 or more|Large effect size|\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "Effect sizes have several advantages over p-values:\r\n",
        "> 1. An effect size helps us get a better idea of how large the difference is between two groups or how strong the association is between two groups. A p-value can only tell us whether or not there is some significant difference or some significant association.\r\n",
        "2. Unlike p-values, effect sizes can be used to quantitatively compare the results of different studies done in different settings. For this reason, effect sizes are often used in meta-analyses.\r\n",
        "3. P-values can be affected by large sample sizes. The larger the sample size, the greater the statistical power of a hypothesis test, which enables it to detect even small effects. This can lead to low p-values, despite small effect sizes that may have no practical significance.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riDqQR3tOe_4"
      },
      "source": [
        "### One proportion z-test (Normal approximation method) - Dichotomous Outcome\r\n",
        "\r\n",
        "Use to estimate a binomial population proportion and to compare the proportion to a target value or a reference value\r\n",
        "- Determine whether the population proportion differs from the hypothesized proportion that you specify.\r\n",
        "- Calculate a range of values that is likely to include the population proportion.\r\n",
        "\r\n",
        "The sample mean of binomial distribution (distribution of sample proportions) approximation to normal distribution (CLT) is sufficiently accurate if \r\n",
        "\r\n",
        "Sample size constraint:\r\n",
        "> min( $np_{0}, np_{0}(1-p_{0}))>5$ \r\n",
        "\r\n",
        "where $p_{0}$ denotes the population proportion under H0, $p_{1}$ is the proportion under H1 and n is its related sample size.\r\n",
        "\r\n",
        "Hypotheses:\r\n",
        "- H0: $p = p_{0}$, sample proportion is same as hypothesized population proportion\r\n",
        "- H1: $p \\neq p_{0}$, sample proportion is significantly different from hypothesized population proportion for two tailed test\r\n",
        "\r\n",
        "z score, $z = \\frac{(p-p_{0})}{s.e.}$ follows normal distribution when \r\n",
        "\r\n",
        "- For given significane level $\\alpha$, the critical value for two tailed = $z_{\\alpha/2}$\r\n",
        "\r\n",
        "\r\n",
        "where\r\n",
        "\r\n",
        "- hypothesized mean = $np_{0}$, \r\n",
        "\r\n",
        "- hypothesized variance = $np_{0}(1-p_{0})$,\r\n",
        "\r\n",
        "- standard error, $s.e. = \\sqrt{\\frac{np_{0}(1-p_{0})}{n}}$\r\n",
        "\r\n",
        "- effect size, $ES = \\frac{p_{1}-p_{0}}{\\sqrt{p_{0}(1-p_{0})}}$\r\n",
        "\r\n",
        "- confidence interval, $p_{0} \\pm z_{\\alpha/2}*s.e.$\r\n",
        "\r\n",
        "- error, $e = z_{\\alpha/2}*s.e.$\r\n",
        "\r\n",
        "- sample size, $n = (\\frac{z_{1-\\alpha/2}+z_{1-\\beta}}{ES})^2 = z_{\\alpha/2}^2*(\\frac{np_{0}(1-p_{0})}{e})^2$\r\n",
        "\r\n",
        "Decision rule:\r\n",
        ">-  Reject H0 if ($z \\lt z_{\\alpha/2}$ or $z \\gt z_{\\alpha/2}$) or (p-value < SL)\r\n",
        "-  Do not reject H0 if ($z_{\\alpha/2} \\le z \\le z_{\\alpha/2}$) or (p-value >= SL)\r\n",
        "\r\n",
        "<figure>\r\n",
        "<img src=\"https://saylordotorg.github.io/text_introductory-statistics/section_12/01fe19537789cf83979f79f172b522c5.jpg\" width=\"500\"/>\r\n",
        "<figcaption>Source: saylordotorg.github.io</figcaption>\r\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7u4IbSrZbyXg"
      },
      "source": [
        "### Two proportion z-test (Independent proportions) - Dichotomous Outcome\r\n",
        "\r\n",
        "A two proportion z-test is used to test for a difference between two population proportions.\r\n",
        "\r\n",
        "- H0: $p_{1} = p_{2}$, the two population proportions are equal\r\n",
        "- H0: $p_{1} \\neq p_{2}$, the two population proportions are not equal (Two tailed) or first proportion is smaller than the second(One tailed), $p_{1} < p_{2}$\r\n",
        "\r\n",
        "For the risk difference, \r\n",
        "> H0: $p_{1} - p_{2} = 0$, versus H1: $p_{1} - p_{2} \\neq 0$, which are, by definition, equal to H0: $RD = 0$ versus H1: $RD \\neq 0$.\r\n",
        "\r\n",
        "For Risk ratio\r\n",
        "> If an investigator wants to focus on the risk ratio, the equivalent hypotheses are H0: $RR = 1$ versus H1: $RR \\neq 1$.\r\n",
        "\r\n",
        "for Odds ratio\r\n",
        "> If the investigator wants to focus on the odds ratio, the equivalent hypotheses are H0: $OR = 1$ versus H1: $OR \\neq 1$.\r\n",
        "\r\n",
        "Sample size constraint:\r\n",
        "> min($np_{1}, np_{1}(1-p_{1}), np_{2}, np_{2}(1-p_{2}))>5$ \r\n",
        "\r\n",
        "If $p_{1}$ and $p_{2}$ are the sample proportions, $n_{1}$ and $n_{2}$ are the sample sizes, and where p is the total pooled proportion calculated as:\r\n",
        "\r\n",
        "z score, $z = \\frac{(p_{1}-p_{2})}{s.e.}$ follows normal distribution when \r\n",
        "\r\n",
        "where\r\n",
        "\r\n",
        "- standard error, $s.e. = \\sqrt{p_{pooled}(1-p_{pooled})(\\frac{1}{n_{1}} + \\frac{1}{n_{2}})}$, where $p_{pooled} = \\frac{p_{1}n_{1}+p_{2}n_{2}}{n_{1}+n_{2}}$\r\n",
        "\r\n",
        "- For given significane level $\\alpha$, the critical value for two tailed = $z_{\\alpha/2}$\r\n",
        "\r\n",
        "- Confidence interval: $(p_{1}-p_{2}) \\pm z_{\\alpha/2}*s.e.$\r\n",
        "\r\n",
        "- error, $e = z_{\\alpha/2}*s.e.$\r\n",
        "\r\n",
        "- sample size, $n_{1} = n_{2} = ( p_{1}(1-p_{1})+p_{2}(1-p_{2}) )*(\\frac{z_{\\alpha/2}}{e})^2$\r\n",
        "\r\n",
        "Decision rule:\r\n",
        ">-  Reject H0 if ($z \\lt z_{\\alpha/2}$ or $z \\gt z_{\\alpha/2}$) or (p-value < SL)\r\n",
        "-  Do not reject H0 if ($z_{\\alpha/2} \\le z \\le z_{\\alpha/2}$) or (p-value >= SL)\r\n",
        "\r\n",
        "<figure>\r\n",
        "<img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2020/03/Screenshot-from-2020-03-03-18-01-53.png\" width=\"500\"/>\r\n",
        "<figcaption>Source: analyticsvidhya.com</figcaption>\r\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PRGof9Nv6O4"
      },
      "source": [
        "### One sample t-test\r\n",
        "\r\n",
        "Tests whether the mean of a single population is equal to a target value. \r\n",
        "\r\n",
        "$t = \\frac{\\text{Difference between groups (means)}}{\\text{Normal variability within group (or standard error SE)}} $\r\n",
        "\r\n",
        "1. Determine whether the population mean($\\mu$) differs from the hypothesized mean ($\\mu_{0}$). \r\n",
        "  - Two tailed: H0: $\\mu = \\mu_{0}$, H1: $\\mu \\neq \\mu_{0}$\r\n",
        "  - One tailed: H0: $\\mu = \\mu_{0}$, H1: $\\mu < \\mu_{0}$ or $\\mu > \\mu_{0}$\r\n",
        "\r\n",
        "2. Calculate a range of values that is likely to include the population mean.\r\n",
        "\r\n",
        "> Input\r\n",
        "1. sample size=$n$, \r\n",
        "2. hypothesized population mean under H0 =$\\mu_{0}$, \r\n",
        "3. sample mean = $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n}x_{i}$, \r\n",
        "4. sample variance = $s^2 = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_{i}-\\bar{x})^2$ \r\n",
        "\r\n",
        "- test statistic, $t = \\frac{\\bar{x}-\\mu_{0}}{s.e.} \\sim t_{n-1}$\r\n",
        "\r\n",
        "- standard error, $ s.e. = s/\\sqrt{n}$\r\n",
        "\r\n",
        "- degree of freedom = $(n-1)$\r\n",
        "\r\n",
        "- confidence interval = $\\mu \\pm t_{\\alpha/2}*s.e.$\r\n",
        "\r\n",
        "\r\n",
        "the test statistic, t follows a t-distribution of (n-1) degree of freedom.\r\n",
        "\r\n",
        "For given significance level $\\alpha$, critical value for two tailed = $t_{\\alpha/2}$\r\n",
        "\r\n",
        "\r\n",
        "Decision rule:\r\n",
        "-  Reject H0 if ($t \\lt -t_{\\alpha/2}$ or $t \\gt t_{\\alpha/2}$) or (p-value < SL)\r\n",
        "-  Do not reject H0 if ($t_{\\alpha/2} \\le t \\le t_{\\alpha/2}$) or (p-value >= SL)\r\n",
        "\r\n",
        "<figure>\r\n",
        "<img src=\"https://saylordotorg.github.io/text_introductory-statistics/section_12/ecf5f771ca148089665859c88d8679df.jpg\" width=\"450\"/>\r\n",
        "<img src=\"https://saylordotorg.github.io/text_introductory-statistics/section_12/37a4f201cad923d15a8bfab828bd7640.jpg\" width=\"450\"/>\r\n",
        "<figcaption>Source: saylordotorg.github.io</figcaption>\r\n",
        "</figure>\r\n",
        "\r\n",
        "- For given significane level $\\alpha$, the critical value for two tailed = $z_{\\alpha/2}$\r\n",
        "\r\n",
        "> Assumptions\r\n",
        "- The variable under study should be either an interval or ratio variable.\r\n",
        "- The observations in the sample should be independent.\r\n",
        "- The variable under study should be approximately normally distributed. You can check this assumption by creating a histogram and visually checking if the distribution has roughly a “bell shape.”\r\n",
        "- The variable under study should have no outliers. You can check this assumption by creating a boxplot and visually checking for outliers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Xf0LrFm5vgF"
      },
      "source": [
        "### Two sample t-test (Paired or Unpaired)\r\n",
        "\r\n",
        "Tests whether the difference between the true means of two independent populations is equal to a target value\r\n",
        "\r\n",
        "Determine whether the population means of two independent groups differ.\r\n",
        "Calculate a range of values that is likely to include the difference between the population means\r\n",
        "\r\n",
        "1. Paired or Matched\r\n",
        "2. Unpaired or Independent\r\n",
        " - Equal population variance : Student t-test\r\n",
        " - UnEqual population variance : Welch t-test\r\n",
        "\r\n",
        "Hypothesis:\r\n",
        "\r\n",
        "  - Two tailed: H0: $\\mu_{1} = \\mu_{2}$, H1: $\\mu \\neq \\mu_{0}$\r\n",
        "  - One tailed: H0: $\\mu_{1} = \\mu_{2}$, H1: $\\mu < \\mu_{0}$ or $\\mu_{1} > \\mu_{2}$\r\n",
        "\r\n",
        "> Input\r\n",
        "1. sample size of first sample = $n_{1}$, \r\n",
        "2. sample size of second sample = $n_{2}$, \r\n",
        "3. sample mean of first sample = $\\mu_{1}$, \r\n",
        "4. sample mean of second sample = $\\mu_{2}$, \r\n",
        "\r\n",
        "<img src=\"https://www.statology.org/wp-content/uploads/2020/04/confIntmeans1-1024x341.png\"/>\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6wbD0ly5w1K"
      },
      "source": [
        "#### Paired Sample t-test\r\n",
        "Tests whether the mean of the differences between dependent or paired observations is equal to a target value\r\n",
        "\r\n",
        "> Assumptions\r\n",
        "- Paired, Dependent or Matched samples: Each observation in one sample corresponds to a specific observation in the other sample. The two comparison groups are said to be dependent, and the data can arise from a single sample of participants where each participant is measured twice (possibly before and after an intervention) or from two samples that are matched on specific characteristics (e.g., siblings). \r\n",
        "- Equal sample size: Both sample should be of same size. \r\n",
        "- Normality: The data should be approximately normally distributed.\r\n",
        "\r\n",
        "If we have paired data (samples), both samples must be of the same size say $n$. Let x be the first sample with sample mean $\\bar{x}$ and  population mean $\\mu_{1}$ and y be the second sample with sample mean $\\bar{y}$  and population mean $\\mu_{2}$. Then x1 is paired with y1, x2 is paired with y2 etc. so for i=1,2,…,n, every $x_{i}$ is paired with $y_{i}$. \r\n",
        "\r\n",
        "Difference scores between the pairs, $d_{i} = x_{i}-y_{i}$ for each observation pairs$(x_{i}, y_{i})$,\r\n",
        "\r\n",
        "Hypotheses:\r\n",
        "- Null hypothesis(H0): No difference in the population means i.e. $\\mu_{d} = 0$\r\n",
        "- Alternate hypothesis(H0): Significant difference in the population means i.e. $\\mu_{d} \\neq 0$\r\n",
        "\r\n",
        "\r\n",
        "- test statistic,  $t = \\frac{\\bar{d}}{s_{d}/\\sqrt{n}}$\r\n",
        "\r\n",
        "- mean of paired differences, $\\bar{d} = \\frac{1}{n}\\sum_{1}^{n}(d_{i})$\r\n",
        "\r\n",
        "- variance of paired differences, $s_{d}^2 = \\frac{1}{n-1}\\sum_{1}^{n}(d_{i}-\\bar{d})^2$\r\n",
        "\r\n",
        "- confidence interval = $\\bar{d} \\pm t_{\\alpha/2}*s.e.$\r\n",
        "\r\n",
        "\r\n",
        "It can understood as doing one sample t-test of differenced values.\r\n",
        "\r\n",
        "NOTE:\r\n",
        "For this test to be valid the differences only need to be approximately normally distributed.\r\n",
        "Therefore, it would not be advisable to use a paired t-test where there were any extreme\r\n",
        "outliers.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9I4m11uikQN"
      },
      "source": [
        "#### Student's t-test (Independent samples and Equal Population Variance)\r\n",
        "\r\n",
        "this test assumes that both groups of data are sampled from populations that follow a normal distribution and that both populations have the same variance. Independent samples may occur, for instance, when the subjects in condition A are different from the subjects in condition B.\r\n",
        "\r\n",
        "> Assumptions\r\n",
        "- Independent or unpaired samples: The observations in one sample should be independent of the observations in the other sample. \r\n",
        "- Normality: The data should be approximately normally distributed.\r\n",
        "- Equal variance: The two samples should have approximately the same variance.\r\n",
        "\r\n",
        "test statistic, $ t =\\frac{(\\bar{x}_{1}-\\bar{x}_{2})-(\\mu_{1}-\\mu_{2})}{s.e.}$ follows t-distribution with df degree of freedom, where \r\n",
        "\r\n",
        "- standard error, $s.e. = \\sqrt{s_{pooled}^2 * (\\frac{1}{n_{1}}+\\frac{1}{n_{2}})}$\r\n",
        "\r\n",
        "- Pooled sample variance, $s_{pooled}^2 = \\frac{(n_{1}-1)s_{1}^2+(n_{2}-1)s_{2}^2}{(n_{1}+n_{2}-2)}$ \\\r\n",
        "\r\n",
        "- degree of freedom, $df = (n_{1}+n_{2}-2)$\r\n",
        "\r\n",
        "- confidence interval = $(\\bar{x}_{1} - \\bar{x}_{2}) \\pm t_{\\alpha/2}*s.e.$\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiTydfF86IKE"
      },
      "source": [
        "#### Welch’s t-test (Independent samples, Unequal Population Variance) \r\n",
        "\r\n",
        "this test assumes that both groups of data are sampled from populations that follow a normal distribution, but it does not assume that those two populations have the same variance.\r\n",
        "\r\n",
        "> Assumptions\r\n",
        "- Independent or unpaired samples: The observations in one sample should be independent of the observations in the other sample.\r\n",
        "- Normality: The data should be approximately normally distributed.\r\n",
        "- Unequal variance: The two samples should have unequal variance.\r\n",
        "\r\n",
        "test statistic,  $t = \\frac{(\\bar{x}_{1}-\\bar{x}_{2})-(\\mu_{1}-\\mu_{2})}{s.e.}$ ,where\r\n",
        "- standard error, $s.e. = \\sqrt{(\\frac{s_{1}^2}{n_{1}}+\\frac{s_{2}^2}{n_{2}})}$\r\n",
        "- degree of freedom, $df = \\frac{(\\frac{s_{1}^2}{n_{1}}+\\frac{s_{2}^2}{n_{2}})}{\\frac{(s_{1}^2/n_{1})^2}{n_{1}-1}+\\frac{(s_{2}^2/n_{2})^2}{n_{2}-1}}$\r\n",
        "\r\n",
        "The confidence interval for the difference in means $\\mu_{1}-\\mu_{2}$ is given by\r\n",
        "- confidence interval = $(\\bar{x}_{1}-\\bar{x}_{2}) \\pm t_{\\alpha/2}*s.e.$\r\n",
        "\r\n",
        "- error units, $e = t_{\\alpha/2}*s.e.$\r\n",
        "\r\n",
        "- sample size, $n_{1} = n_{2} = 2*(\\frac{z_{\\alpha/2}\\sigma}{e})^2$\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boiXzoIFZaTb"
      },
      "source": [
        "## Non-parametric Tests\r\n",
        "\r\n",
        "- One sample\r\n",
        " - Dichotomous data:\r\n",
        "   - Binomial test\r\n",
        " - Categorical data:\r\n",
        "   - Chi-square goodness-of-fit test\r\n",
        " - Quantitative data:\r\n",
        "   - Sign test for 1 median\r\n",
        "   - Kolmogorov-Smirnov test\r\n",
        "   - Shapiro-Wilk test\r\n",
        "- Two samples\r\n",
        " - Both dichotomous data:\r\n",
        "   - McNemar test\r\n",
        " - Both nominal data:\r\n",
        "   - Chi-square independence test\r\n",
        "   - Fisher’s Exact Test \r\n",
        " - Both ordinal data:\r\n",
        "   - Wilcoxon signed-ranks test\r\n",
        "   - Sign test for 2 related medians\r\n",
        "   - Mann-Whitney test (mean ranks)\r\n",
        "   - Median test for 2+ independent medians\r\n",
        "\r\n",
        "- Three or more samples\r\n",
        " - All dichotomous data:\r\n",
        "   - Cochran Q test \r\n",
        " - All nominal data:\r\n",
        "   - Chi-square independence test\r\n",
        " - All ordinal data:\r\n",
        "   - Friedman test\r\n",
        "   - Kruskal-Wallis test (mean ranks)\r\n",
        "   - Median test for 2+ independent medians"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoBF5Wc8iiOp"
      },
      "source": [
        "### Assigning Ranking\r\n",
        "\r\n",
        "- Ordinal ranking (\"1234\" ranking): assigns ascending rank to all observations in the ordered list\r\n",
        "- Fractional ranking (\"1 2.5 2.5 4\" ranking): assign rank to equal observations as mean of their ordinal  e.g rank for Rank(7) = (5+6)/2\r\n",
        "- Standard competition ranking (\"1224\" ranking): first observations are ordinally ranked, then assigns lowest ordinal rank to equal observations\r\n",
        "- Standard competition ranking (\"1334\" ranking): first observations are ordinally ranked, then assigns highest ordinal rank to equal observations, skips (n-1) ranks for n equal observations\r\n",
        "- Dense ranking (\"1223\" ranking): assigns lowest ordinal rank to equal observations and continued ranking without skipping any rank number\r\n",
        "\r\n",
        "||Data|0|2|3|5|7|7|9|10|\r\n",
        "|---|---|---|---|---|---|---|---|---|---|\r\n",
        "|Ordinal ranking|Rank|1|2|3|4|5|6|7|8|\r\n",
        "|Fractional ranking|Rank|1|2|3|4.5|4.5|6|7|8|\r\n",
        "|Standard ranking|Rank|1|2|3|4|5|5|7|8|\r\n",
        "|Dense ranking|Rank|1|2|3|4|5|5|6|7|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXyJmcjVszmO"
      },
      "source": [
        "### Mann-Whitney U test or Wilcoxon rank-sum test\r\n",
        "\r\n",
        "A Mann-Whitney U test (sometimes called the Wilcoxon rank-sum test) is used to compare the differences between two independent samples\r\n",
        "- to test whether two samples are likely to derive from the same population (i.e., that the two populations have the same shape)\r\n",
        "- when the sample distributions are not normally distributed and \r\n",
        "- the sample sizes are small (n<30) \r\n",
        "\r\n",
        "It is considered to be the nonparametric equivalent to the two-sample independent t-test.\r\n",
        "\r\n",
        "Assumptions:\r\n",
        "- Ordinal or Continuous: The variable you’re analyzing is ordinal or continuous. Examples of ordinal variables include Likert items (e.g., a 5-point scale from “strongly disagree” to “strongly agree”). Examples of continuous variables include height (measured in inches), weight (measured in pounds), or exam scores (measured from 0 to 100).\r\n",
        "- Independence: All of the observations from both groups are independent of each other.\r\n",
        "Shape: The shapes of the distributions for the two groups are roughly the same.\r\n",
        "\r\n",
        "<figure>\r\n",
        "<img src=\"https://www.statstest.com/wp-content/uploads/2020/02/mann-whitney-u-test.png\" width=\"450\"/>\r\n",
        "<figcaption>Source: statstest.com</figcaption>\r\n",
        "</figure>\r\n",
        "\r\n",
        "Hypotheses:\r\n",
        "- H0: The two populations are equal\r\n",
        "- H1: The two populations are not equal\r\n",
        "\r\n",
        "If there are two samples (groups) of ordinal data\r\n",
        "\r\n",
        "n1 = the sample sizes for sample 1 \r\n",
        "n2 = the sample sizes for sample 2\r\n",
        "\r\n",
        "Observations from two groups are ranked together using Fractional Ranking approach and then test statistic is calculated as below.\r\n",
        "\r\n",
        "R1 = Sum of the ranks in group 1 and \r\n",
        "R2 = Sum of the ranks in group 2 \r\n",
        "\r\n",
        "- $U_{1} = n_{1}*n_{2}  +  n_{1}*(n_{1}+1)/2 - R_{1}$\r\n",
        "- $U_{2} = n_{1}*n_{2}  +  n_{2}*(n_{2}+1)/2 - R_{2}$\r\n",
        "\r\n",
        "Mann Whitney U statistic, $U = min(U_{1}, U_{2})$ \r\n",
        "\r\n",
        "Always, $(U_{1} + U_{2}) = n_{1}*n_{2}$\r\n",
        "\r\n",
        "To determine the appropriate critical value we need sample sizes (n1, n2) and two-sided level of significance (e.g. 0.05).\r\n",
        "\r\n",
        "Decision rule: \r\n",
        ">- Reject H0 if U <= Critical value or p-value <= Significance level\r\n",
        "- Do not reject H0 if U > Critical value or p-value <= Significance level\r\n",
        "\r\n",
        "Note:\r\n",
        ">For any Mann-Whitney U test, \r\n",
        "- the theoretical range of U is from 0 (complete separation between groups, H0 most likely false and H1 most likely true) to n1*n2 (H1 most likely true , little evidence in support of H1).\r\n",
        "- Smaller values of U support the research hypothesis (i.e., we reject H0 if U is small). On the other hand large values support Null hypothesis. \r\n",
        "\r\n",
        "Example:\r\n",
        "\r\n",
        "|Ranking|1|2|3|4.5|4.5|6|7.5|7.5|9|10|Rank sum|\r\n",
        "|---|---|---|---|---|---|---|---|---|---|---|---|\r\n",
        "|Placebo|1|2|3|4|||6||||18|\r\n",
        "|New Drug|||||4|5||6|7|12|37|\r\n",
        "\r\n",
        "- $R_{1}$ = 16\r\n",
        "- $R_{2}$ = 24\r\n",
        "- $U_{1}$ = 5*5 + 5*6/2 - 18 = 22\r\n",
        "- $U_{2}$ = 5*5+5*6/2 - 37 = 3 \r\n",
        "- $U = min(U_{1}, U_{2})$ = 3\r\n",
        "\r\n",
        "Critical value for (n1=n2=5 and SL=0.05) = 2\r\n",
        "since U = 3 > critical value, hence Null hypothese that the two groups come from same population can not be rejected.\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47oU5ZKbt356"
      },
      "source": [
        "### Wilcoxon Signed Rank Test\r\n",
        "\r\n",
        "The Wilcoxon Signed Rank Test is the non-parametric version of the paired t-test to compare outcomes between two matched or paired groups. It is used to test whether or not there is a significant difference between two population means. It is used when the distribution of the differences between the pairs is severely non-normally distributed.\r\n",
        "\r\n",
        "Check for normality:\r\n",
        "The easiest way to determine if the differences are non-normally distributed is to create a histogram of the differences and see if they follow a somewhat normal, “bell-shaped” distribution.\r\n",
        "\r\n",
        "Keep in mind that the paired t-test is fairly robust to departures from normality, so the deviation from a normal distribution needs to be pretty severe to justify the use of the Wilcoxon Signed Rank test.\r\n",
        "<figure>\r\n",
        "<img src=\"https://www.statstest.com/wp-content/uploads/2020/11/Wilcoxon-Signed-Rank-Test.jpg\" width=\"450\"/>\r\n",
        "<figcaption>Source: statstest.com</figcaption>\r\n",
        "</figure>\r\n",
        "\r\n",
        "Steps to perform test:\r\n",
        "- Find the difference and absolute difference for each pair.\r\n",
        "- Order the pairs by the absolute differences and assign a rank (signed fractional rank) from the smallest to largest absolute differences. Ignore pairs that have an absolute difference of zero and assign mean ranks when there are ties.\r\n",
        "- Find the sum of the positive ranks and the negative ranks.\r\n",
        "\r\n",
        ">|Groups|1|2|3|4|5|6|7|8|\r\n",
        "|---|---|---|---|---|---|---|---|---|\r\n",
        "|Before treatment|85|60|70|75|95|80|85|80|\r\n",
        "|After treatment|70|70|65|80|75|70|75|85|\r\n",
        "|Difference|15|-10|5|-5|20|10|10|-5|\r\n",
        "|Ordinal Rank|7|4|1|2|8|5|6|3|\r\n",
        "|Fractional Rank|7|5|2|2|8|5|5|2|\r\n",
        "|Signed Fractional Rank|+7|-5|+2|-2|+8|+5|+5|-2|\r\n",
        "\r\n",
        "number of groups  = n = 8\r\n",
        "> Note: sum of the ranks (ignoring the signs) will always equal n(n+1)/2.\r\n",
        "\r\n",
        "Hypotheses:\r\n",
        "- Null hypothesis (H0): The median difference is zero \r\n",
        "- Alternate hypothesis (H1): The median difference is positive\r\n",
        "\r\n",
        "The test statistic for the Wilcoxon Signed Rank Test is W, defined as the smaller of W+ (sum of absolute values the positive ranks) and W- (sum of absolute values the negative ranks). \r\n",
        "\r\n",
        "> $W = min(W_{+}, W_{-})$\r\n",
        "\r\n",
        "We find the one-sided critical value for given sample size (n) and given evel of significance ($\\alpha$) from the table of Critical Values of W.\r\n",
        "\r\n",
        "Decision rule:\r\n",
        ">- Reject H0 if $W<=W_{\\alpha}$ or p-value<= SL\r\n",
        "- Do not reject H0 if $W>W_{\\alpha}$ or p-value> SL\r\n",
        "\r\n",
        "Inference:\r\n",
        "- If the null hypothesis is true, we expect to see similar numbers of lower and higher ranks that are both positive and negative (i.e., W+ and W- would be similar). \r\n",
        "- If the alternate hypothesis is true we expect to see more higher and positive ranks (W+ much larger than W-)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF-7pC25bCZL"
      },
      "source": [
        "### Sign Test\r\n",
        "\r\n",
        "The Sign Test is the simplest nonparametric test for matched or paired data. \r\n",
        "- The test usage the signs of the difference scores between two matched (paired) groups to test whether both groups are related or not.\r\n",
        "- It do not account for the magnitude of those differences.  \r\n",
        "- It is used when the distribution of the differences between the pairs is severely non-normally distributed.\r\n",
        "\r\n",
        "Hypotheses:\r\n",
        "- Null hypothesis (H0): The median difference is zero \r\n",
        "- Alternate hypothesis (H1): The median difference is positive\r\n",
        "\r\n",
        "Steps to perform test:\r\n",
        "- Find the difference for each pair.\r\n",
        " - If there is just one difference score of zero, some investigators drop that observation and reduce the sample size by 1 (i.e., the sample size for the binomial distribution would be n-1).\r\n",
        " - If there is an even number of zeros, we randomly assign them positive or negative signs.\r\n",
        "- If there is an odd number of zeros (>=3), we randomly drop one and reduce the sample size by 1, and then randomly assign the remaining observations positive or negative signs. \r\n",
        "- Count the number of positive and negative differences\r\n",
        "\r\n",
        ">|Groups|1|2|3|4|5|6|7|8|\r\n",
        "|---|---|---|---|---|---|---|---|---|\r\n",
        "|Before treatment|85|60|70|75|95|80|85|80|\r\n",
        "|After treatment|70|70|65|80|75|70|75|85|\r\n",
        "|Difference|15|-10|5|-5|20|10|10|-5|\r\n",
        "|Sign|+|-|+|-|+|+|+|-|\r\n",
        "\r\n",
        "n = total number of signs (all positive and negative)\r\n",
        "\r\n",
        "The test statistic for the Sign Test is the number of positive signs or number of negative signs, whichever is smaller.\r\n",
        "\r\n",
        ">Test statistic, $S = min (Count_{+} , Count_{-})$\r\n",
        "\r\n",
        "By using the binomial distribution formula, we can compute the probability of observing this distribution of positive and neagtive combination: \r\n",
        "\r\n",
        "> P-value calculation\r\n",
        "- For one tailed test, p-value is the sum of probabilities of getting at most S number of signs out of n \\\r\n",
        "p-value(One tailed) = $P(x \\le S) = \\sum_{x=1}^{S}\\frac{n!}{(n-x)!} p^x {1-p)}^{(n-x)}$\r\n",
        "- For one tailed test, p-value(One tailed) = $2*P(x \\le S)$ = 2*p-value(One tailed)\r\n",
        "\r\n",
        "For one tailed (side) significance level of 0.05 ~ $P(x \\le k)$, we should have at most k positive or negative sign (critical value = k) out of the n.\r\n",
        "\r\n",
        "Decision rule:\r\n",
        ">- Reject H0: if the smaller of the number of positive or negative signs is less than or equal to that critical value (S<=k), then we reject H0 in favor of H1 \r\n",
        "- Do not reject H0: if the smaller of the number of positive or negative signs is greater than the critical value(S>k), then we do not reject H0. \r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0V4wOkruWsi"
      },
      "source": [
        "### Chi-Square goodness of fit test\r\n",
        "\r\n",
        "A Chi-Square goodness of fit test is used to determine whether or not a categorical variable follows a hypothesized distribution.\r\n",
        "\r\n",
        "- Used when we have only one independent variable\r\n",
        "\r\n",
        "Hypotheses:\r\n",
        "- H0: (null hypothesis) A variable follows a hypothesized distribution.\r\n",
        "- H1: (alternative hypothesis) A variable does not follow a hypothesized distribution.\r\n",
        "\r\n",
        "The test statistic follows chi-square distribution of n-1 degrees of freedom (where n is the number of categories).\r\n",
        "\r\n",
        "|Observed frequency|$o_{1}$|$o_{2}$|$o_{3}$|-|$o_{n}$|\r\n",
        "|---|---|---|---|---|---|\r\n",
        "|Expected frequency|$e_{1}$|$e_{2}$|$e_{3}$|-|$e_{n}$\r\n",
        "\r\n",
        "Chi squared statistic:\r\n",
        "- chi-square statistic, $\\chi^2 = \\sum \\frac{(O-E)^2}{E} = \\sum_{i=1}^{n}\\frac{(o_{i}-e_{i})^2}{e_{i}} \\sim \\chi^2_{df}$ where:\r\n",
        ">$\\chi^2 = \\sum \\frac{(O-E)^2}{E} \\sim \\chi^2_{n-1}$\r\n",
        "where:\r\n",
        "- O = observed frequency\r\n",
        "- E = expected frequency\r\n",
        "- n =  number of categories\r\n",
        "- (n-1) = degree of freedom of chi-square distribution\r\n",
        "- $\\alpha$ = significance level (SL) for right hand tailed test\r\n",
        "- right tailed critical value for SL = $\\chi_{\\alpha}$\r\n",
        "\r\n",
        "Again, with χ2 tests there are no upper, lower or two-tailed tests. If the null hypothesis is true, the observed and expected frequencies will be close in value and the $\\chi^2$ statistic will be close to zero. If the null hypothesis is false, then the $\\chi^2$ statistic will be large. The rejection region for the $\\chi^2$ test of independence is always in the upper (right-hand) tail of the distribution. \r\n",
        "\r\n",
        "<figure>\r\n",
        "<img src=\"https://saylordotorg.github.io/text_introductory-statistics/section_15/3406a41dcf8b2ad498d3271e90a762c1.jpg\" width=\"500\"/>\r\n",
        "<figcaption>Source: saylordotorg.github.io</figcaption>\r\n",
        "</figure>\r\n",
        "\r\n",
        "For given significance level $\\alpha$, right tailed critical value = $\\chi^2_{\\alpha}$\r\n",
        "Decision Rule:\r\n",
        ">-  Reject H0 if (statistic >= right tailed critical value) or (p value <= SL)\r\n",
        "-  Do not reject H0 if (statistic < right tailed critical value) or (p value > SL)\r\n",
        "\r\n",
        "Example:\r\n",
        "\r\n",
        "|Observed frequency|4|5|7|3|8|5|6|Statitic|\r\n",
        "|---|---|---|---|---|---|---|---|---|\r\n",
        "|Expected frequency|4|5|6|2|7|8|8||\r\n",
        "|$(O-E)^2$|0|0|1|1|1|9|4||\r\n",
        "|$(O-E)^2/E$|0|0|1/6|1/2|1/7|9/8|1/2|2.43|\r\n",
        "\r\n",
        "Right tailed critical value for chi square distribution with 7-1=6 degree of freedom $\\chi^2(6) = 12.59$ \r\n",
        "\r\n",
        "Since this statistic, 2.43 < 12.59 is smaller than critical value, null hypotheseis that both groups are independent can not be rejected.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWGt61JcvWba"
      },
      "source": [
        "### Chi-Square Test of Independence test - Two or More Independent categorical Samples\r\n",
        "\r\n",
        "A Chi-Square Test of Independence is used to determine whether or not there is a significant association between two or more categorical (discrete) variables.\r\n",
        "\r\n",
        "- Used when we have two or more independent variables\r\n",
        "- we compare the numbers of observations in each\r\n",
        "category of each variable to the numbers we would\r\n",
        "expect if the variables were independent of each\r\n",
        "other\r\n",
        "\r\n",
        "In the table below, the grouping variable is shown in the rows of the table; r denotes the number of independent groups. \r\n",
        "\r\n",
        "The outcome variable is shown in the columns of the table; c denotes the number of response options in the outcome variable. \r\n",
        "\r\n",
        "Each combination of a row (group) and column (response) is called a cell of the table. The table has r*c cells and is sometimes called an r x c (\"r by c\") table.\r\n",
        "\r\n",
        "- Variable 1 levels: col-1, col-2, col-3, col-4\r\n",
        "- Variable 2 levels: row-1, row-2, row-3, row-4 \r\n",
        "\r\n",
        "**Contingency table with observed frequencies**\r\n",
        ">|Index|col-1|col-2|col-3|col-4|col-total|\r\n",
        "|---|---|---|---|---|---|\r\n",
        "|row-1|$o_{11}$|$o_{12}$|$o_{13}$|$o_{14}$|$RT_{1}$|\r\n",
        "|row-2|$o_{21}$|$o_{22}$|$o_{23}$|$o_{24}$|$RT_{2}$|\r\n",
        "|row-3|$o_{31}$|$o_{32}$|$o_{33}$|$o_{34}$|$RT_{3}$|\r\n",
        "|row-4|$o_{41}$|$o_{42}$|$o_{43}$|$o_{44}$|$RT_{4}$|\r\n",
        "|row-5|$o_{51}$|$o_{52}$|$o_{53}$|$o_{54}$|$RT_{5}$|\r\n",
        "|row-total|$CT_{1}$|$CT_{2}$|$CT_{3}$|$CT_{4}$|$GT$|\r\n",
        "\r\n",
        "**Contingency table with expected frequencies**\r\n",
        ">|Index|col-1|col-2|col-3|col-4|\r\n",
        "|---|---|---|---|---|\r\n",
        "|row-1|$e_{11}$|$e_{12}$|$e_{13}$|$e_{14}$|\r\n",
        "|row-2|$e_{21}$|$e_{22}$|$e_{23}$|$e_{24}$|\r\n",
        "|row-3|$e_{31}$|$e_{32}$|$e_{33}$|$e_{34}$|\r\n",
        "|row-4|$e_{41}$|$e_{42}$|$e_{43}$|$e_{44}$|\r\n",
        "|row-5|$e_{51}$|$e_{52}$|$e_{53}$|$e_{54}$|\r\n",
        "\r\n",
        "Hypotheses:\r\n",
        "- Null hypothesis (H0): The two factors (categorical variables) are independent\r\n",
        "- Alternative hypothesis (H1): The two factors ( categorical variables) are not independent\r\n",
        "\r\n",
        "If the variable 1 and variable 2 are assumed independent, the probablity of a cell is equal to product of the row (response) probability and the column (group) probability.\r\n",
        "\r\n",
        "> Two events, A and B, are independent, if P(A and B) = P(A) P(B). Therefore, \\\r\n",
        "P($cell_{ij}) = P(row_{i})*P(col_{j}) = \\frac{RT_{i}}{GT}*\\frac{CT_{j}}{GT}$ \r\n",
        "\r\n",
        "> Expected frequency of $cell_{ij}$, \r\n",
        "$E = P(cell_{ij}) * GT = \\frac{RT_{i} * CT_{j}}{GT}$\r\n",
        "\r\n",
        "Or simply, Expected Cell Frequency, $E = \\frac{\\text{Row Total} * \\text{Column Total}}{\\text{Grand Total}}$\r\n",
        "\r\n",
        "- number of rows = r\r\n",
        "- number of columns = c\r\n",
        "- $RT_{i}$ = row total for i-th row (marginal row frequency)\r\n",
        "- $CT_{j}$ = column total for j-th column (marginal column frequency)\r\n",
        "- $GT$ =  Grand total of all observations (total sample size)\r\n",
        "- chi-square statistic, $\\chi^2 = \\sum \\frac{(O-E)^2}{E} = \\sum_{i=1}^{r}\\sum_{j=1}^{c}\\frac{(o_{ij}-e_{ij})^2}{e_{ij}} \\sim \\chi^2_{df}$ where:\r\n",
        "- O = observed frequency\r\n",
        "- E = expected frequency\r\n",
        "- n =  number of categories\r\n",
        "- chi-square degree of freedom, $df = (r-1)*(c-1)$\r\n",
        "- $\\alpha$ = significance level (SL) for right hand tailed test\r\n",
        "- right tailed critical value for SL = $\\chi_{\\alpha}$\r\n",
        "\r\n",
        "Decision Rule:\r\n",
        ">-  Reject H0 if (statistic >= right tailed critical value) or (p value <= SL)\r\n",
        "-  Do not reject H0 if (statistic < right tailed critical value) or (p value > SL ) \r\n",
        "\r\n",
        "<figure>\r\n",
        "<img src=\"https://saylordotorg.github.io/text_introductory-statistics/section_15/3406a41dcf8b2ad498d3271e90a762c1.jpg\" width=\"500\"/>\r\n",
        "<figcaption>Source: saylordotorg.github.io</figcaption>\r\n",
        "</figure>\r\n",
        "\r\n",
        "\r\n",
        "If the p-value that corresponds to the test statistic is less than chosen significance level then we can reject the null hypothesis, this shows there is some association between the the two variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0mKnvl6wh98"
      },
      "source": [
        "### Fisher’s Exact Test\r\n",
        "\r\n",
        "Fisher’s Exact Test is used to determine whether or not there is a significant association between two categorical variables. \r\n",
        "\r\n",
        "- Chi square test of independance is not accurate when we have a small number of observations \r\n",
        " - expected frequency of less than 5 in more than 20% of cells) \r\n",
        " - or generally if one or more of the cell counts in a 2×2 table is less than 5), we can substitute Fischer’s exact in a 2 x 2\r\n",
        "design\r\n",
        "\r\n",
        "Hypotheses:\r\n",
        "- Null hypothesis (H0): The two variables are independent.\r\n",
        "- Alternative hypothesis (H1): The two variables are not independent.\r\n",
        "Suppose we have the following 2×2 table:\r\n",
        "\r\n",
        "||Group 1|Group 2|Row Total|\r\n",
        "|---|---|---|---|\r\n",
        "|Category|1|a|b|a+b\r\n",
        "|Category|2|c|d|c+d\r\n",
        "|Column Total|a+c|b+d|a+b+c+d = n|\r\n",
        "\r\n",
        "\r\n",
        "The one-tailed p value for Fisher’s Exact Test is calculated as:\r\n",
        "\r\n",
        "$p = \\frac{(a+b)!(c+d)!(a+c)!(b+d)!}{(a!b!c!d!n!)}$\r\n",
        "\r\n",
        "This produces the same p value as the CDF of the hypergeometric distribution with the following parameters:\r\n",
        "\r\n",
        "- population size = n\r\n",
        "- population \"successes\" = a+b\r\n",
        "- sample size = a + c\r\n",
        "- sample \"successes\" = a\r\n",
        "\r\n",
        "The two-tailed p value for Fisher’s Exact Test is less straightforward to calculate and can’t be found by simply multiplying the one-tailed p value by two."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SOEy0RNyj55"
      },
      "source": [
        "### Chi square test - Effect size\r\n",
        "\r\n",
        "Phi (φ)\r\n",
        "How to Calculate \r\n",
        "Phi is calculated as φ = √(X2 / n)\r\n",
        "\r\n",
        "where:\r\n",
        "\r\n",
        "X2 is the Chi-Square test statistic\r\n",
        "\r\n",
        "n = total number of observations\r\n",
        "\r\n",
        "When to Use\r\n",
        "It’s appropriate to calculate φ only when you’re working with a 2 x 2 contingency table (i.e. a table with exactly two rows and two columns).\r\n",
        "\r\n",
        "How to Interpret\r\n",
        "A value of φ  = 0.1 is considered to be a small effect, 0.3 a medium effect, and 0.5 a large effect.\r\n",
        "\r\n",
        "Cramer’s V (V)\r\n",
        "How to Calculate \r\n",
        "Cramer’s V is calculated as V = √(X2 / n*df)\r\n",
        "\r\n",
        "where:\r\n",
        "\r\n",
        "X2 is the Chi-Square test statistic\r\n",
        "\r\n",
        "n = total number of observations\r\n",
        "\r\n",
        "df = (#rows-1) * (#columns-1)\r\n",
        "\r\n",
        "When to Use\r\n",
        "It’s appropriate to calculate V when you’re working with any table larger than a 2 x 2 contingency table.\r\n",
        "\r\n",
        "How to Interpret\r\n",
        "The following table shows how to interpret V based on the degrees of freedom:\r\n",
        "\r\n",
        "Degrees of freedom\tSmall\tMedium\tLarge\r\n",
        "1\t0.10\t0.30\t0.50\r\n",
        "2\t0.07\t0.21\t0.35\r\n",
        "3\t0.06\t0.17\t0.29\r\n",
        "4\t0.05\t0.15\t0.25\r\n",
        "5\t0.04\t0.13\t0.22\r\n",
        "Odds Ratio (OR)\r\n",
        "How to Calculate \r\n",
        "Given the following 2 x2 table:\r\n",
        "\r\n",
        "Effect Size\t# Successes\t# Failures\r\n",
        "Treatment Group\tA\tB\r\n",
        "Control Group\tC\tD\r\n",
        "The odds ratio would be calculated as:\r\n",
        "\r\n",
        "Odds ratio = (AD) / (BC)\r\n",
        "\r\n",
        "When to Use\r\n",
        "It’s appropriate to calculate the odds ratio only when you’re working with a 2 x 2 contingency table. Typically the odds ratio is calculated when you’re interested in studying the odds of success in a treatment group relative to the odds of success in a control group.\r\n",
        "\r\n",
        "How to Interpret\r\n",
        "There is no specific value at which we deem an odds ratio be a small, medium, or large effect, but the  further away the odds ratio is from 1, the higher the likelihood that the treatment has an actual effect.\r\n",
        "\r\n",
        "It’s best to use domain specific expertise to determine if a given odds ratio should be considered small, medium, or large.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSLs81HpwApd"
      },
      "source": [
        "### Friedman Test\r\n",
        "\r\n",
        "The Friedman Test is a non-parametric alternative to the Repeated Measures ANOVA. It is used to determine whether or not there is a statistically significant difference between the means of three or more groups in which the same subjects show up in each group.\r\n",
        "\r\n",
        "When to Use the Friedman Test\r\n",
        "The Friedman Test is commonly used in two situations:\r\n",
        "\r\n",
        "1. Measuring the mean scores of subjects during three or more time points.\r\n",
        "\r\n",
        "For example, you might want to measure the resting heart rate of subjects one month before they start a training program, one month after starting the program, and two months after using the program. You can perform the Friedman Test to see if there is a significant difference in the mean resting heart rate of patients across these three time points.\r\n",
        "\r\n",
        "2. Measuring the mean scores of subjects under three different conditions.\r\n",
        "\r\n",
        "For example, you might have subjects watch three different movies and rate each one based on how much they enjoyed it. Since each subject shows up in each sample, you can perform a Friedman Test to see if there is a significant difference in the mean rating of the three movies.\r\n",
        "\r\n",
        "Hypotheses:\r\n",
        "\r\n",
        "The null hypothesis (H0): µ1 = µ2 = µ3 (the mean reaction times across the populations are all equal)\r\n",
        "\r\n",
        "The alternative hypothesis: (Ha): at least one population mean is different from the rest\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cY1UuS1rwNk8"
      },
      "source": [
        "### Kruskal-Wallis test\r\n",
        "\r\n",
        "A Kruskal-Wallis test is used to determine whether or not there is a statistically significant difference between the medians of three or more independent groups. This test is the nonparametric equivalent of the one-way ANOVA and is typically used when the normality assumption is violated.   \r\n",
        "\r\n",
        "- The Kruskal-Wallis test does not assume normality in the data \r\n",
        "- It is much less sensitive to outliers than the one-way ANOVA.\r\n",
        "\r\n",
        "Kruskal-Wallis Test Assumptions\r\n",
        ">1. Ordinal or Continuous Response Variable – the response variable should be an ordinal or continuous variable. An example of an ordinal variable is a survey response question measured on a Likert Scale (e.g. a 5-point scale from “strongly disagree” to “strongly agree”) and an example of a continuous variable is weight (e.g. measured in pounds).\r\n",
        "2. Independence – the observations in each group need to be independent of each other. Usually a randomized design will take care of this.\r\n",
        "3. Distributions have similar shapes – the distributions in each group need to have a similar shape.\r\n",
        "\r\n",
        "Hypotheses:\r\n",
        "- The null hypothesis (H0): The k population medians are not all equal\r\n",
        "- The alternative hypothesis: (Ha): At least one of the median is different from the others.\r\n",
        "\r\n",
        "If there are k number of groups to be compated, \r\n",
        ">- observations from all groups consider together \r\n",
        "- are ranked together using Fractional Ranking approach \r\n",
        "\r\n",
        "Note: the sum of the ranks will always equal n(n+1)/2\r\n",
        "\r\n",
        "- $n_{j}$ = the sample size for j-th sample\r\n",
        "- $R_{j}$ = sum of the ranks in j-th group\r\n",
        "- Highest Rank, $N = \\sum_{j=1}^{k} n_{j}$ = total number of observations of all groups\r\n",
        "\r\n",
        "test statistic, $H = \\frac{12}{N(N+1)}\\sum_{j=1}{k}\\frac{R_{j}^2}{n_{j}}-3(N+1)$\r\n",
        "\r\n",
        "Look into Kruskal Wallis Test Critical Values table for the appropriate critical value corresponding to the sample sizes (n1, n2, n3,.., nk) and given level of significance\r\n",
        "> If there are 3 or more comparison groups and 5 or more observations in each of the comparison groups, the test statistic H approximates a chi-square distribution with df=k-1. The critical value can also be looked up in the Critical Values of for the chi-square table\r\n",
        "\r\n",
        "Decision Rule:\r\n",
        "-  Reject H0 if (H >= critical value) or p-value <= SL, there is a difference in the medians of groups\r\n",
        "- Do no reject H0 if (H < critical value) or p-value > SL, there is no difference in the medians of groups\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-rn1-0nxjOo"
      },
      "source": [
        "\r\n",
        "### Dunn’s Test (Post Hoc test)\r\n",
        "\r\n",
        "A Kruskal-Wallis test is used to determine whether or not there is a statistically significant difference between the medians of three or more independent groups. It is considered to be the non-parametric equivalent of the One-Way ANOVA.\r\n",
        "\r\n",
        "If the results of a Kruskal-Wallis test are statistically significant, then it’s appropriate to conduct Dunn’s Test to determine exactly which groups are different.\r\n",
        "\r\n",
        "Dunn’s Test performs pairwise comparisons between each independent group and tells you which groups are statistically significantly different at some level of α.\r\n",
        "\r\n",
        "For example, suppose a researcher wants to know whether three different drugs have different effects on back pain. He recruits 30 subjects for the study and randomly assigns them to use Drug A, Drug B, or Drug C for one month and then measures their back pain at the end of the month.\r\n",
        "\r\n",
        "The researcher can perform a Kruskal-Wallis test to determine if the median back pain is equal among the three drugs. If the p-value of the Kruskal-Wallis test is below a certain threshold, it can be said that the three drugs produce different effects. \r\n",
        "\r\n",
        "Following this, the researcher could then perform Dunn’s Test to determine which drugs produce statistically significant effects.\r\n",
        "\r\n",
        "Dunn’s Test: The Formula\r\n",
        "You will likely never have to perform Dunn’s Test by hand since it can be performed using statistical software (like R, Python, Stata, SPSS, etc.) but the formula to calculate the z-test statistic for the difference between two groups is:\r\n",
        "\r\n",
        "zi = yi / σi\r\n",
        "\r\n",
        "where i is one of the 1 to m comparisons, yi =WA – WB (where WA is the average of the sum of the ranks for the ith group) and σi is calculated as:\r\n",
        "\r\n",
        "σi  =  √((N(N+1)/12) – (ΣT3s – Ts/(12(N-1)) / ((1/nA)+(1/nB))\r\n",
        "\r\n",
        "where N is the total number of observations across all groups, r is the number of tied ranks, and Ts is the number of observations tied at the sth specific tied value.\r\n",
        "\r\n",
        "How to Control the Family-wise Error Rate\r\n",
        "Whenever we make multiple comparisons at once, it’s important that we control the family-wise error rate. One way to do so is to adjust the p-values that results from the multiple comparisons.\r\n",
        "\r\n",
        "There are several ways to adjust the p-values, but the two most common adjustment methods are:\r\n",
        "\r\n",
        "1. The Bonferroni Adjustment\r\n",
        "\r\n",
        "Adjusted p-value = p*m\r\n",
        "\r\n",
        "where:\r\n",
        "\r\n",
        "p: The original p-value\r\n",
        "m: The total number of comparisons being made\r\n",
        "2. The Sidak Adjustment\r\n",
        "\r\n",
        "Adjusted p-value = 1 – (1-p)m\r\n",
        "\r\n",
        "where:\r\n",
        "\r\n",
        "p: The original p-value\r\n",
        "m: The total number of comparisons being made\r\n",
        "By using one of these p-value adjustments, we can dramatically reduce the probability of committing a type I error among the set of multiple comparisons."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntYD8QbRx4f7"
      },
      "source": [
        "### Dunnett’s test (Post Hoc test)\r\n",
        "\r\n",
        "An ANOVA (Analysis of Variance) is used to determine whether or not there is a statistically significant difference between the means of three or more independent groups. \r\n",
        "\r\n",
        "If the p-value from the ANOVA is less than some chosen significance level, we can reject the null hypothesis and conclude that we have sufficient evidence to say that at least one of the means of the groups is different from the others.\r\n",
        "\r\n",
        "However, this doesn’t tell us which groups are different from each other. It simply tells us that not all of the group means are equal. In order to find out exactly which groups are different from each other, we must conduct a post-hoc test.\r\n",
        "\r\n",
        "If one of the groups in the study is considered the control group, then we should use Dunnett’s test as the post-hoc test following the ANOVA.\r\n",
        "\r\n",
        "Dunnett’s Test: Definition\r\n",
        "We can use the following two steps to perform Dunnett’s test:\r\n",
        "\r\n",
        "Step 1: Find Dunnett’s critical value.\r\n",
        "\r\n",
        "First, we must find Dunnett’s critical value. This is calculated as:\r\n",
        "\r\n",
        "Dunnett’s Critical value: td√2MSw/n\r\n",
        "\r\n",
        "where:\r\n",
        "\r\n",
        "td: The value found in Dunnett’s Table for a given alpha level, number of groups, and group sample sizes.\r\n",
        "MSw: The Mean Squares of the “Within Group” in the ANOVA output table\r\n",
        "n: The size of the group samples\r\n",
        "Step 2: Compare the differences in group means to Dunnett’s critical value.\r\n",
        "\r\n",
        "Next, we calculate the absolute difference between the mean of each group with the mean of the control group. If the difference exceeds Dunnett’s critical value, then that difference is said to be statistically significant.\r\n",
        "\r\n",
        "The following example shows how to perform Dunnett’s test in practice.\r\n",
        "\r\n",
        "Dunnett’s Test: Example\r\n",
        "Suppose a teacher wants to know whether or not two new studying techniques have the potential to increase exam scores for her students. To test this, she randomly splits her class of 30 students into the following three groups:\r\n",
        "\r\n",
        "Control Group: 10 students\r\n",
        "New Study technique 1: 10 students\r\n",
        "New Study Technique 2: 10 students\r\n",
        "After one week of using their assigned study technique, each student takes the same exam. The results are as follows:\r\n",
        "\r\n",
        "Mean exam score of control group: 81.6\r\n",
        "Mean exam score of new study technique 1 group: 85.8\r\n",
        "Mean exam score of new study technique 2 group: 87.7\r\n",
        "Mean Squares of the “Within Group” in the ANOVA output table: 23.3\r\n",
        "Using this information, we can perform Dunnett’s test to determine if either of the two new study techniques produce significantly different mean exam scores compared to the control group.\r\n",
        "\r\n",
        "Step 1: Find Dunnett’s critical value.\r\n",
        "\r\n",
        "Using α = .05, group sample size n = 10 and total groups = 3, Dunnett’s table tells us to use a value of 2.57 in the critical value calculation.\r\n",
        "\r\n",
        "Example of using Dunnett's table for multiple comparisons\r\n",
        "\r\n",
        "Next, we can plug this number into the formula to find Dunnett’s Critical value:\r\n",
        "\r\n",
        "Dunnett’s Critical value: td√2MSw/n  =  2.57√2(23.3)/10  =  5.548\r\n",
        "\r\n",
        "Step 2: Compare the differences in group means to Dunnett’s critical value.\r\n",
        "\r\n",
        "The absolute difference between the means of each study technique and the control group are as follows:\r\n",
        "\r\n",
        "Abs. diff between new technique 1 and control: |85.8 – 81.6| = 4.2\r\n",
        "Abs. diff between new technique 2 and control: |87.7 – 81.6| = 6.1\r\n",
        "Only the absolute difference between technique 2 and the control group is greater than Dunnett’s critical value of 5.548.\r\n",
        "\r\n",
        "Thus, we can say that the new studying technique #2 produces significantly different exam scores compared to the control group, but the new studying technique #1 does not."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wy0jwVGgghr"
      },
      "source": [
        "## References:\r\n",
        "\r\n",
        "- https://www.statstutor.ac.uk/resources/uploaded/paired-t-test.pdf\r\n",
        "- https://www.statstutor.ac.uk/resources/uploaded/unpaired-t-test.pdf\r\n",
        "- https://www.ncss.com/software/ncss/nonparametric-analysis-in-ncss/\r\n",
        "- https://www.ncss.com/software/ncss/comparing-means-in-ncss/\r\n",
        "- https://www.cse.iitk.ac.in/users/nsrivast/HCC/lec07-09.pdf\r\n",
        "- https://tjmurphy.github.io/jabstb/nonparametrics.html\r\n",
        "- https://www.statstutor.ac.uk/resources/uploaded/tutorsquickguidetostatistics.pdf\r\n",
        "- https://saylordotorg.github.io/text_introductory-statistics/index.html\r\n",
        "- https://www.spss-tutorials.com/effect-size/\r\n",
        "- http://tss.awf.poznan.pl/files/3_Trends_Vol21_2014__no1_20.pdf\r\n",
        "- https://cran.r-project.org/web/packages/statsExpressions/vignettes/stats_details.html\r\n",
        "- https://www.scribbr.com/statistics/statistical-power/\r\n",
        "- https://www.statstest.com/mann-whitney-u-test/\r\n",
        "- https://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/BS704_Power/BS704_Power_print.html\r\n",
        "- https://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704_nonparametric/bs704_nonparametric_print.html\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUuKDl52Owwf"
      },
      "source": [
        "import numpy as np\r\n",
        "import scipy.stats as stats\r\n",
        "import seaborn as sns\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "def t_dist_known_sd(group_size):\r\n",
        "    t_values_array = []\r\n",
        "    sd = 2\r\n",
        "    for i in range(50000):\r\n",
        "        X1 = stats.norm.rvs(loc=5,\r\n",
        "                            scale=sd,\r\n",
        "                            size=group_size,\r\n",
        "                            random_state=(i + 1))\r\n",
        "        t_stat = (np.mean(X1) - 5) / (sd / np.sqrt(len(X1)))\r\n",
        "        t_values_array.append(t_stat)\r\n",
        "    t_values_array = np.array(t_values_array)\r\n",
        "    return t_values_array\r\n",
        "\r\n",
        "def t_dist_unknown_sd(group_size):\r\n",
        "    t_values_array = []\r\n",
        "    for i in range(50000):\r\n",
        "        X1 = stats.norm.rvs(loc=5,\r\n",
        "                            scale=2,\r\n",
        "                            size=group_size,\r\n",
        "                            random_state=(i + 1))\r\n",
        "        t_stat = (np.mean(X1) - 5) / (np.std(X1, ddof=1) / np.sqrt(len(X1)))\r\n",
        "        t_values_array.append(t_stat)\r\n",
        "    t_values_array = np.array(t_values_array)\r\n",
        "    return t_values_array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTHE-g4jOzCm"
      },
      "source": [
        "# Distributions when the standard deviation of the population is known for different sample size.\r\n",
        "t_5 = t_dist_known_sd(5)\r\n",
        "t_10 = t_dist_known_sd(10)\r\n",
        "t_30 = t_dist_known_sd(30)\r\n",
        "t_100 = t_dist_known_sd(100)\r\n",
        "sns.distplot(x=t_5, color='black', hist=False, label='Sample size: 5')\r\n",
        "sns.distplot(x=t_10, color='darkblue', hist=False, label='Sample size: 10')\r\n",
        "sns.distplot(x=t_30, color='green', hist=False, label='Sample size: 30')\r\n",
        "sns.distplot(x=t_100, color='orange', hist=False, label='Sample size: 100')\r\n",
        "plt.xlim(-5, 5)\r\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qg3l2CIuO1ZE"
      },
      "source": [
        "# Distributions when the standard deviation of the population is unknown for different sample size\r\n",
        "t_5 = t_dist_unknown_sd(5)\r\n",
        "t_10 = t_dist_unknown_sd(10)\r\n",
        "t_30 = t_dist_unknown_sd(30)\r\n",
        "t_100 = t_dist_unknown_sd(100)\r\n",
        "sns.distplot(x=t_5, color='black', hist=False, label='Sample size: 5')\r\n",
        "sns.distplot(x=t_10, color='darkblue', hist=False, label='Sample size: 10')\r\n",
        "sns.distplot(x=t_30, color='green', hist=False, label='Sample size: 30')\r\n",
        "sns.distplot(x=t_100, color='orange', hist=False, label='Sample size: 100')\r\n",
        "plt.xlim(-5, 5)\r\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}